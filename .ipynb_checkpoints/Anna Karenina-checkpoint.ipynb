{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Analysis of Anna Karenina with generative text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's load that file\n",
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text:\n",
      " hat she could not go on living in the same house with him.\n",
      "This position of affairs had now lasted th \n",
      "\n",
      "length of vocab is:\n",
      " 83 \n",
      "\n",
      "vocab:\n",
      " ['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] \n",
      "\n",
      "enumerated vocab:\n",
      " {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '$', 5: '%', 6: '&', 7: \"'\", 8: '(', 9: ')', 10: '*', 11: ',', 12: '-', 13: '.', 14: '/', 15: '0', 16: '1', 17: '2', 18: '3', 19: '4', 20: '5', 21: '6', 22: '7', 23: '8', 24: '9', 25: ':', 26: ';', 27: '?', 28: '@', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '_', 56: '`', 57: 'a', 58: 'b', 59: 'c', 60: 'd', 61: 'e', 62: 'f', 63: 'g', 64: 'h', 65: 'i', 66: 'j', 67: 'k', 68: 'l', 69: 'm', 70: 'n', 71: 'o', 72: 'p', 73: 'q', 74: 'r', 75: 's', 76: 't', 77: 'u', 78: 'v', 79: 'w', 80: 'x', 81: 'y', 82: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# Let's look atthis garbage.\n",
    "print('sample text:\\n', text[313:414], '\\n')\n",
    "print('length of vocab is:\\n', len(vocab), '\\n')\n",
    "print('vocab:\\n', vocab, '\\n')\n",
    "print('enumerated vocab:\\n', int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to get our batches so that we can train our data.\n",
    "def get_batches( arr, n_seqs, n_steps):\n",
    "    \"\"\"\n",
    "    This function is a generator that returns batches of size n_seq * n_steps\n",
    "    \n",
    "    Args:\n",
    "    arr: Just an array to make batches from.\n",
    "    n_seqs: Batch size\n",
    "    n_steps: Steps\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get characters per batch\n",
    "    characters_per_batch = n_seqs * n_steps\n",
    "    n_batches = len(arr) // characters_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make a full batch\n",
    "    arr = arr[:n_batches * characters_per_batch]\n",
    "    \n",
    "    # Reshape into n_seq rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        \n",
    "        #features\n",
    "        x  = arr[:, n: n + n_steps]\n",
    "        \n",
    "        #shifted targets\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get them\n",
    "batches = get_batches(encoded, 13, 49)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[31 64 57 72 76 61 74  1 16  0]\n",
      " [79 60 13  1 47 76 61 72 57 70]\n",
      " [75 65 63 64 76  1 71 62  1 68]\n",
      " [81  1 75 65 63 64 11  1 75 64]\n",
      " [61  1 79 57 75 64 61 60 11  1]\n",
      " [ 1 68 57 75 76  1 76 65 69 61]\n",
      " [13  1 29 70 60  0 62 71 74  1]\n",
      " [61 75 75 65 71 70  1 62 71 74]\n",
      " [61 80 72 74 61 75 75  1 76 64]\n",
      " [63 57 65 70  1 64 61 74 75 61]]\n",
      "\n",
      "y\n",
      " [[64 57 72 76 61 74  1 16  0  0]\n",
      " [60 13  1 47 76 61 72 57 70  1]\n",
      " [65 63 64 76  1 71 62  1 68 71]\n",
      " [ 1 75 65 63 64 11  1 75 64 61]\n",
      " [ 1 79 57 75 64 61 60 11  1 76]\n",
      " [68 57 75 76  1 76 65 69 61 13]\n",
      " [ 1 29 70 60  0 62 71 74  1 77]\n",
      " [75 75 65 71 70  1 62 71 74 27]\n",
      " [80 72 74 61 75 75  1 76 64 65]\n",
      " [57 65 70  1 64 61 74 75 61 68]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sweet, now we have to build the model. We're going to use a network of LSTM's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "def build_inputs(batch_size, num_steps):\n",
    "    \"\"\"\n",
    "    Defining placeholders for the inputs, targets, and dropouts\n",
    "    \n",
    "    Args:\n",
    "    batch_size: Batch Size, Sequences per Batch\n",
    "    num_steps: Number of sequences in a batch\n",
    "    \"\"\"\n",
    "    \n",
    "    # Declare placeholders\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name = 'inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name = 'targets')\n",
    "    \n",
    "    # Let's get a probability for the dropout layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    \n",
    "    # Return statement\n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM Cell\n",
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Building an LSTM Cell\n",
    "    \n",
    "    Args:\n",
    "    keep_prob = Dropout probability\n",
    "    lstm_size = Number of cells per layer\n",
    "    num_layers = Number of layers\n",
    "    batch_size = Batch size\n",
    "    \"\"\"\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        \n",
    "        # Basic LSTM:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        \n",
    "        # Dropout addition:\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = keep_prob)\n",
    "        \n",
    "        # return statement:\n",
    "        return drop\n",
    "    \n",
    "    # Stack up multiple LSTM layers:\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    # return statement:\n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Outputs:\n",
    "\n",
    "def build_output(lstm_output, in_size, out_size):\n",
    "    \"\"\"\n",
    "    Build Softmax layer, return outputs and logits\n",
    "    \n",
    "    Args:\n",
    "    lstm_outpit: Input tensor\n",
    "    in_size: Size of input tensor\n",
    "    out_size: Size of output tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape the output so it's a bunch of rows.\n",
    "    # One row for each step in each sequence\n",
    "    # Shape is ( M * N ) X L; batch_size * num_steps by LSTM size\n",
    "    seq_output = tf.concat(lstm_output, axis = 1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # We need to connect the RNN network with the softmax layer:\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev = 0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "        \n",
    "    # output are just RNN cell outputs.\n",
    "    # logits will be a rows of logit outputs. One for each step & sequence\n",
    "    # This is a very complicated formula, follow it closely.\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # let's use our softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name = 'predictions')\n",
    "    \n",
    "    # Return statement:\n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Loss:\n",
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    \"\"\"\n",
    "    Calculate loss from the logits and the targets\n",
    "    \n",
    "    Args:\n",
    "    logits: Logits from the final fully connected layer\n",
    "    targets: Targets for supervised learning\n",
    "    lstm_size: Number of LSTM hidden units\n",
    "    num_classes: Number of classes in targets\n",
    "    \"\"\"\n",
    "    \n",
    "    # One hot encode targets and reshape to match our logits\n",
    "    # ( N * M ) X C; C = num_classes\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    # Return statement:\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimization:\n",
    "def build_optimizer( loss, learning_rate, grad_clip):\n",
    "    \"\"\"\n",
    "    Building an optimizer for training and bounding our gradients\n",
    "    \n",
    "    Args:\n",
    "    loss: network loss\n",
    "    learning_rate: learning rate\n",
    "    grad_clip: the bound for our gradients\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pretty straight forward here:\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Return statement:\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Construction:\n",
    "\n",
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size = 32, num_steps = 50, lstm_size = 128,\n",
    "                 num_layers = 3, learning_rate = 0.001, grad_clip = 5, sampling = False):\n",
    "        \n",
    "        # When using this network for sampling we'll pass one charcter at a time.\n",
    "        # This is that option.\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "        \n",
    "        # Reset the network.\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Here we are, fear is like a forest.\n",
    "        # Build the placeholder tensor:\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "        \n",
    "        # Build LSTM nodes:\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "        \n",
    "        # To run data through the RNN layers:\n",
    "        # One hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN and grab your outputs:\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state = self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get your predictions and logits:\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer:\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 256         # Size of hidden layers in LSTMs\n",
    "num_layers = 3          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability\n",
    "epochs = 20             # epochs\n",
    "save_every_n = 200      # save every 500 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 1...  Training loss: 4.4198...  1.6721 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2...  Training loss: 4.3861...  1.6324 sec/batch\n",
      "Epoch: 1/20...  Training Step: 3...  Training loss: 4.2907...  1.6992 sec/batch\n",
      "Epoch: 1/20...  Training Step: 4...  Training loss: 3.9213...  1.7818 sec/batch\n",
      "Epoch: 1/20...  Training Step: 5...  Training loss: 3.7292...  1.7914 sec/batch\n",
      "Epoch: 1/20...  Training Step: 6...  Training loss: 3.6439...  1.7800 sec/batch\n",
      "Epoch: 1/20...  Training Step: 7...  Training loss: 3.5213...  1.6933 sec/batch\n",
      "Epoch: 1/20...  Training Step: 8...  Training loss: 3.4899...  1.8991 sec/batch\n",
      "Epoch: 1/20...  Training Step: 9...  Training loss: 3.4413...  2.0655 sec/batch\n",
      "Epoch: 1/20...  Training Step: 10...  Training loss: 3.4159...  1.9083 sec/batch\n",
      "Epoch: 1/20...  Training Step: 11...  Training loss: 3.3590...  2.0699 sec/batch\n",
      "Epoch: 1/20...  Training Step: 12...  Training loss: 3.3642...  1.9999 sec/batch\n",
      "Epoch: 1/20...  Training Step: 13...  Training loss: 3.3229...  1.9237 sec/batch\n",
      "Epoch: 1/20...  Training Step: 14...  Training loss: 3.3431...  1.8583 sec/batch\n",
      "Epoch: 1/20...  Training Step: 15...  Training loss: 3.3182...  1.8668 sec/batch\n",
      "Epoch: 1/20...  Training Step: 16...  Training loss: 3.3033...  1.8159 sec/batch\n",
      "Epoch: 1/20...  Training Step: 17...  Training loss: 3.2874...  2.1053 sec/batch\n",
      "Epoch: 1/20...  Training Step: 18...  Training loss: 3.3102...  1.8274 sec/batch\n",
      "Epoch: 1/20...  Training Step: 19...  Training loss: 3.2794...  1.9903 sec/batch\n",
      "Epoch: 1/20...  Training Step: 20...  Training loss: 3.2366...  1.7507 sec/batch\n",
      "Epoch: 1/20...  Training Step: 21...  Training loss: 3.2602...  1.8800 sec/batch\n",
      "Epoch: 1/20...  Training Step: 22...  Training loss: 3.2439...  1.7432 sec/batch\n",
      "Epoch: 1/20...  Training Step: 23...  Training loss: 3.2409...  1.7943 sec/batch\n",
      "Epoch: 1/20...  Training Step: 24...  Training loss: 3.2355...  1.7728 sec/batch\n",
      "Epoch: 1/20...  Training Step: 25...  Training loss: 3.2246...  1.8550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 26...  Training loss: 3.2407...  1.7838 sec/batch\n",
      "Epoch: 1/20...  Training Step: 27...  Training loss: 3.2343...  1.8314 sec/batch\n",
      "Epoch: 1/20...  Training Step: 28...  Training loss: 3.2108...  1.8825 sec/batch\n",
      "Epoch: 1/20...  Training Step: 29...  Training loss: 3.2218...  2.1663 sec/batch\n",
      "Epoch: 1/20...  Training Step: 30...  Training loss: 3.2264...  1.9577 sec/batch\n",
      "Epoch: 1/20...  Training Step: 31...  Training loss: 3.2453...  1.6564 sec/batch\n",
      "Epoch: 1/20...  Training Step: 32...  Training loss: 3.2041...  1.7823 sec/batch\n",
      "Epoch: 1/20...  Training Step: 33...  Training loss: 3.2001...  1.8610 sec/batch\n",
      "Epoch: 1/20...  Training Step: 34...  Training loss: 3.2241...  1.8026 sec/batch\n",
      "Epoch: 1/20...  Training Step: 35...  Training loss: 3.1914...  1.8635 sec/batch\n",
      "Epoch: 1/20...  Training Step: 36...  Training loss: 3.2121...  1.7432 sec/batch\n",
      "Epoch: 1/20...  Training Step: 37...  Training loss: 3.1798...  1.8519 sec/batch\n",
      "Epoch: 1/20...  Training Step: 38...  Training loss: 3.1952...  1.8350 sec/batch\n",
      "Epoch: 1/20...  Training Step: 39...  Training loss: 3.1860...  1.8379 sec/batch\n",
      "Epoch: 1/20...  Training Step: 40...  Training loss: 3.1916...  1.8160 sec/batch\n",
      "Epoch: 1/20...  Training Step: 41...  Training loss: 3.1801...  1.7542 sec/batch\n",
      "Epoch: 1/20...  Training Step: 42...  Training loss: 3.1817...  1.8700 sec/batch\n",
      "Epoch: 1/20...  Training Step: 43...  Training loss: 3.1874...  1.9721 sec/batch\n",
      "Epoch: 1/20...  Training Step: 44...  Training loss: 3.1756...  1.8027 sec/batch\n",
      "Epoch: 1/20...  Training Step: 45...  Training loss: 3.1602...  1.8013 sec/batch\n",
      "Epoch: 1/20...  Training Step: 46...  Training loss: 3.1760...  1.9520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 47...  Training loss: 3.1875...  1.8652 sec/batch\n",
      "Epoch: 1/20...  Training Step: 48...  Training loss: 3.2005...  1.8961 sec/batch\n",
      "Epoch: 1/20...  Training Step: 49...  Training loss: 3.1893...  1.8006 sec/batch\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 3.1903...  1.8249 sec/batch\n",
      "Epoch: 1/20...  Training Step: 51...  Training loss: 3.1721...  1.8835 sec/batch\n",
      "Epoch: 1/20...  Training Step: 52...  Training loss: 3.1681...  1.8444 sec/batch\n",
      "Epoch: 1/20...  Training Step: 53...  Training loss: 3.1855...  1.8680 sec/batch\n",
      "Epoch: 1/20...  Training Step: 54...  Training loss: 3.1649...  1.7732 sec/batch\n",
      "Epoch: 1/20...  Training Step: 55...  Training loss: 3.1746...  1.8469 sec/batch\n",
      "Epoch: 1/20...  Training Step: 56...  Training loss: 3.1497...  1.7677 sec/batch\n",
      "Epoch: 1/20...  Training Step: 57...  Training loss: 3.1694...  1.9243 sec/batch\n",
      "Epoch: 1/20...  Training Step: 58...  Training loss: 3.1705...  1.9671 sec/batch\n",
      "Epoch: 1/20...  Training Step: 59...  Training loss: 3.1543...  1.9592 sec/batch\n",
      "Epoch: 1/20...  Training Step: 60...  Training loss: 3.1702...  2.0846 sec/batch\n",
      "Epoch: 1/20...  Training Step: 61...  Training loss: 3.1704...  2.1758 sec/batch\n",
      "Epoch: 1/20...  Training Step: 62...  Training loss: 3.1849...  2.1221 sec/batch\n",
      "Epoch: 1/20...  Training Step: 63...  Training loss: 3.1931...  2.1708 sec/batch\n",
      "Epoch: 1/20...  Training Step: 64...  Training loss: 3.1461...  2.3924 sec/batch\n",
      "Epoch: 1/20...  Training Step: 65...  Training loss: 3.1595...  2.0946 sec/batch\n",
      "Epoch: 1/20...  Training Step: 66...  Training loss: 3.1763...  2.0750 sec/batch\n",
      "Epoch: 1/20...  Training Step: 67...  Training loss: 3.1768...  2.0565 sec/batch\n",
      "Epoch: 1/20...  Training Step: 68...  Training loss: 3.1297...  1.9668 sec/batch\n",
      "Epoch: 1/20...  Training Step: 69...  Training loss: 3.1503...  2.0570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 70...  Training loss: 3.1659...  1.9176 sec/batch\n",
      "Epoch: 1/20...  Training Step: 71...  Training loss: 3.1601...  1.9707 sec/batch\n",
      "Epoch: 1/20...  Training Step: 72...  Training loss: 3.1756...  2.0299 sec/batch\n",
      "Epoch: 1/20...  Training Step: 73...  Training loss: 3.1592...  1.9703 sec/batch\n",
      "Epoch: 1/20...  Training Step: 74...  Training loss: 3.1592...  2.0424 sec/batch\n",
      "Epoch: 1/20...  Training Step: 75...  Training loss: 3.1688...  1.9677 sec/batch\n",
      "Epoch: 1/20...  Training Step: 76...  Training loss: 3.1757...  2.0996 sec/batch\n",
      "Epoch: 1/20...  Training Step: 77...  Training loss: 3.1680...  2.0259 sec/batch\n",
      "Epoch: 1/20...  Training Step: 78...  Training loss: 3.1627...  2.1237 sec/batch\n",
      "Epoch: 1/20...  Training Step: 79...  Training loss: 3.1504...  2.1427 sec/batch\n",
      "Epoch: 1/20...  Training Step: 80...  Training loss: 3.1407...  1.9402 sec/batch\n",
      "Epoch: 1/20...  Training Step: 81...  Training loss: 3.1512...  2.0028 sec/batch\n",
      "Epoch: 1/20...  Training Step: 82...  Training loss: 3.1700...  1.8830 sec/batch\n",
      "Epoch: 1/20...  Training Step: 83...  Training loss: 3.1705...  1.9532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 84...  Training loss: 3.1514...  1.9893 sec/batch\n",
      "Epoch: 1/20...  Training Step: 85...  Training loss: 3.1381...  1.9437 sec/batch\n",
      "Epoch: 1/20...  Training Step: 86...  Training loss: 3.1435...  2.0294 sec/batch\n",
      "Epoch: 1/20...  Training Step: 87...  Training loss: 3.1435...  1.9206 sec/batch\n",
      "Epoch: 1/20...  Training Step: 88...  Training loss: 3.1457...  1.9306 sec/batch\n",
      "Epoch: 1/20...  Training Step: 89...  Training loss: 3.1658...  2.0620 sec/batch\n",
      "Epoch: 1/20...  Training Step: 90...  Training loss: 3.1603...  1.9703 sec/batch\n",
      "Epoch: 1/20...  Training Step: 91...  Training loss: 3.1617...  2.0605 sec/batch\n",
      "Epoch: 1/20...  Training Step: 92...  Training loss: 3.1549...  1.8715 sec/batch\n",
      "Epoch: 1/20...  Training Step: 93...  Training loss: 3.1610...  1.8820 sec/batch\n",
      "Epoch: 1/20...  Training Step: 94...  Training loss: 3.1576...  1.7542 sec/batch\n",
      "Epoch: 1/20...  Training Step: 95...  Training loss: 3.1528...  1.7928 sec/batch\n",
      "Epoch: 1/20...  Training Step: 96...  Training loss: 3.1489...  1.8404 sec/batch\n",
      "Epoch: 1/20...  Training Step: 97...  Training loss: 3.1653...  1.7677 sec/batch\n",
      "Epoch: 1/20...  Training Step: 98...  Training loss: 3.1480...  1.8191 sec/batch\n",
      "Epoch: 1/20...  Training Step: 99...  Training loss: 3.1582...  1.7417 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 3.1480...  1.7116 sec/batch\n",
      "Epoch: 1/20...  Training Step: 101...  Training loss: 3.1575...  1.8045 sec/batch\n",
      "Epoch: 1/20...  Training Step: 102...  Training loss: 3.1555...  1.7414 sec/batch\n",
      "Epoch: 1/20...  Training Step: 103...  Training loss: 3.1584...  1.8336 sec/batch\n",
      "Epoch: 1/20...  Training Step: 104...  Training loss: 3.1438...  1.7178 sec/batch\n",
      "Epoch: 1/20...  Training Step: 105...  Training loss: 3.1553...  1.7298 sec/batch\n",
      "Epoch: 1/20...  Training Step: 106...  Training loss: 3.1488...  1.7507 sec/batch\n",
      "Epoch: 1/20...  Training Step: 107...  Training loss: 3.1268...  1.7633 sec/batch\n",
      "Epoch: 1/20...  Training Step: 108...  Training loss: 3.1257...  1.8284 sec/batch\n",
      "Epoch: 1/20...  Training Step: 109...  Training loss: 3.1506...  1.7407 sec/batch\n",
      "Epoch: 1/20...  Training Step: 110...  Training loss: 3.1184...  1.8165 sec/batch\n",
      "Epoch: 1/20...  Training Step: 111...  Training loss: 3.2351...  1.6865 sec/batch\n",
      "Epoch: 1/20...  Training Step: 112...  Training loss: 3.7125...  1.7358 sec/batch\n",
      "Epoch: 1/20...  Training Step: 113...  Training loss: 3.6257...  1.8184 sec/batch\n",
      "Epoch: 1/20...  Training Step: 114...  Training loss: 3.5020...  1.7567 sec/batch\n",
      "Epoch: 1/20...  Training Step: 115...  Training loss: 3.3574...  1.8265 sec/batch\n",
      "Epoch: 1/20...  Training Step: 116...  Training loss: 3.1475...  1.7236 sec/batch\n",
      "Epoch: 1/20...  Training Step: 117...  Training loss: 3.1463...  1.7297 sec/batch\n",
      "Epoch: 1/20...  Training Step: 118...  Training loss: 3.1655...  1.8211 sec/batch\n",
      "Epoch: 1/20...  Training Step: 119...  Training loss: 3.1678...  1.7400 sec/batch\n",
      "Epoch: 1/20...  Training Step: 120...  Training loss: 3.1429...  1.8323 sec/batch\n",
      "Epoch: 1/20...  Training Step: 121...  Training loss: 3.1836...  1.7553 sec/batch\n",
      "Epoch: 1/20...  Training Step: 122...  Training loss: 3.1582...  1.8248 sec/batch\n",
      "Epoch: 1/20...  Training Step: 123...  Training loss: 3.1652...  1.6993 sec/batch\n",
      "Epoch: 1/20...  Training Step: 124...  Training loss: 3.1719...  1.7307 sec/batch\n",
      "Epoch: 1/20...  Training Step: 125...  Training loss: 3.1426...  1.8204 sec/batch\n",
      "Epoch: 1/20...  Training Step: 126...  Training loss: 3.1319...  1.7602 sec/batch\n",
      "Epoch: 1/20...  Training Step: 127...  Training loss: 3.1592...  1.8444 sec/batch\n",
      "Epoch: 1/20...  Training Step: 128...  Training loss: 3.1632...  1.7232 sec/batch\n",
      "Epoch: 1/20...  Training Step: 129...  Training loss: 3.1501...  1.7356 sec/batch\n",
      "Epoch: 1/20...  Training Step: 130...  Training loss: 3.1565...  1.8289 sec/batch\n",
      "Epoch: 1/20...  Training Step: 131...  Training loss: 3.1667...  1.7489 sec/batch\n",
      "Epoch: 1/20...  Training Step: 132...  Training loss: 3.1437...  1.8154 sec/batch\n",
      "Epoch: 1/20...  Training Step: 133...  Training loss: 3.1545...  1.6980 sec/batch\n",
      "Epoch: 1/20...  Training Step: 134...  Training loss: 3.1428...  1.7492 sec/batch\n",
      "Epoch: 1/20...  Training Step: 135...  Training loss: 3.1160...  1.8139 sec/batch\n",
      "Epoch: 1/20...  Training Step: 136...  Training loss: 3.1237...  1.7816 sec/batch\n",
      "Epoch: 1/20...  Training Step: 137...  Training loss: 3.1403...  1.8104 sec/batch\n",
      "Epoch: 1/20...  Training Step: 138...  Training loss: 3.1294...  1.7858 sec/batch\n",
      "Epoch: 1/20...  Training Step: 139...  Training loss: 3.1563...  1.8206 sec/batch\n",
      "Epoch: 1/20...  Training Step: 140...  Training loss: 3.1485...  1.7261 sec/batch\n",
      "Epoch: 1/20...  Training Step: 141...  Training loss: 3.1431...  1.7788 sec/batch\n",
      "Epoch: 1/20...  Training Step: 142...  Training loss: 3.1232...  1.8422 sec/batch\n",
      "Epoch: 1/20...  Training Step: 143...  Training loss: 3.1290...  1.8154 sec/batch\n",
      "Epoch: 1/20...  Training Step: 144...  Training loss: 3.1345...  1.8634 sec/batch\n",
      "Epoch: 1/20...  Training Step: 145...  Training loss: 3.1383...  1.7191 sec/batch\n",
      "Epoch: 1/20...  Training Step: 146...  Training loss: 3.1450...  1.7559 sec/batch\n",
      "Epoch: 1/20...  Training Step: 147...  Training loss: 3.1584...  1.8815 sec/batch\n",
      "Epoch: 1/20...  Training Step: 148...  Training loss: 3.1700...  1.7683 sec/batch\n",
      "Epoch: 1/20...  Training Step: 149...  Training loss: 3.1311...  1.8073 sec/batch\n",
      "Epoch: 1/20...  Training Step: 150...  Training loss: 3.1426...  1.7261 sec/batch\n",
      "Epoch: 1/20...  Training Step: 151...  Training loss: 3.1593...  1.7192 sec/batch\n",
      "Epoch: 1/20...  Training Step: 152...  Training loss: 3.1709...  1.8251 sec/batch\n",
      "Epoch: 1/20...  Training Step: 153...  Training loss: 3.1456...  1.7412 sec/batch\n",
      "Epoch: 1/20...  Training Step: 154...  Training loss: 3.1457...  1.8471 sec/batch\n",
      "Epoch: 1/20...  Training Step: 155...  Training loss: 3.1275...  1.7607 sec/batch\n",
      "Epoch: 1/20...  Training Step: 156...  Training loss: 3.1365...  1.8475 sec/batch\n",
      "Epoch: 1/20...  Training Step: 157...  Training loss: 3.1316...  1.7033 sec/batch\n",
      "Epoch: 1/20...  Training Step: 158...  Training loss: 3.1370...  1.7652 sec/batch\n",
      "Epoch: 1/20...  Training Step: 159...  Training loss: 3.1203...  1.8339 sec/batch\n",
      "Epoch: 1/20...  Training Step: 160...  Training loss: 3.1197...  1.7897 sec/batch\n",
      "Epoch: 1/20...  Training Step: 161...  Training loss: 3.1497...  1.8171 sec/batch\n",
      "Epoch: 1/20...  Training Step: 162...  Training loss: 3.1101...  1.7350 sec/batch\n",
      "Epoch: 1/20...  Training Step: 163...  Training loss: 3.1122...  1.7467 sec/batch\n",
      "Epoch: 1/20...  Training Step: 164...  Training loss: 3.1381...  1.8571 sec/batch\n",
      "Epoch: 1/20...  Training Step: 165...  Training loss: 3.1304...  1.7629 sec/batch\n",
      "Epoch: 1/20...  Training Step: 166...  Training loss: 3.1243...  1.8555 sec/batch\n",
      "Epoch: 1/20...  Training Step: 167...  Training loss: 3.1390...  1.7432 sec/batch\n",
      "Epoch: 1/20...  Training Step: 168...  Training loss: 3.1382...  1.8304 sec/batch\n",
      "Epoch: 1/20...  Training Step: 169...  Training loss: 3.1366...  1.7252 sec/batch\n",
      "Epoch: 1/20...  Training Step: 170...  Training loss: 3.1072...  1.7765 sec/batch\n",
      "Epoch: 1/20...  Training Step: 171...  Training loss: 3.1406...  1.8339 sec/batch\n",
      "Epoch: 1/20...  Training Step: 172...  Training loss: 3.1711...  1.7760 sec/batch\n",
      "Epoch: 1/20...  Training Step: 173...  Training loss: 3.1781...  1.8283 sec/batch\n",
      "Epoch: 1/20...  Training Step: 174...  Training loss: 3.1770...  1.7322 sec/batch\n",
      "Epoch: 1/20...  Training Step: 175...  Training loss: 3.1540...  1.7375 sec/batch\n",
      "Epoch: 1/20...  Training Step: 176...  Training loss: 3.1511...  1.8378 sec/batch\n",
      "Epoch: 1/20...  Training Step: 177...  Training loss: 3.1368...  1.7527 sec/batch\n",
      "Epoch: 1/20...  Training Step: 178...  Training loss: 3.1134...  1.8448 sec/batch\n",
      "Epoch: 1/20...  Training Step: 179...  Training loss: 3.1243...  1.7552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 180...  Training loss: 3.1148...  1.7737 sec/batch\n",
      "Epoch: 1/20...  Training Step: 181...  Training loss: 3.1320...  1.8490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 182...  Training loss: 3.1452...  1.7843 sec/batch\n",
      "Epoch: 1/20...  Training Step: 183...  Training loss: 3.1192...  1.7607 sec/batch\n",
      "Epoch: 1/20...  Training Step: 184...  Training loss: 3.1470...  1.7607 sec/batch\n",
      "Epoch: 1/20...  Training Step: 185...  Training loss: 3.1728...  1.7592 sec/batch\n",
      "Epoch: 1/20...  Training Step: 186...  Training loss: 3.1284...  1.6881 sec/batch\n",
      "Epoch: 1/20...  Training Step: 187...  Training loss: 3.1327...  1.8716 sec/batch\n",
      "Epoch: 1/20...  Training Step: 188...  Training loss: 3.1207...  2.1298 sec/batch\n",
      "Epoch: 1/20...  Training Step: 189...  Training loss: 3.1353...  1.9567 sec/batch\n",
      "Epoch: 1/20...  Training Step: 190...  Training loss: 3.1318...  1.9759 sec/batch\n",
      "Epoch: 1/20...  Training Step: 191...  Training loss: 3.1365...  1.7083 sec/batch\n",
      "Epoch: 1/20...  Training Step: 192...  Training loss: 3.1025...  1.7634 sec/batch\n",
      "Epoch: 1/20...  Training Step: 193...  Training loss: 3.1290...  1.9222 sec/batch\n",
      "Epoch: 1/20...  Training Step: 194...  Training loss: 3.1233...  1.7491 sec/batch\n",
      "Epoch: 1/20...  Training Step: 195...  Training loss: 3.1024...  1.9096 sec/batch\n",
      "Epoch: 1/20...  Training Step: 196...  Training loss: 3.1163...  1.8813 sec/batch\n",
      "Epoch: 1/20...  Training Step: 197...  Training loss: 3.1176...  1.8605 sec/batch\n",
      "Epoch: 1/20...  Training Step: 198...  Training loss: 3.1175...  1.9007 sec/batch\n",
      "Epoch: 2/20...  Training Step: 199...  Training loss: 3.1648...  1.8350 sec/batch\n",
      "Epoch: 2/20...  Training Step: 200...  Training loss: 3.1087...  1.8565 sec/batch\n",
      "Epoch: 2/20...  Training Step: 201...  Training loss: 3.1051...  1.7562 sec/batch\n",
      "Epoch: 2/20...  Training Step: 202...  Training loss: 3.1185...  1.7922 sec/batch\n",
      "Epoch: 2/20...  Training Step: 203...  Training loss: 3.1255...  1.6995 sec/batch\n",
      "Epoch: 2/20...  Training Step: 204...  Training loss: 3.1398...  1.8034 sec/batch\n",
      "Epoch: 2/20...  Training Step: 205...  Training loss: 3.1326...  1.9973 sec/batch\n",
      "Epoch: 2/20...  Training Step: 206...  Training loss: 3.1333...  2.0165 sec/batch\n",
      "Epoch: 2/20...  Training Step: 207...  Training loss: 3.1148...  1.9187 sec/batch\n",
      "Epoch: 2/20...  Training Step: 208...  Training loss: 3.1180...  1.7941 sec/batch\n",
      "Epoch: 2/20...  Training Step: 209...  Training loss: 3.1069...  1.8975 sec/batch\n",
      "Epoch: 2/20...  Training Step: 210...  Training loss: 3.1235...  1.9302 sec/batch\n",
      "Epoch: 2/20...  Training Step: 211...  Training loss: 3.1171...  1.9320 sec/batch\n",
      "Epoch: 2/20...  Training Step: 212...  Training loss: 3.1381...  1.8779 sec/batch\n",
      "Epoch: 2/20...  Training Step: 213...  Training loss: 3.1283...  1.7916 sec/batch\n",
      "Epoch: 2/20...  Training Step: 214...  Training loss: 3.1226...  1.8981 sec/batch\n",
      "Epoch: 2/20...  Training Step: 215...  Training loss: 3.1184...  1.8454 sec/batch\n",
      "Epoch: 2/20...  Training Step: 216...  Training loss: 3.1492...  1.7903 sec/batch\n",
      "Epoch: 2/20...  Training Step: 217...  Training loss: 3.1307...  1.8555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 218...  Training loss: 3.0993...  1.7743 sec/batch\n",
      "Epoch: 2/20...  Training Step: 219...  Training loss: 3.1255...  1.8977 sec/batch\n",
      "Epoch: 2/20...  Training Step: 220...  Training loss: 3.1238...  1.7803 sec/batch\n",
      "Epoch: 2/20...  Training Step: 221...  Training loss: 3.1182...  1.8507 sec/batch\n",
      "Epoch: 2/20...  Training Step: 222...  Training loss: 3.1190...  1.8815 sec/batch\n",
      "Epoch: 2/20...  Training Step: 223...  Training loss: 3.1092...  1.8034 sec/batch\n",
      "Epoch: 2/20...  Training Step: 224...  Training loss: 3.1285...  1.8700 sec/batch\n",
      "Epoch: 2/20...  Training Step: 225...  Training loss: 3.1319...  1.7557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 226...  Training loss: 3.1076...  1.7850 sec/batch\n",
      "Epoch: 2/20...  Training Step: 227...  Training loss: 3.1186...  1.8631 sec/batch\n",
      "Epoch: 2/20...  Training Step: 228...  Training loss: 3.1274...  1.8153 sec/batch\n",
      "Epoch: 2/20...  Training Step: 229...  Training loss: 3.1461...  1.9164 sec/batch\n",
      "Epoch: 2/20...  Training Step: 230...  Training loss: 3.1160...  1.8010 sec/batch\n",
      "Epoch: 2/20...  Training Step: 231...  Training loss: 3.1053...  1.8770 sec/batch\n",
      "Epoch: 2/20...  Training Step: 232...  Training loss: 3.1287...  1.9206 sec/batch\n",
      "Epoch: 2/20...  Training Step: 233...  Training loss: 3.1032...  1.8853 sec/batch\n",
      "Epoch: 2/20...  Training Step: 234...  Training loss: 3.1246...  1.9392 sec/batch\n",
      "Epoch: 2/20...  Training Step: 235...  Training loss: 3.0922...  1.8224 sec/batch\n",
      "Epoch: 2/20...  Training Step: 236...  Training loss: 3.1046...  1.8950 sec/batch\n",
      "Epoch: 2/20...  Training Step: 237...  Training loss: 3.0970...  1.7858 sec/batch\n",
      "Epoch: 2/20...  Training Step: 238...  Training loss: 3.1018...  1.8184 sec/batch\n",
      "Epoch: 2/20...  Training Step: 239...  Training loss: 3.0907...  1.8790 sec/batch\n",
      "Epoch: 2/20...  Training Step: 240...  Training loss: 3.0982...  1.7838 sec/batch\n",
      "Epoch: 2/20...  Training Step: 241...  Training loss: 3.0914...  1.9507 sec/batch\n",
      "Epoch: 2/20...  Training Step: 242...  Training loss: 3.0934...  1.7995 sec/batch\n",
      "Epoch: 2/20...  Training Step: 243...  Training loss: 3.0893...  1.7453 sec/batch\n",
      "Epoch: 2/20...  Training Step: 244...  Training loss: 3.1037...  1.8277 sec/batch\n",
      "Epoch: 2/20...  Training Step: 245...  Training loss: 3.0999...  1.7783 sec/batch\n",
      "Epoch: 2/20...  Training Step: 246...  Training loss: 3.1117...  1.8453 sec/batch\n",
      "Epoch: 2/20...  Training Step: 247...  Training loss: 3.1079...  1.7940 sec/batch\n",
      "Epoch: 2/20...  Training Step: 248...  Training loss: 3.1008...  1.8809 sec/batch\n",
      "Epoch: 2/20...  Training Step: 249...  Training loss: 3.0919...  1.7311 sec/batch\n",
      "Epoch: 2/20...  Training Step: 250...  Training loss: 3.0864...  1.7838 sec/batch\n",
      "Epoch: 2/20...  Training Step: 251...  Training loss: 3.0924...  1.8655 sec/batch\n",
      "Epoch: 2/20...  Training Step: 252...  Training loss: 3.0758...  1.8565 sec/batch\n",
      "Epoch: 2/20...  Training Step: 253...  Training loss: 3.0805...  1.8289 sec/batch\n",
      "Epoch: 2/20...  Training Step: 254...  Training loss: 3.0595...  1.7352 sec/batch\n",
      "Epoch: 2/20...  Training Step: 255...  Training loss: 3.0710...  1.7733 sec/batch\n",
      "Epoch: 2/20...  Training Step: 256...  Training loss: 3.0696...  1.8976 sec/batch\n",
      "Epoch: 2/20...  Training Step: 257...  Training loss: 3.0489...  1.8109 sec/batch\n",
      "Epoch: 2/20...  Training Step: 258...  Training loss: 3.0588...  1.8653 sec/batch\n",
      "Epoch: 2/20...  Training Step: 259...  Training loss: 3.0544...  1.8033 sec/batch\n",
      "Epoch: 2/20...  Training Step: 260...  Training loss: 3.0737...  1.8421 sec/batch\n",
      "Epoch: 2/20...  Training Step: 261...  Training loss: 3.1107...  1.7261 sec/batch\n",
      "Epoch: 2/20...  Training Step: 262...  Training loss: 3.1081...  1.7961 sec/batch\n",
      "Epoch: 2/20...  Training Step: 263...  Training loss: 3.0341...  1.8751 sec/batch\n",
      "Epoch: 2/20...  Training Step: 264...  Training loss: 3.0950...  1.8625 sec/batch\n",
      "Epoch: 2/20...  Training Step: 265...  Training loss: 3.0567...  1.8367 sec/batch\n",
      "Epoch: 2/20...  Training Step: 266...  Training loss: 3.0228...  1.8677 sec/batch\n",
      "Epoch: 2/20...  Training Step: 267...  Training loss: 3.0254...  1.9748 sec/batch\n",
      "Epoch: 2/20...  Training Step: 268...  Training loss: 3.0353...  1.9255 sec/batch\n",
      "Epoch: 2/20...  Training Step: 269...  Training loss: 3.0379...  1.7914 sec/batch\n",
      "Epoch: 2/20...  Training Step: 270...  Training loss: 3.0422...  1.8715 sec/batch\n",
      "Epoch: 2/20...  Training Step: 271...  Training loss: 3.0169...  1.8055 sec/batch\n",
      "Epoch: 2/20...  Training Step: 272...  Training loss: 3.0225...  1.8947 sec/batch\n",
      "Epoch: 2/20...  Training Step: 273...  Training loss: 3.0196...  1.9136 sec/batch\n",
      "Epoch: 2/20...  Training Step: 274...  Training loss: 3.0188...  2.1671 sec/batch\n",
      "Epoch: 2/20...  Training Step: 275...  Training loss: 3.0110...  2.1056 sec/batch\n",
      "Epoch: 2/20...  Training Step: 276...  Training loss: 2.9980...  1.8991 sec/batch\n",
      "Epoch: 2/20...  Training Step: 277...  Training loss: 2.9805...  2.0324 sec/batch\n",
      "Epoch: 2/20...  Training Step: 278...  Training loss: 2.9655...  1.9186 sec/batch\n",
      "Epoch: 2/20...  Training Step: 279...  Training loss: 2.9631...  1.9442 sec/batch\n",
      "Epoch: 2/20...  Training Step: 280...  Training loss: 2.9781...  2.0575 sec/batch\n",
      "Epoch: 2/20...  Training Step: 281...  Training loss: 2.9710...  1.9527 sec/batch\n",
      "Epoch: 2/20...  Training Step: 282...  Training loss: 2.9553...  2.1051 sec/batch\n",
      "Epoch: 2/20...  Training Step: 283...  Training loss: 2.9282...  1.9949 sec/batch\n",
      "Epoch: 2/20...  Training Step: 284...  Training loss: 2.9458...  1.9958 sec/batch\n",
      "Epoch: 2/20...  Training Step: 285...  Training loss: 2.9307...  2.0856 sec/batch\n",
      "Epoch: 2/20...  Training Step: 286...  Training loss: 2.9332...  1.9507 sec/batch\n",
      "Epoch: 2/20...  Training Step: 287...  Training loss: 2.9434...  2.0755 sec/batch\n",
      "Epoch: 2/20...  Training Step: 288...  Training loss: 2.9372...  1.9577 sec/batch\n",
      "Epoch: 2/20...  Training Step: 289...  Training loss: 2.9353...  1.9627 sec/batch\n",
      "Epoch: 2/20...  Training Step: 290...  Training loss: 2.9226...  2.0479 sec/batch\n",
      "Epoch: 2/20...  Training Step: 291...  Training loss: 2.9292...  1.9512 sec/batch\n",
      "Epoch: 2/20...  Training Step: 292...  Training loss: 2.9127...  2.0670 sec/batch\n",
      "Epoch: 2/20...  Training Step: 293...  Training loss: 2.9036...  1.9818 sec/batch\n",
      "Epoch: 2/20...  Training Step: 294...  Training loss: 2.8926...  2.0104 sec/batch\n",
      "Epoch: 2/20...  Training Step: 295...  Training loss: 2.9107...  1.9041 sec/batch\n",
      "Epoch: 2/20...  Training Step: 296...  Training loss: 2.9024...  1.9437 sec/batch\n",
      "Epoch: 2/20...  Training Step: 297...  Training loss: 2.9044...  2.0500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 298...  Training loss: 2.8997...  1.9382 sec/batch\n",
      "Epoch: 2/20...  Training Step: 299...  Training loss: 2.9025...  2.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 300...  Training loss: 2.8953...  1.9472 sec/batch\n",
      "Epoch: 2/20...  Training Step: 301...  Training loss: 2.8957...  1.9502 sec/batch\n",
      "Epoch: 2/20...  Training Step: 302...  Training loss: 2.8873...  2.1292 sec/batch\n",
      "Epoch: 2/20...  Training Step: 303...  Training loss: 2.8797...  2.0731 sec/batch\n",
      "Epoch: 2/20...  Training Step: 304...  Training loss: 2.8915...  2.0124 sec/batch\n",
      "Epoch: 2/20...  Training Step: 305...  Training loss: 2.8579...  1.9312 sec/batch\n",
      "Epoch: 2/20...  Training Step: 306...  Training loss: 2.8779...  2.0214 sec/batch\n",
      "Epoch: 2/20...  Training Step: 307...  Training loss: 2.8822...  1.8880 sec/batch\n",
      "Epoch: 2/20...  Training Step: 308...  Training loss: 2.8423...  1.9311 sec/batch\n",
      "Epoch: 2/20...  Training Step: 309...  Training loss: 2.8526...  2.0510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 310...  Training loss: 2.8746...  1.9477 sec/batch\n",
      "Epoch: 2/20...  Training Step: 311...  Training loss: 2.8505...  2.0354 sec/batch\n",
      "Epoch: 2/20...  Training Step: 312...  Training loss: 2.8529...  1.9321 sec/batch\n",
      "Epoch: 2/20...  Training Step: 313...  Training loss: 2.8384...  1.9552 sec/batch\n",
      "Epoch: 2/20...  Training Step: 314...  Training loss: 2.8342...  2.0670 sec/batch\n",
      "Epoch: 2/20...  Training Step: 315...  Training loss: 2.8462...  1.9707 sec/batch\n",
      "Epoch: 2/20...  Training Step: 316...  Training loss: 2.8508...  2.2420 sec/batch\n",
      "Epoch: 2/20...  Training Step: 317...  Training loss: 2.8679...  2.1993 sec/batch\n",
      "Epoch: 2/20...  Training Step: 318...  Training loss: 2.8323...  2.1913 sec/batch\n",
      "Epoch: 2/20...  Training Step: 319...  Training loss: 2.8666...  2.2690 sec/batch\n",
      "Epoch: 2/20...  Training Step: 320...  Training loss: 2.8477...  2.1963 sec/batch\n",
      "Epoch: 2/20...  Training Step: 321...  Training loss: 2.8341...  2.0274 sec/batch\n",
      "Epoch: 2/20...  Training Step: 322...  Training loss: 2.8517...  2.0625 sec/batch\n",
      "Epoch: 2/20...  Training Step: 323...  Training loss: 2.8175...  2.1003 sec/batch\n",
      "Epoch: 2/20...  Training Step: 324...  Training loss: 2.8102...  2.2776 sec/batch\n",
      "Epoch: 2/20...  Training Step: 325...  Training loss: 2.8395...  2.1001 sec/batch\n",
      "Epoch: 2/20...  Training Step: 326...  Training loss: 2.8397...  2.1913 sec/batch\n",
      "Epoch: 2/20...  Training Step: 327...  Training loss: 2.8105...  2.1357 sec/batch\n",
      "Epoch: 2/20...  Training Step: 328...  Training loss: 2.8190...  2.2715 sec/batch\n",
      "Epoch: 2/20...  Training Step: 329...  Training loss: 2.8221...  2.2961 sec/batch\n",
      "Epoch: 2/20...  Training Step: 330...  Training loss: 2.8020...  2.1136 sec/batch\n",
      "Epoch: 2/20...  Training Step: 331...  Training loss: 2.8279...  2.2079 sec/batch\n",
      "Epoch: 2/20...  Training Step: 332...  Training loss: 2.8166...  1.9983 sec/batch\n",
      "Epoch: 2/20...  Training Step: 333...  Training loss: 2.7785...  2.0901 sec/batch\n",
      "Epoch: 2/20...  Training Step: 334...  Training loss: 2.7854...  1.9853 sec/batch\n",
      "Epoch: 2/20...  Training Step: 335...  Training loss: 2.8000...  1.9788 sec/batch\n",
      "Epoch: 2/20...  Training Step: 336...  Training loss: 2.7861...  2.1076 sec/batch\n",
      "Epoch: 2/20...  Training Step: 337...  Training loss: 2.8037...  1.9838 sec/batch\n",
      "Epoch: 2/20...  Training Step: 338...  Training loss: 2.7965...  2.0755 sec/batch\n",
      "Epoch: 2/20...  Training Step: 339...  Training loss: 2.8308...  1.9818 sec/batch\n",
      "Epoch: 2/20...  Training Step: 340...  Training loss: 2.7848...  2.0620 sec/batch\n",
      "Epoch: 2/20...  Training Step: 341...  Training loss: 2.7840...  1.9387 sec/batch\n",
      "Epoch: 2/20...  Training Step: 342...  Training loss: 2.7869...  1.7888 sec/batch\n",
      "Epoch: 2/20...  Training Step: 343...  Training loss: 2.7859...  1.8153 sec/batch\n",
      "Epoch: 2/20...  Training Step: 344...  Training loss: 2.8022...  1.7852 sec/batch\n",
      "Epoch: 2/20...  Training Step: 345...  Training loss: 2.7848...  1.8143 sec/batch\n",
      "Epoch: 2/20...  Training Step: 346...  Training loss: 2.8057...  1.7161 sec/batch\n",
      "Epoch: 2/20...  Training Step: 347...  Training loss: 2.7643...  1.7567 sec/batch\n",
      "Epoch: 2/20...  Training Step: 348...  Training loss: 2.7685...  1.8106 sec/batch\n",
      "Epoch: 2/20...  Training Step: 349...  Training loss: 2.8112...  1.7580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 350...  Training loss: 2.8083...  1.8149 sec/batch\n",
      "Epoch: 2/20...  Training Step: 351...  Training loss: 2.7739...  1.7694 sec/batch\n",
      "Epoch: 2/20...  Training Step: 352...  Training loss: 2.7797...  1.8575 sec/batch\n",
      "Epoch: 2/20...  Training Step: 353...  Training loss: 2.7617...  1.7482 sec/batch\n",
      "Epoch: 2/20...  Training Step: 354...  Training loss: 2.7640...  1.7509 sec/batch\n",
      "Epoch: 2/20...  Training Step: 355...  Training loss: 2.7471...  1.8376 sec/batch\n",
      "Epoch: 2/20...  Training Step: 356...  Training loss: 2.7442...  1.7692 sec/batch\n",
      "Epoch: 2/20...  Training Step: 357...  Training loss: 2.7211...  1.8043 sec/batch\n",
      "Epoch: 2/20...  Training Step: 358...  Training loss: 2.7557...  1.7222 sec/batch\n",
      "Epoch: 2/20...  Training Step: 359...  Training loss: 2.7443...  1.7553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 360...  Training loss: 2.7165...  1.7662 sec/batch\n",
      "Epoch: 2/20...  Training Step: 361...  Training loss: 2.7082...  1.7442 sec/batch\n",
      "Epoch: 2/20...  Training Step: 362...  Training loss: 2.7246...  1.7622 sec/batch\n",
      "Epoch: 2/20...  Training Step: 363...  Training loss: 2.7325...  1.6875 sec/batch\n",
      "Epoch: 2/20...  Training Step: 364...  Training loss: 2.7305...  1.7291 sec/batch\n",
      "Epoch: 2/20...  Training Step: 365...  Training loss: 2.7478...  1.7912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 366...  Training loss: 2.7268...  1.7582 sec/batch\n",
      "Epoch: 2/20...  Training Step: 367...  Training loss: 2.7247...  1.7828 sec/batch\n",
      "Epoch: 2/20...  Training Step: 368...  Training loss: 2.6909...  1.7431 sec/batch\n",
      "Epoch: 2/20...  Training Step: 369...  Training loss: 2.7340...  1.7671 sec/batch\n",
      "Epoch: 2/20...  Training Step: 370...  Training loss: 2.7562...  1.6955 sec/batch\n",
      "Epoch: 2/20...  Training Step: 371...  Training loss: 2.7653...  1.7346 sec/batch\n",
      "Epoch: 2/20...  Training Step: 372...  Training loss: 2.7639...  1.8464 sec/batch\n",
      "Epoch: 2/20...  Training Step: 373...  Training loss: 2.7453...  1.7639 sec/batch\n",
      "Epoch: 2/20...  Training Step: 374...  Training loss: 2.7134...  1.7988 sec/batch\n",
      "Epoch: 2/20...  Training Step: 375...  Training loss: 2.6990...  1.7143 sec/batch\n",
      "Epoch: 2/20...  Training Step: 376...  Training loss: 2.6725...  1.7271 sec/batch\n",
      "Epoch: 2/20...  Training Step: 377...  Training loss: 2.6718...  1.7853 sec/batch\n",
      "Epoch: 2/20...  Training Step: 378...  Training loss: 2.6675...  1.7512 sec/batch\n",
      "Epoch: 2/20...  Training Step: 379...  Training loss: 2.6832...  1.8334 sec/batch\n",
      "Epoch: 2/20...  Training Step: 380...  Training loss: 2.6824...  1.7256 sec/batch\n",
      "Epoch: 2/20...  Training Step: 381...  Training loss: 2.6746...  1.7647 sec/batch\n",
      "Epoch: 2/20...  Training Step: 382...  Training loss: 2.6957...  1.8271 sec/batch\n",
      "Epoch: 2/20...  Training Step: 383...  Training loss: 2.7325...  1.7608 sec/batch\n",
      "Epoch: 2/20...  Training Step: 384...  Training loss: 2.6800...  1.8480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 385...  Training loss: 2.6575...  1.7733 sec/batch\n",
      "Epoch: 2/20...  Training Step: 386...  Training loss: 2.6543...  1.8274 sec/batch\n",
      "Epoch: 2/20...  Training Step: 387...  Training loss: 2.6532...  1.7368 sec/batch\n",
      "Epoch: 2/20...  Training Step: 388...  Training loss: 2.6579...  1.7506 sec/batch\n",
      "Epoch: 2/20...  Training Step: 389...  Training loss: 2.6653...  1.8610 sec/batch\n",
      "Epoch: 2/20...  Training Step: 390...  Training loss: 2.6314...  1.7612 sec/batch\n",
      "Epoch: 2/20...  Training Step: 391...  Training loss: 2.6530...  1.9261 sec/batch\n",
      "Epoch: 2/20...  Training Step: 392...  Training loss: 2.6423...  1.7296 sec/batch\n",
      "Epoch: 2/20...  Training Step: 393...  Training loss: 2.6422...  1.7971 sec/batch\n",
      "Epoch: 2/20...  Training Step: 394...  Training loss: 2.6422...  1.8289 sec/batch\n",
      "Epoch: 2/20...  Training Step: 395...  Training loss: 2.6236...  1.7789 sec/batch\n",
      "Epoch: 2/20...  Training Step: 396...  Training loss: 2.6206...  1.8669 sec/batch\n",
      "Epoch: 3/20...  Training Step: 397...  Training loss: 2.7129...  1.7915 sec/batch\n",
      "Epoch: 3/20...  Training Step: 398...  Training loss: 2.5981...  1.8477 sec/batch\n",
      "Epoch: 3/20...  Training Step: 399...  Training loss: 2.6059...  1.7808 sec/batch\n",
      "Epoch: 3/20...  Training Step: 400...  Training loss: 2.6162...  1.8169 sec/batch\n",
      "Epoch: 3/20...  Training Step: 401...  Training loss: 2.6022...  1.6524 sec/batch\n",
      "Epoch: 3/20...  Training Step: 402...  Training loss: 2.6163...  1.6820 sec/batch\n",
      "Epoch: 3/20...  Training Step: 403...  Training loss: 2.6141...  1.8164 sec/batch\n",
      "Epoch: 3/20...  Training Step: 404...  Training loss: 2.6167...  1.7632 sec/batch\n",
      "Epoch: 3/20...  Training Step: 405...  Training loss: 2.6020...  1.7707 sec/batch\n",
      "Epoch: 3/20...  Training Step: 406...  Training loss: 2.5861...  1.8745 sec/batch\n",
      "Epoch: 3/20...  Training Step: 407...  Training loss: 2.5986...  1.7953 sec/batch\n",
      "Epoch: 3/20...  Training Step: 408...  Training loss: 2.6133...  1.8981 sec/batch\n",
      "Epoch: 3/20...  Training Step: 409...  Training loss: 2.5788...  1.7497 sec/batch\n",
      "Epoch: 3/20...  Training Step: 410...  Training loss: 2.6243...  1.7914 sec/batch\n",
      "Epoch: 3/20...  Training Step: 411...  Training loss: 2.5996...  1.8099 sec/batch\n",
      "Epoch: 3/20...  Training Step: 412...  Training loss: 2.5871...  1.7627 sec/batch\n",
      "Epoch: 3/20...  Training Step: 413...  Training loss: 2.5920...  1.8595 sec/batch\n",
      "Epoch: 3/20...  Training Step: 414...  Training loss: 2.6129...  1.8046 sec/batch\n",
      "Epoch: 3/20...  Training Step: 415...  Training loss: 2.5839...  1.8324 sec/batch\n",
      "Epoch: 3/20...  Training Step: 416...  Training loss: 2.5391...  1.7562 sec/batch\n",
      "Epoch: 3/20...  Training Step: 417...  Training loss: 2.5688...  1.7708 sec/batch\n",
      "Epoch: 3/20...  Training Step: 418...  Training loss: 2.6040...  1.8722 sec/batch\n",
      "Epoch: 3/20...  Training Step: 419...  Training loss: 2.5666...  1.7859 sec/batch\n",
      "Epoch: 3/20...  Training Step: 420...  Training loss: 2.5595...  1.8785 sec/batch\n",
      "Epoch: 3/20...  Training Step: 421...  Training loss: 2.5398...  1.7608 sec/batch\n",
      "Epoch: 3/20...  Training Step: 422...  Training loss: 2.5542...  1.7898 sec/batch\n",
      "Epoch: 3/20...  Training Step: 423...  Training loss: 2.5286...  1.8645 sec/batch\n",
      "Epoch: 3/20...  Training Step: 424...  Training loss: 2.5305...  1.7965 sec/batch\n",
      "Epoch: 3/20...  Training Step: 425...  Training loss: 2.5416...  1.8730 sec/batch\n",
      "Epoch: 3/20...  Training Step: 426...  Training loss: 2.5303...  1.7712 sec/batch\n",
      "Epoch: 3/20...  Training Step: 427...  Training loss: 2.5504...  1.7587 sec/batch\n",
      "Epoch: 3/20...  Training Step: 428...  Training loss: 2.5175...  1.8459 sec/batch\n",
      "Epoch: 3/20...  Training Step: 429...  Training loss: 2.5142...  1.7798 sec/batch\n",
      "Epoch: 3/20...  Training Step: 430...  Training loss: 2.5308...  1.8319 sec/batch\n",
      "Epoch: 3/20...  Training Step: 431...  Training loss: 2.5013...  1.7662 sec/batch\n",
      "Epoch: 3/20...  Training Step: 432...  Training loss: 2.5240...  1.8460 sec/batch\n",
      "Epoch: 3/20...  Training Step: 433...  Training loss: 2.5018...  1.7572 sec/batch\n",
      "Epoch: 3/20...  Training Step: 434...  Training loss: 2.4861...  1.7663 sec/batch\n",
      "Epoch: 3/20...  Training Step: 435...  Training loss: 2.4924...  1.8527 sec/batch\n",
      "Epoch: 3/20...  Training Step: 436...  Training loss: 2.4800...  1.7879 sec/batch\n",
      "Epoch: 3/20...  Training Step: 437...  Training loss: 2.4848...  1.8690 sec/batch\n",
      "Epoch: 3/20...  Training Step: 438...  Training loss: 2.4836...  1.7567 sec/batch\n",
      "Epoch: 3/20...  Training Step: 439...  Training loss: 2.4713...  1.7632 sec/batch\n",
      "Epoch: 3/20...  Training Step: 440...  Training loss: 2.4739...  1.8590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 441...  Training loss: 2.4709...  1.7949 sec/batch\n",
      "Epoch: 3/20...  Training Step: 442...  Training loss: 2.4440...  1.8284 sec/batch\n",
      "Epoch: 3/20...  Training Step: 443...  Training loss: 2.4890...  1.7602 sec/batch\n",
      "Epoch: 3/20...  Training Step: 444...  Training loss: 2.4771...  1.8867 sec/batch\n",
      "Epoch: 3/20...  Training Step: 445...  Training loss: 2.4689...  1.7477 sec/batch\n",
      "Epoch: 3/20...  Training Step: 446...  Training loss: 2.4993...  1.7858 sec/batch\n",
      "Epoch: 3/20...  Training Step: 447...  Training loss: 2.4560...  1.8882 sec/batch\n",
      "Epoch: 3/20...  Training Step: 448...  Training loss: 2.4702...  1.8139 sec/batch\n",
      "Epoch: 3/20...  Training Step: 449...  Training loss: 2.4577...  1.8318 sec/batch\n",
      "Epoch: 3/20...  Training Step: 450...  Training loss: 2.4470...  1.7266 sec/batch\n",
      "Epoch: 3/20...  Training Step: 451...  Training loss: 2.4530...  1.7788 sec/batch\n",
      "Epoch: 3/20...  Training Step: 452...  Training loss: 2.4602...  1.8946 sec/batch\n",
      "Epoch: 3/20...  Training Step: 453...  Training loss: 2.4505...  1.8132 sec/batch\n",
      "Epoch: 3/20...  Training Step: 454...  Training loss: 2.4411...  1.8384 sec/batch\n",
      "Epoch: 3/20...  Training Step: 455...  Training loss: 2.4401...  1.7364 sec/batch\n",
      "Epoch: 3/20...  Training Step: 456...  Training loss: 2.4622...  1.7683 sec/batch\n",
      "Epoch: 3/20...  Training Step: 457...  Training loss: 2.4504...  1.8667 sec/batch\n",
      "Epoch: 3/20...  Training Step: 458...  Training loss: 2.4549...  1.7767 sec/batch\n",
      "Epoch: 3/20...  Training Step: 459...  Training loss: 2.4624...  1.8645 sec/batch\n",
      "Epoch: 3/20...  Training Step: 460...  Training loss: 2.4239...  1.7993 sec/batch\n",
      "Epoch: 3/20...  Training Step: 461...  Training loss: 2.4167...  1.8200 sec/batch\n",
      "Epoch: 3/20...  Training Step: 462...  Training loss: 2.4490...  1.7301 sec/batch\n",
      "Epoch: 3/20...  Training Step: 463...  Training loss: 2.4381...  1.7756 sec/batch\n",
      "Epoch: 3/20...  Training Step: 464...  Training loss: 2.4022...  1.8445 sec/batch\n",
      "Epoch: 3/20...  Training Step: 465...  Training loss: 2.4090...  1.8389 sec/batch\n",
      "Epoch: 3/20...  Training Step: 466...  Training loss: 2.4422...  1.8813 sec/batch\n",
      "Epoch: 3/20...  Training Step: 467...  Training loss: 2.4441...  1.7322 sec/batch\n",
      "Epoch: 3/20...  Training Step: 468...  Training loss: 2.4503...  1.7834 sec/batch\n",
      "Epoch: 3/20...  Training Step: 469...  Training loss: 2.4288...  1.8460 sec/batch\n",
      "Epoch: 3/20...  Training Step: 470...  Training loss: 2.4220...  1.7855 sec/batch\n",
      "Epoch: 3/20...  Training Step: 471...  Training loss: 2.4135...  1.8400 sec/batch\n",
      "Epoch: 3/20...  Training Step: 472...  Training loss: 2.4604...  1.7647 sec/batch\n",
      "Epoch: 3/20...  Training Step: 473...  Training loss: 2.4238...  1.7517 sec/batch\n",
      "Epoch: 3/20...  Training Step: 474...  Training loss: 2.4254...  1.8700 sec/batch\n",
      "Epoch: 3/20...  Training Step: 475...  Training loss: 2.4064...  1.7674 sec/batch\n",
      "Epoch: 3/20...  Training Step: 476...  Training loss: 2.4051...  1.8184 sec/batch\n",
      "Epoch: 3/20...  Training Step: 477...  Training loss: 2.3961...  1.7838 sec/batch\n",
      "Epoch: 3/20...  Training Step: 478...  Training loss: 2.4245...  1.8602 sec/batch\n",
      "Epoch: 3/20...  Training Step: 479...  Training loss: 2.3902...  1.7523 sec/batch\n",
      "Epoch: 3/20...  Training Step: 480...  Training loss: 2.3780...  1.7662 sec/batch\n",
      "Epoch: 3/20...  Training Step: 481...  Training loss: 2.3637...  1.8499 sec/batch\n",
      "Epoch: 3/20...  Training Step: 482...  Training loss: 2.3863...  1.7978 sec/batch\n",
      "Epoch: 3/20...  Training Step: 483...  Training loss: 2.3880...  1.8575 sec/batch\n",
      "Epoch: 3/20...  Training Step: 484...  Training loss: 2.3917...  1.7614 sec/batch\n",
      "Epoch: 3/20...  Training Step: 485...  Training loss: 2.3757...  1.7684 sec/batch\n",
      "Epoch: 3/20...  Training Step: 486...  Training loss: 2.3964...  1.7793 sec/batch\n",
      "Epoch: 3/20...  Training Step: 487...  Training loss: 2.3862...  1.7667 sec/batch\n",
      "Epoch: 3/20...  Training Step: 488...  Training loss: 2.3913...  1.7803 sec/batch\n",
      "Epoch: 3/20...  Training Step: 489...  Training loss: 2.3850...  1.7331 sec/batch\n",
      "Epoch: 3/20...  Training Step: 490...  Training loss: 2.3792...  1.7924 sec/batch\n",
      "Epoch: 3/20...  Training Step: 491...  Training loss: 2.3613...  1.6885 sec/batch\n",
      "Epoch: 3/20...  Training Step: 492...  Training loss: 2.3739...  1.7312 sec/batch\n",
      "Epoch: 3/20...  Training Step: 493...  Training loss: 2.3760...  1.8259 sec/batch\n",
      "Epoch: 3/20...  Training Step: 494...  Training loss: 2.3781...  1.7662 sec/batch\n",
      "Epoch: 3/20...  Training Step: 495...  Training loss: 2.3779...  1.8184 sec/batch\n",
      "Epoch: 3/20...  Training Step: 496...  Training loss: 2.3606...  1.7307 sec/batch\n",
      "Epoch: 3/20...  Training Step: 497...  Training loss: 2.3957...  1.7482 sec/batch\n",
      "Epoch: 3/20...  Training Step: 498...  Training loss: 2.3754...  1.8409 sec/batch\n",
      "Epoch: 3/20...  Training Step: 499...  Training loss: 2.3472...  1.7833 sec/batch\n",
      "Epoch: 3/20...  Training Step: 500...  Training loss: 2.3580...  1.8471 sec/batch\n",
      "Epoch: 3/20...  Training Step: 501...  Training loss: 2.3482...  1.7436 sec/batch\n",
      "Epoch: 3/20...  Training Step: 502...  Training loss: 2.3665...  1.7809 sec/batch\n",
      "Epoch: 3/20...  Training Step: 503...  Training loss: 2.3550...  1.8505 sec/batch\n",
      "Epoch: 3/20...  Training Step: 504...  Training loss: 2.3888...  1.7899 sec/batch\n",
      "Epoch: 3/20...  Training Step: 505...  Training loss: 2.3724...  1.8189 sec/batch\n",
      "Epoch: 3/20...  Training Step: 506...  Training loss: 2.3351...  1.7698 sec/batch\n",
      "Epoch: 3/20...  Training Step: 507...  Training loss: 2.3563...  1.8144 sec/batch\n",
      "Epoch: 3/20...  Training Step: 508...  Training loss: 2.3785...  1.7452 sec/batch\n",
      "Epoch: 3/20...  Training Step: 509...  Training loss: 2.3475...  1.7512 sec/batch\n",
      "Epoch: 3/20...  Training Step: 510...  Training loss: 2.3445...  1.8475 sec/batch\n",
      "Epoch: 3/20...  Training Step: 511...  Training loss: 2.3577...  1.7723 sec/batch\n",
      "Epoch: 3/20...  Training Step: 512...  Training loss: 2.3261...  1.8412 sec/batch\n",
      "Epoch: 3/20...  Training Step: 513...  Training loss: 2.3474...  1.7327 sec/batch\n",
      "Epoch: 3/20...  Training Step: 514...  Training loss: 2.3496...  1.7750 sec/batch\n",
      "Epoch: 3/20...  Training Step: 515...  Training loss: 2.3901...  1.8319 sec/batch\n",
      "Epoch: 3/20...  Training Step: 516...  Training loss: 2.3511...  1.8060 sec/batch\n",
      "Epoch: 3/20...  Training Step: 517...  Training loss: 2.3760...  1.8680 sec/batch\n",
      "Epoch: 3/20...  Training Step: 518...  Training loss: 2.3496...  1.7564 sec/batch\n",
      "Epoch: 3/20...  Training Step: 519...  Training loss: 2.3278...  1.7637 sec/batch\n",
      "Epoch: 3/20...  Training Step: 520...  Training loss: 2.3538...  1.8320 sec/batch\n",
      "Epoch: 3/20...  Training Step: 521...  Training loss: 2.3476...  1.7682 sec/batch\n",
      "Epoch: 3/20...  Training Step: 522...  Training loss: 2.3225...  1.8745 sec/batch\n",
      "Epoch: 3/20...  Training Step: 523...  Training loss: 2.3373...  1.7973 sec/batch\n",
      "Epoch: 3/20...  Training Step: 524...  Training loss: 2.3580...  1.8710 sec/batch\n",
      "Epoch: 3/20...  Training Step: 525...  Training loss: 2.3353...  1.7797 sec/batch\n",
      "Epoch: 3/20...  Training Step: 526...  Training loss: 2.3456...  1.8078 sec/batch\n",
      "Epoch: 3/20...  Training Step: 527...  Training loss: 2.3371...  1.8178 sec/batch\n",
      "Epoch: 3/20...  Training Step: 528...  Training loss: 2.3048...  1.7712 sec/batch\n",
      "Epoch: 3/20...  Training Step: 529...  Training loss: 2.3526...  1.7929 sec/batch\n",
      "Epoch: 3/20...  Training Step: 530...  Training loss: 2.3419...  1.7055 sec/batch\n",
      "Epoch: 3/20...  Training Step: 531...  Training loss: 2.3180...  1.7582 sec/batch\n",
      "Epoch: 3/20...  Training Step: 532...  Training loss: 2.3216...  1.8299 sec/batch\n",
      "Epoch: 3/20...  Training Step: 533...  Training loss: 2.3207...  1.7743 sec/batch\n",
      "Epoch: 3/20...  Training Step: 534...  Training loss: 2.3237...  1.8369 sec/batch\n",
      "Epoch: 3/20...  Training Step: 535...  Training loss: 2.3462...  1.7422 sec/batch\n",
      "Epoch: 3/20...  Training Step: 536...  Training loss: 2.3191...  1.8515 sec/batch\n",
      "Epoch: 3/20...  Training Step: 537...  Training loss: 2.3326...  1.7567 sec/batch\n",
      "Epoch: 3/20...  Training Step: 538...  Training loss: 2.3132...  1.7793 sec/batch\n",
      "Epoch: 3/20...  Training Step: 539...  Training loss: 2.3225...  1.8500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 540...  Training loss: 2.3089...  1.7517 sec/batch\n",
      "Epoch: 3/20...  Training Step: 541...  Training loss: 2.3039...  1.7823 sec/batch\n",
      "Epoch: 3/20...  Training Step: 542...  Training loss: 2.3566...  1.6795 sec/batch\n",
      "Epoch: 3/20...  Training Step: 543...  Training loss: 2.3131...  1.7577 sec/batch\n",
      "Epoch: 3/20...  Training Step: 544...  Training loss: 2.3376...  1.7898 sec/batch\n",
      "Epoch: 3/20...  Training Step: 545...  Training loss: 2.3061...  1.7787 sec/batch\n",
      "Epoch: 3/20...  Training Step: 546...  Training loss: 2.3022...  1.7908 sec/batch\n",
      "Epoch: 3/20...  Training Step: 547...  Training loss: 2.3446...  1.7076 sec/batch\n",
      "Epoch: 3/20...  Training Step: 548...  Training loss: 2.3562...  1.7121 sec/batch\n",
      "Epoch: 3/20...  Training Step: 549...  Training loss: 2.3175...  1.8379 sec/batch\n",
      "Epoch: 3/20...  Training Step: 550...  Training loss: 2.3238...  1.7788 sec/batch\n",
      "Epoch: 3/20...  Training Step: 551...  Training loss: 2.2980...  1.8688 sec/batch\n",
      "Epoch: 3/20...  Training Step: 552...  Training loss: 2.3069...  1.7686 sec/batch\n",
      "Epoch: 3/20...  Training Step: 553...  Training loss: 2.2984...  1.8400 sec/batch\n",
      "Epoch: 3/20...  Training Step: 554...  Training loss: 2.2952...  1.7487 sec/batch\n",
      "Epoch: 3/20...  Training Step: 555...  Training loss: 2.2741...  1.7853 sec/batch\n",
      "Epoch: 3/20...  Training Step: 556...  Training loss: 2.3263...  1.8500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 557...  Training loss: 2.3101...  1.7869 sec/batch\n",
      "Epoch: 3/20...  Training Step: 558...  Training loss: 2.2840...  1.8399 sec/batch\n",
      "Epoch: 3/20...  Training Step: 559...  Training loss: 2.2818...  1.7422 sec/batch\n",
      "Epoch: 3/20...  Training Step: 560...  Training loss: 2.2846...  1.7540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 561...  Training loss: 2.3028...  1.8374 sec/batch\n",
      "Epoch: 3/20...  Training Step: 562...  Training loss: 2.2909...  1.7788 sec/batch\n",
      "Epoch: 3/20...  Training Step: 563...  Training loss: 2.2972...  1.8279 sec/batch\n",
      "Epoch: 3/20...  Training Step: 564...  Training loss: 2.2981...  1.7178 sec/batch\n",
      "Epoch: 3/20...  Training Step: 565...  Training loss: 2.2957...  1.7733 sec/batch\n",
      "Epoch: 3/20...  Training Step: 566...  Training loss: 2.2799...  1.8369 sec/batch\n",
      "Epoch: 3/20...  Training Step: 567...  Training loss: 2.3060...  1.7891 sec/batch\n",
      "Epoch: 3/20...  Training Step: 568...  Training loss: 2.3079...  1.8147 sec/batch\n",
      "Epoch: 3/20...  Training Step: 569...  Training loss: 2.3356...  1.7768 sec/batch\n",
      "Epoch: 3/20...  Training Step: 570...  Training loss: 2.3272...  1.8606 sec/batch\n",
      "Epoch: 3/20...  Training Step: 571...  Training loss: 2.3230...  1.7367 sec/batch\n",
      "Epoch: 3/20...  Training Step: 572...  Training loss: 2.2874...  1.7682 sec/batch\n",
      "Epoch: 3/20...  Training Step: 573...  Training loss: 2.2874...  1.8319 sec/batch\n",
      "Epoch: 3/20...  Training Step: 574...  Training loss: 2.2708...  1.7657 sec/batch\n",
      "Epoch: 3/20...  Training Step: 575...  Training loss: 2.2777...  1.8670 sec/batch\n",
      "Epoch: 3/20...  Training Step: 576...  Training loss: 2.2489...  1.7264 sec/batch\n",
      "Epoch: 3/20...  Training Step: 577...  Training loss: 2.2754...  1.7618 sec/batch\n",
      "Epoch: 3/20...  Training Step: 578...  Training loss: 2.2932...  1.8079 sec/batch\n",
      "Epoch: 3/20...  Training Step: 579...  Training loss: 2.2660...  1.8045 sec/batch\n",
      "Epoch: 3/20...  Training Step: 580...  Training loss: 2.3178...  1.8655 sec/batch\n",
      "Epoch: 3/20...  Training Step: 581...  Training loss: 2.2945...  1.7590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 582...  Training loss: 2.2761...  1.7386 sec/batch\n",
      "Epoch: 3/20...  Training Step: 583...  Training loss: 2.2666...  1.7507 sec/batch\n",
      "Epoch: 3/20...  Training Step: 584...  Training loss: 2.2501...  1.7752 sec/batch\n",
      "Epoch: 3/20...  Training Step: 585...  Training loss: 2.2623...  1.8464 sec/batch\n",
      "Epoch: 3/20...  Training Step: 586...  Training loss: 2.2668...  1.7823 sec/batch\n",
      "Epoch: 3/20...  Training Step: 587...  Training loss: 2.2831...  1.8858 sec/batch\n",
      "Epoch: 3/20...  Training Step: 588...  Training loss: 2.2445...  1.7352 sec/batch\n",
      "Epoch: 3/20...  Training Step: 589...  Training loss: 2.2756...  1.7698 sec/batch\n",
      "Epoch: 3/20...  Training Step: 590...  Training loss: 2.2567...  1.8541 sec/batch\n",
      "Epoch: 3/20...  Training Step: 591...  Training loss: 2.2408...  1.7928 sec/batch\n",
      "Epoch: 3/20...  Training Step: 592...  Training loss: 2.2556...  1.8419 sec/batch\n",
      "Epoch: 3/20...  Training Step: 593...  Training loss: 2.2612...  1.7622 sec/batch\n",
      "Epoch: 3/20...  Training Step: 594...  Training loss: 2.2481...  1.7788 sec/batch\n",
      "Epoch: 4/20...  Training Step: 595...  Training loss: 2.3516...  1.8543 sec/batch\n",
      "Epoch: 4/20...  Training Step: 596...  Training loss: 2.2258...  1.7826 sec/batch\n",
      "Epoch: 4/20...  Training Step: 597...  Training loss: 2.2158...  1.8389 sec/batch\n",
      "Epoch: 4/20...  Training Step: 598...  Training loss: 2.2396...  1.7682 sec/batch\n",
      "Epoch: 4/20...  Training Step: 599...  Training loss: 2.2375...  1.8656 sec/batch\n",
      "Epoch: 4/20...  Training Step: 600...  Training loss: 2.2402...  1.7562 sec/batch\n",
      "Epoch: 4/20...  Training Step: 601...  Training loss: 2.2447...  1.6965 sec/batch\n",
      "Epoch: 4/20...  Training Step: 602...  Training loss: 2.2543...  1.7246 sec/batch\n",
      "Epoch: 4/20...  Training Step: 603...  Training loss: 2.2875...  1.7056 sec/batch\n",
      "Epoch: 4/20...  Training Step: 604...  Training loss: 2.2456...  1.8038 sec/batch\n",
      "Epoch: 4/20...  Training Step: 605...  Training loss: 2.2387...  1.7110 sec/batch\n",
      "Epoch: 4/20...  Training Step: 606...  Training loss: 2.2434...  1.7578 sec/batch\n",
      "Epoch: 4/20...  Training Step: 607...  Training loss: 2.2523...  1.8613 sec/batch\n",
      "Epoch: 4/20...  Training Step: 608...  Training loss: 2.2801...  1.7838 sec/batch\n",
      "Epoch: 4/20...  Training Step: 609...  Training loss: 2.2486...  1.8008 sec/batch\n",
      "Epoch: 4/20...  Training Step: 610...  Training loss: 2.2429...  1.7337 sec/batch\n",
      "Epoch: 4/20...  Training Step: 611...  Training loss: 2.2404...  1.7514 sec/batch\n",
      "Epoch: 4/20...  Training Step: 612...  Training loss: 2.2817...  1.8168 sec/batch\n",
      "Epoch: 4/20...  Training Step: 613...  Training loss: 2.2588...  1.7564 sec/batch\n",
      "Epoch: 4/20...  Training Step: 614...  Training loss: 2.2203...  1.8034 sec/batch\n",
      "Epoch: 4/20...  Training Step: 615...  Training loss: 2.2205...  1.7422 sec/batch\n",
      "Epoch: 4/20...  Training Step: 616...  Training loss: 2.2931...  1.8167 sec/batch\n",
      "Epoch: 4/20...  Training Step: 617...  Training loss: 2.2400...  1.7136 sec/batch\n",
      "Epoch: 4/20...  Training Step: 618...  Training loss: 2.2320...  1.7622 sec/batch\n",
      "Epoch: 4/20...  Training Step: 619...  Training loss: 2.2349...  1.8259 sec/batch\n",
      "Epoch: 4/20...  Training Step: 620...  Training loss: 2.2294...  1.7883 sec/batch\n",
      "Epoch: 4/20...  Training Step: 621...  Training loss: 2.2193...  1.9840 sec/batch\n",
      "Epoch: 4/20...  Training Step: 622...  Training loss: 2.2317...  2.1131 sec/batch\n",
      "Epoch: 4/20...  Training Step: 623...  Training loss: 2.2622...  2.1237 sec/batch\n",
      "Epoch: 4/20...  Training Step: 624...  Training loss: 2.2407...  2.1410 sec/batch\n",
      "Epoch: 4/20...  Training Step: 625...  Training loss: 2.2555...  1.9322 sec/batch\n",
      "Epoch: 4/20...  Training Step: 626...  Training loss: 2.2091...  1.9783 sec/batch\n",
      "Epoch: 4/20...  Training Step: 627...  Training loss: 2.2234...  2.0809 sec/batch\n",
      "Epoch: 4/20...  Training Step: 628...  Training loss: 2.2524...  2.1623 sec/batch\n",
      "Epoch: 4/20...  Training Step: 629...  Training loss: 2.2124...  2.1877 sec/batch\n",
      "Epoch: 4/20...  Training Step: 630...  Training loss: 2.2309...  1.9854 sec/batch\n",
      "Epoch: 4/20...  Training Step: 631...  Training loss: 2.2227...  2.0595 sec/batch\n",
      "Epoch: 4/20...  Training Step: 632...  Training loss: 2.1934...  2.0653 sec/batch\n",
      "Epoch: 4/20...  Training Step: 633...  Training loss: 2.2039...  1.9950 sec/batch\n",
      "Epoch: 4/20...  Training Step: 634...  Training loss: 2.1848...  1.8827 sec/batch\n",
      "Epoch: 4/20...  Training Step: 635...  Training loss: 2.2033...  1.8935 sec/batch\n",
      "Epoch: 4/20...  Training Step: 636...  Training loss: 2.2085...  1.9750 sec/batch\n",
      "Epoch: 4/20...  Training Step: 637...  Training loss: 2.1860...  1.9724 sec/batch\n",
      "Epoch: 4/20...  Training Step: 638...  Training loss: 2.1918...  2.0817 sec/batch\n",
      "Epoch: 4/20...  Training Step: 639...  Training loss: 2.2073...  1.8795 sec/batch\n",
      "Epoch: 4/20...  Training Step: 640...  Training loss: 2.1560...  1.8956 sec/batch\n",
      "Epoch: 4/20...  Training Step: 641...  Training loss: 2.2227...  2.0045 sec/batch\n",
      "Epoch: 4/20...  Training Step: 642...  Training loss: 2.1971...  1.9352 sec/batch\n",
      "Epoch: 4/20...  Training Step: 643...  Training loss: 2.1964...  2.1498 sec/batch\n",
      "Epoch: 4/20...  Training Step: 644...  Training loss: 2.2393...  1.9609 sec/batch\n",
      "Epoch: 4/20...  Training Step: 645...  Training loss: 2.1797...  1.9874 sec/batch\n",
      "Epoch: 4/20...  Training Step: 646...  Training loss: 2.2297...  1.9592 sec/batch\n",
      "Epoch: 4/20...  Training Step: 647...  Training loss: 2.2060...  2.1818 sec/batch\n",
      "Epoch: 4/20...  Training Step: 648...  Training loss: 2.2041...  2.3543 sec/batch\n",
      "Epoch: 4/20...  Training Step: 649...  Training loss: 2.1955...  2.1925 sec/batch\n",
      "Epoch: 4/20...  Training Step: 650...  Training loss: 2.2273...  1.8945 sec/batch\n",
      "Epoch: 4/20...  Training Step: 651...  Training loss: 2.2006...  2.0059 sec/batch\n",
      "Epoch: 4/20...  Training Step: 652...  Training loss: 2.1877...  2.1262 sec/batch\n",
      "Epoch: 4/20...  Training Step: 653...  Training loss: 2.1969...  2.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 654...  Training loss: 2.1995...  2.4330 sec/batch\n",
      "Epoch: 4/20...  Training Step: 655...  Training loss: 2.2028...  2.3863 sec/batch\n",
      "Epoch: 4/20...  Training Step: 656...  Training loss: 2.2112...  2.0028 sec/batch\n",
      "Epoch: 4/20...  Training Step: 657...  Training loss: 2.2256...  2.4673 sec/batch\n",
      "Epoch: 4/20...  Training Step: 658...  Training loss: 2.1868...  2.7203 sec/batch\n",
      "Epoch: 4/20...  Training Step: 659...  Training loss: 2.1820...  2.7077 sec/batch\n",
      "Epoch: 4/20...  Training Step: 660...  Training loss: 2.2135...  2.6751 sec/batch\n",
      "Epoch: 4/20...  Training Step: 661...  Training loss: 2.2113...  2.3002 sec/batch\n",
      "Epoch: 4/20...  Training Step: 662...  Training loss: 2.1676...  2.4069 sec/batch\n",
      "Epoch: 4/20...  Training Step: 663...  Training loss: 2.1727...  2.0048 sec/batch\n",
      "Epoch: 4/20...  Training Step: 664...  Training loss: 2.1942...  2.0414 sec/batch\n",
      "Epoch: 4/20...  Training Step: 665...  Training loss: 2.2074...  2.0946 sec/batch\n",
      "Epoch: 4/20...  Training Step: 666...  Training loss: 2.2079...  2.0199 sec/batch\n",
      "Epoch: 4/20...  Training Step: 667...  Training loss: 2.2112...  2.1768 sec/batch\n",
      "Epoch: 4/20...  Training Step: 668...  Training loss: 2.1832...  2.0209 sec/batch\n",
      "Epoch: 4/20...  Training Step: 669...  Training loss: 2.1819...  2.0545 sec/batch\n",
      "Epoch: 4/20...  Training Step: 670...  Training loss: 2.2335...  2.0916 sec/batch\n",
      "Epoch: 4/20...  Training Step: 671...  Training loss: 2.1770...  2.0685 sec/batch\n",
      "Epoch: 4/20...  Training Step: 672...  Training loss: 2.1961...  2.1267 sec/batch\n",
      "Epoch: 4/20...  Training Step: 673...  Training loss: 2.1633...  2.0214 sec/batch\n",
      "Epoch: 4/20...  Training Step: 674...  Training loss: 2.1765...  1.9547 sec/batch\n",
      "Epoch: 4/20...  Training Step: 675...  Training loss: 2.1588...  1.9738 sec/batch\n",
      "Epoch: 4/20...  Training Step: 676...  Training loss: 2.1927...  2.1036 sec/batch\n",
      "Epoch: 4/20...  Training Step: 677...  Training loss: 2.1465...  2.1029 sec/batch\n",
      "Epoch: 4/20...  Training Step: 678...  Training loss: 2.1622...  2.0946 sec/batch\n",
      "Epoch: 4/20...  Training Step: 679...  Training loss: 2.1345...  2.1277 sec/batch\n",
      "Epoch: 4/20...  Training Step: 680...  Training loss: 2.1617...  2.0159 sec/batch\n",
      "Epoch: 4/20...  Training Step: 681...  Training loss: 2.1683...  2.0931 sec/batch\n",
      "Epoch: 4/20...  Training Step: 682...  Training loss: 2.1529...  2.1562 sec/batch\n",
      "Epoch: 4/20...  Training Step: 683...  Training loss: 2.1457...  2.0826 sec/batch\n",
      "Epoch: 4/20...  Training Step: 684...  Training loss: 2.1960...  2.0956 sec/batch\n",
      "Epoch: 4/20...  Training Step: 685...  Training loss: 2.1382...  2.0239 sec/batch\n",
      "Epoch: 4/20...  Training Step: 686...  Training loss: 2.1696...  2.0505 sec/batch\n",
      "Epoch: 4/20...  Training Step: 687...  Training loss: 2.1484...  2.0319 sec/batch\n",
      "Epoch: 4/20...  Training Step: 688...  Training loss: 2.1461...  2.0003 sec/batch\n",
      "Epoch: 4/20...  Training Step: 689...  Training loss: 2.1509...  2.0319 sec/batch\n",
      "Epoch: 4/20...  Training Step: 690...  Training loss: 2.1626...  1.9938 sec/batch\n",
      "Epoch: 4/20...  Training Step: 691...  Training loss: 2.1668...  2.0856 sec/batch\n",
      "Epoch: 4/20...  Training Step: 692...  Training loss: 2.1535...  1.9597 sec/batch\n",
      "Epoch: 4/20...  Training Step: 693...  Training loss: 2.1616...  2.0946 sec/batch\n",
      "Epoch: 4/20...  Training Step: 694...  Training loss: 2.1379...  2.1312 sec/batch\n",
      "Epoch: 4/20...  Training Step: 695...  Training loss: 2.1738...  1.9736 sec/batch\n",
      "Epoch: 4/20...  Training Step: 696...  Training loss: 2.1694...  2.1710 sec/batch\n",
      "Epoch: 4/20...  Training Step: 697...  Training loss: 2.1512...  1.9106 sec/batch\n",
      "Epoch: 4/20...  Training Step: 698...  Training loss: 2.1392...  2.1262 sec/batch\n",
      "Epoch: 4/20...  Training Step: 699...  Training loss: 2.1405...  2.1236 sec/batch\n",
      "Epoch: 4/20...  Training Step: 700...  Training loss: 2.1558...  2.0369 sec/batch\n",
      "Epoch: 4/20...  Training Step: 701...  Training loss: 2.1501...  2.1177 sec/batch\n",
      "Epoch: 4/20...  Training Step: 702...  Training loss: 2.1713...  1.9918 sec/batch\n",
      "Epoch: 4/20...  Training Step: 703...  Training loss: 2.1614...  2.1517 sec/batch\n",
      "Epoch: 4/20...  Training Step: 704...  Training loss: 2.1410...  2.1066 sec/batch\n",
      "Epoch: 4/20...  Training Step: 705...  Training loss: 2.1472...  2.0780 sec/batch\n",
      "Epoch: 4/20...  Training Step: 706...  Training loss: 2.1587...  2.1549 sec/batch\n",
      "Epoch: 4/20...  Training Step: 707...  Training loss: 2.1527...  2.0464 sec/batch\n",
      "Epoch: 4/20...  Training Step: 708...  Training loss: 2.1388...  2.1246 sec/batch\n",
      "Epoch: 4/20...  Training Step: 709...  Training loss: 2.1403...  1.9973 sec/batch\n",
      "Epoch: 4/20...  Training Step: 710...  Training loss: 2.1098...  2.0916 sec/batch\n",
      "Epoch: 4/20...  Training Step: 711...  Training loss: 2.1513...  2.1913 sec/batch\n",
      "Epoch: 4/20...  Training Step: 712...  Training loss: 2.1458...  2.1828 sec/batch\n",
      "Epoch: 4/20...  Training Step: 713...  Training loss: 2.1537...  2.4560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 714...  Training loss: 2.1591...  2.0971 sec/batch\n",
      "Epoch: 4/20...  Training Step: 715...  Training loss: 2.1675...  1.9948 sec/batch\n",
      "Epoch: 4/20...  Training Step: 716...  Training loss: 2.1370...  2.1623 sec/batch\n",
      "Epoch: 4/20...  Training Step: 717...  Training loss: 2.1296...  2.0800 sec/batch\n",
      "Epoch: 4/20...  Training Step: 718...  Training loss: 2.1632...  2.1006 sec/batch\n",
      "Epoch: 4/20...  Training Step: 719...  Training loss: 2.1457...  1.9662 sec/batch\n",
      "Epoch: 4/20...  Training Step: 720...  Training loss: 2.1175...  1.9863 sec/batch\n",
      "Epoch: 4/20...  Training Step: 721...  Training loss: 2.1498...  1.9953 sec/batch\n",
      "Epoch: 4/20...  Training Step: 722...  Training loss: 2.1691...  2.0414 sec/batch\n",
      "Epoch: 4/20...  Training Step: 723...  Training loss: 2.1481...  2.1808 sec/batch\n",
      "Epoch: 4/20...  Training Step: 724...  Training loss: 2.1537...  2.1126 sec/batch\n",
      "Epoch: 4/20...  Training Step: 725...  Training loss: 2.1376...  2.0742 sec/batch\n",
      "Epoch: 4/20...  Training Step: 726...  Training loss: 2.1236...  2.0429 sec/batch\n",
      "Epoch: 4/20...  Training Step: 727...  Training loss: 2.1546...  2.0515 sec/batch\n",
      "Epoch: 4/20...  Training Step: 728...  Training loss: 2.1524...  2.1036 sec/batch\n",
      "Epoch: 4/20...  Training Step: 729...  Training loss: 2.1381...  2.0339 sec/batch\n",
      "Epoch: 4/20...  Training Step: 730...  Training loss: 2.1402...  2.0926 sec/batch\n",
      "Epoch: 4/20...  Training Step: 731...  Training loss: 2.1417...  1.9592 sec/batch\n",
      "Epoch: 4/20...  Training Step: 732...  Training loss: 2.1407...  2.0289 sec/batch\n",
      "Epoch: 4/20...  Training Step: 733...  Training loss: 2.1680...  2.1823 sec/batch\n",
      "Epoch: 4/20...  Training Step: 734...  Training loss: 2.1378...  2.1417 sec/batch\n",
      "Epoch: 4/20...  Training Step: 735...  Training loss: 2.1479...  2.2364 sec/batch\n",
      "Epoch: 4/20...  Training Step: 736...  Training loss: 2.1227...  2.0359 sec/batch\n",
      "Epoch: 4/20...  Training Step: 737...  Training loss: 2.1360...  2.0760 sec/batch\n",
      "Epoch: 4/20...  Training Step: 738...  Training loss: 2.1249...  1.9732 sec/batch\n",
      "Epoch: 4/20...  Training Step: 739...  Training loss: 2.1221...  1.9918 sec/batch\n",
      "Epoch: 4/20...  Training Step: 740...  Training loss: 2.1659...  2.0394 sec/batch\n",
      "Epoch: 4/20...  Training Step: 741...  Training loss: 2.1438...  1.9542 sec/batch\n",
      "Epoch: 4/20...  Training Step: 742...  Training loss: 2.1567...  2.0826 sec/batch\n",
      "Epoch: 4/20...  Training Step: 743...  Training loss: 2.1423...  2.0018 sec/batch\n",
      "Epoch: 4/20...  Training Step: 744...  Training loss: 2.1357...  2.1693 sec/batch\n",
      "Epoch: 4/20...  Training Step: 745...  Training loss: 2.1433...  2.1417 sec/batch\n",
      "Epoch: 4/20...  Training Step: 746...  Training loss: 2.1841...  2.0745 sec/batch\n",
      "Epoch: 4/20...  Training Step: 747...  Training loss: 2.1238...  2.1252 sec/batch\n",
      "Epoch: 4/20...  Training Step: 748...  Training loss: 2.1515...  1.9377 sec/batch\n",
      "Epoch: 4/20...  Training Step: 749...  Training loss: 2.1188...  2.0429 sec/batch\n",
      "Epoch: 4/20...  Training Step: 750...  Training loss: 2.1252...  1.9938 sec/batch\n",
      "Epoch: 4/20...  Training Step: 751...  Training loss: 2.1184...  2.0129 sec/batch\n",
      "Epoch: 4/20...  Training Step: 752...  Training loss: 2.1227...  2.0570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 753...  Training loss: 2.1005...  1.9662 sec/batch\n",
      "Epoch: 4/20...  Training Step: 754...  Training loss: 2.1598...  2.0349 sec/batch\n",
      "Epoch: 4/20...  Training Step: 755...  Training loss: 2.1368...  1.8118 sec/batch\n",
      "Epoch: 4/20...  Training Step: 756...  Training loss: 2.1052...  1.9818 sec/batch\n",
      "Epoch: 4/20...  Training Step: 757...  Training loss: 2.1323...  1.9181 sec/batch\n",
      "Epoch: 4/20...  Training Step: 758...  Training loss: 2.1173...  2.0379 sec/batch\n",
      "Epoch: 4/20...  Training Step: 759...  Training loss: 2.1231...  2.0319 sec/batch\n",
      "Epoch: 4/20...  Training Step: 760...  Training loss: 2.1135...  2.0073 sec/batch\n",
      "Epoch: 4/20...  Training Step: 761...  Training loss: 2.1195...  2.0840 sec/batch\n",
      "Epoch: 4/20...  Training Step: 762...  Training loss: 2.1474...  2.1868 sec/batch\n",
      "Epoch: 4/20...  Training Step: 763...  Training loss: 2.1156...  2.0585 sec/batch\n",
      "Epoch: 4/20...  Training Step: 764...  Training loss: 2.1013...  2.0765 sec/batch\n",
      "Epoch: 4/20...  Training Step: 765...  Training loss: 2.1122...  1.9482 sec/batch\n",
      "Epoch: 4/20...  Training Step: 766...  Training loss: 2.1125...  2.0384 sec/batch\n",
      "Epoch: 4/20...  Training Step: 767...  Training loss: 2.1480...  2.0790 sec/batch\n",
      "Epoch: 4/20...  Training Step: 768...  Training loss: 2.1451...  2.1066 sec/batch\n",
      "Epoch: 4/20...  Training Step: 769...  Training loss: 2.1280...  2.2214 sec/batch\n",
      "Epoch: 4/20...  Training Step: 770...  Training loss: 2.1172...  2.0364 sec/batch\n",
      "Epoch: 4/20...  Training Step: 771...  Training loss: 2.1154...  2.0630 sec/batch\n",
      "Epoch: 4/20...  Training Step: 772...  Training loss: 2.1123...  2.0319 sec/batch\n",
      "Epoch: 4/20...  Training Step: 773...  Training loss: 2.0844...  2.0820 sec/batch\n",
      "Epoch: 4/20...  Training Step: 774...  Training loss: 2.0728...  2.1337 sec/batch\n",
      "Epoch: 4/20...  Training Step: 775...  Training loss: 2.0953...  2.0799 sec/batch\n",
      "Epoch: 4/20...  Training Step: 776...  Training loss: 2.1033...  2.1327 sec/batch\n",
      "Epoch: 4/20...  Training Step: 777...  Training loss: 2.1035...  1.9662 sec/batch\n",
      "Epoch: 4/20...  Training Step: 778...  Training loss: 2.1361...  2.0710 sec/batch\n",
      "Epoch: 4/20...  Training Step: 779...  Training loss: 2.1121...  2.1467 sec/batch\n",
      "Epoch: 4/20...  Training Step: 780...  Training loss: 2.0985...  2.0520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 781...  Training loss: 2.0947...  2.0916 sec/batch\n",
      "Epoch: 4/20...  Training Step: 782...  Training loss: 2.0909...  2.1046 sec/batch\n",
      "Epoch: 4/20...  Training Step: 783...  Training loss: 2.1011...  2.1246 sec/batch\n",
      "Epoch: 4/20...  Training Step: 784...  Training loss: 2.0910...  2.0675 sec/batch\n",
      "Epoch: 4/20...  Training Step: 785...  Training loss: 2.1107...  2.0780 sec/batch\n",
      "Epoch: 4/20...  Training Step: 786...  Training loss: 2.0740...  2.1557 sec/batch\n",
      "Epoch: 4/20...  Training Step: 787...  Training loss: 2.0943...  2.0695 sec/batch\n",
      "Epoch: 4/20...  Training Step: 788...  Training loss: 2.0851...  2.2064 sec/batch\n",
      "Epoch: 4/20...  Training Step: 789...  Training loss: 2.0615...  2.0229 sec/batch\n",
      "Epoch: 4/20...  Training Step: 790...  Training loss: 2.0903...  2.0730 sec/batch\n",
      "Epoch: 4/20...  Training Step: 791...  Training loss: 2.0806...  2.1267 sec/batch\n",
      "Epoch: 4/20...  Training Step: 792...  Training loss: 2.0794...  2.1026 sec/batch\n",
      "Epoch: 5/20...  Training Step: 793...  Training loss: 2.1910...  2.1207 sec/batch\n",
      "Epoch: 5/20...  Training Step: 794...  Training loss: 2.0616...  1.9833 sec/batch\n",
      "Epoch: 5/20...  Training Step: 795...  Training loss: 2.0734...  2.0815 sec/batch\n",
      "Epoch: 5/20...  Training Step: 796...  Training loss: 2.0855...  2.1748 sec/batch\n",
      "Epoch: 5/20...  Training Step: 797...  Training loss: 2.0853...  2.1382 sec/batch\n",
      "Epoch: 5/20...  Training Step: 798...  Training loss: 2.0713...  2.1437 sec/batch\n",
      "Epoch: 5/20...  Training Step: 799...  Training loss: 2.0893...  2.1151 sec/batch\n",
      "Epoch: 5/20...  Training Step: 800...  Training loss: 2.0788...  2.3563 sec/batch\n",
      "Epoch: 5/20...  Training Step: 801...  Training loss: 2.1177...  2.0790 sec/batch\n",
      "Epoch: 5/20...  Training Step: 802...  Training loss: 2.0778...  2.0384 sec/batch\n",
      "Epoch: 5/20...  Training Step: 803...  Training loss: 2.0728...  2.0921 sec/batch\n",
      "Epoch: 5/20...  Training Step: 804...  Training loss: 2.0728...  1.9983 sec/batch\n",
      "Epoch: 5/20...  Training Step: 805...  Training loss: 2.0892...  2.0489 sec/batch\n",
      "Epoch: 5/20...  Training Step: 806...  Training loss: 2.1224...  1.9577 sec/batch\n",
      "Epoch: 5/20...  Training Step: 807...  Training loss: 2.0878...  2.0129 sec/batch\n",
      "Epoch: 5/20...  Training Step: 808...  Training loss: 2.0777...  2.4589 sec/batch\n",
      "Epoch: 5/20...  Training Step: 809...  Training loss: 2.0801...  2.2691 sec/batch\n",
      "Epoch: 5/20...  Training Step: 810...  Training loss: 2.1232...  2.1213 sec/batch\n",
      "Epoch: 5/20...  Training Step: 811...  Training loss: 2.0692...  1.8520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 812...  Training loss: 2.0683...  1.7773 sec/batch\n",
      "Epoch: 5/20...  Training Step: 813...  Training loss: 2.0632...  2.0973 sec/batch\n",
      "Epoch: 5/20...  Training Step: 814...  Training loss: 2.1270...  2.1559 sec/batch\n",
      "Epoch: 5/20...  Training Step: 815...  Training loss: 2.0674...  2.0801 sec/batch\n",
      "Epoch: 5/20...  Training Step: 816...  Training loss: 2.0616...  1.9908 sec/batch\n",
      "Epoch: 5/20...  Training Step: 817...  Training loss: 2.0708...  2.2560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 818...  Training loss: 2.0688...  2.1001 sec/batch\n",
      "Epoch: 5/20...  Training Step: 819...  Training loss: 2.0532...  2.2134 sec/batch\n",
      "Epoch: 5/20...  Training Step: 820...  Training loss: 2.0788...  2.2976 sec/batch\n",
      "Epoch: 5/20...  Training Step: 821...  Training loss: 2.0928...  2.6390 sec/batch\n",
      "Epoch: 5/20...  Training Step: 822...  Training loss: 2.0784...  2.3848 sec/batch\n",
      "Epoch: 5/20...  Training Step: 823...  Training loss: 2.0879...  2.3152 sec/batch\n",
      "Epoch: 5/20...  Training Step: 824...  Training loss: 2.0473...  1.9136 sec/batch\n",
      "Epoch: 5/20...  Training Step: 825...  Training loss: 2.0729...  1.9422 sec/batch\n",
      "Epoch: 5/20...  Training Step: 826...  Training loss: 2.0903...  2.3111 sec/batch\n",
      "Epoch: 5/20...  Training Step: 827...  Training loss: 2.0646...  2.3638 sec/batch\n",
      "Epoch: 5/20...  Training Step: 828...  Training loss: 2.0742...  2.3833 sec/batch\n",
      "Epoch: 5/20...  Training Step: 829...  Training loss: 2.0734...  2.3046 sec/batch\n",
      "Epoch: 5/20...  Training Step: 830...  Training loss: 2.0405...  2.1086 sec/batch\n",
      "Epoch: 5/20...  Training Step: 831...  Training loss: 2.0332...  2.1933 sec/batch\n",
      "Epoch: 5/20...  Training Step: 832...  Training loss: 2.0428...  2.1357 sec/batch\n",
      "Epoch: 5/20...  Training Step: 833...  Training loss: 2.0452...  2.1302 sec/batch\n",
      "Epoch: 5/20...  Training Step: 834...  Training loss: 2.0566...  2.2369 sec/batch\n",
      "Epoch: 5/20...  Training Step: 835...  Training loss: 2.0408...  2.0951 sec/batch\n",
      "Epoch: 5/20...  Training Step: 836...  Training loss: 2.0347...  2.1512 sec/batch\n",
      "Epoch: 5/20...  Training Step: 837...  Training loss: 2.0676...  2.1982 sec/batch\n",
      "Epoch: 5/20...  Training Step: 838...  Training loss: 2.0057...  2.0770 sec/batch\n",
      "Epoch: 5/20...  Training Step: 839...  Training loss: 2.0671...  2.1964 sec/batch\n",
      "Epoch: 5/20...  Training Step: 840...  Training loss: 2.0386...  2.0981 sec/batch\n",
      "Epoch: 5/20...  Training Step: 841...  Training loss: 2.0507...  2.0811 sec/batch\n",
      "Epoch: 5/20...  Training Step: 842...  Training loss: 2.0983...  2.3308 sec/batch\n",
      "Epoch: 5/20...  Training Step: 843...  Training loss: 2.0305...  2.2926 sec/batch\n",
      "Epoch: 5/20...  Training Step: 844...  Training loss: 2.0919...  2.3683 sec/batch\n",
      "Epoch: 5/20...  Training Step: 845...  Training loss: 2.0560...  2.1196 sec/batch\n",
      "Epoch: 5/20...  Training Step: 846...  Training loss: 2.0362...  2.1467 sec/batch\n",
      "Epoch: 5/20...  Training Step: 847...  Training loss: 2.0474...  2.0695 sec/batch\n",
      "Epoch: 5/20...  Training Step: 848...  Training loss: 2.0692...  2.1612 sec/batch\n",
      "Epoch: 5/20...  Training Step: 849...  Training loss: 2.0612...  2.2089 sec/batch\n",
      "Epoch: 5/20...  Training Step: 850...  Training loss: 2.0364...  2.2239 sec/batch\n",
      "Epoch: 5/20...  Training Step: 851...  Training loss: 2.0380...  2.2234 sec/batch\n",
      "Epoch: 5/20...  Training Step: 852...  Training loss: 2.0710...  2.1728 sec/batch\n",
      "Epoch: 5/20...  Training Step: 853...  Training loss: 2.0473...  2.5152 sec/batch\n",
      "Epoch: 5/20...  Training Step: 854...  Training loss: 2.0672...  2.1803 sec/batch\n",
      "Epoch: 5/20...  Training Step: 855...  Training loss: 2.0768...  2.3999 sec/batch\n",
      "Epoch: 5/20...  Training Step: 856...  Training loss: 2.0524...  2.5157 sec/batch\n",
      "Epoch: 5/20...  Training Step: 857...  Training loss: 2.0339...  2.0730 sec/batch\n",
      "Epoch: 5/20...  Training Step: 858...  Training loss: 2.0759...  2.0435 sec/batch\n",
      "Epoch: 5/20...  Training Step: 859...  Training loss: 2.0497...  2.1347 sec/batch\n",
      "Epoch: 5/20...  Training Step: 860...  Training loss: 2.0156...  2.0896 sec/batch\n",
      "Epoch: 5/20...  Training Step: 861...  Training loss: 2.0204...  2.2603 sec/batch\n",
      "Epoch: 5/20...  Training Step: 862...  Training loss: 2.0382...  2.1738 sec/batch\n",
      "Epoch: 5/20...  Training Step: 863...  Training loss: 2.0654...  2.2044 sec/batch\n",
      "Epoch: 5/20...  Training Step: 864...  Training loss: 2.0500...  2.0479 sec/batch\n",
      "Epoch: 5/20...  Training Step: 865...  Training loss: 2.0545...  2.2209 sec/batch\n",
      "Epoch: 5/20...  Training Step: 866...  Training loss: 2.0368...  2.1502 sec/batch\n",
      "Epoch: 5/20...  Training Step: 867...  Training loss: 2.0337...  2.1197 sec/batch\n",
      "Epoch: 5/20...  Training Step: 868...  Training loss: 2.0739...  2.1868 sec/batch\n",
      "Epoch: 5/20...  Training Step: 869...  Training loss: 2.0388...  2.0038 sec/batch\n",
      "Epoch: 5/20...  Training Step: 870...  Training loss: 2.0511...  2.1247 sec/batch\n",
      "Epoch: 5/20...  Training Step: 871...  Training loss: 2.0131...  2.2039 sec/batch\n",
      "Epoch: 5/20...  Training Step: 872...  Training loss: 2.0363...  2.0986 sec/batch\n",
      "Epoch: 5/20...  Training Step: 873...  Training loss: 2.0103...  2.2029 sec/batch\n",
      "Epoch: 5/20...  Training Step: 874...  Training loss: 2.0603...  2.0860 sec/batch\n",
      "Epoch: 5/20...  Training Step: 875...  Training loss: 2.0114...  2.2014 sec/batch\n",
      "Epoch: 5/20...  Training Step: 876...  Training loss: 2.0394...  2.0720 sec/batch\n",
      "Epoch: 5/20...  Training Step: 877...  Training loss: 2.0027...  2.0691 sec/batch\n",
      "Epoch: 5/20...  Training Step: 878...  Training loss: 2.0175...  2.2089 sec/batch\n",
      "Epoch: 5/20...  Training Step: 879...  Training loss: 2.0259...  2.1818 sec/batch\n",
      "Epoch: 5/20...  Training Step: 880...  Training loss: 2.0100...  2.1045 sec/batch\n",
      "Epoch: 5/20...  Training Step: 881...  Training loss: 1.9997...  2.0565 sec/batch\n",
      "Epoch: 5/20...  Training Step: 882...  Training loss: 2.0448...  2.0595 sec/batch\n",
      "Epoch: 5/20...  Training Step: 883...  Training loss: 2.0103...  2.2034 sec/batch\n",
      "Epoch: 5/20...  Training Step: 884...  Training loss: 2.0218...  2.1217 sec/batch\n",
      "Epoch: 5/20...  Training Step: 885...  Training loss: 2.0056...  2.1768 sec/batch\n",
      "Epoch: 5/20...  Training Step: 886...  Training loss: 1.9996...  2.0299 sec/batch\n",
      "Epoch: 5/20...  Training Step: 887...  Training loss: 2.0001...  1.9838 sec/batch\n",
      "Epoch: 5/20...  Training Step: 888...  Training loss: 2.0231...  2.1097 sec/batch\n",
      "Epoch: 5/20...  Training Step: 889...  Training loss: 2.0103...  2.1648 sec/batch\n",
      "Epoch: 5/20...  Training Step: 890...  Training loss: 1.9992...  2.1943 sec/batch\n",
      "Epoch: 5/20...  Training Step: 891...  Training loss: 2.0157...  2.0740 sec/batch\n",
      "Epoch: 5/20...  Training Step: 892...  Training loss: 1.9878...  2.1743 sec/batch\n",
      "Epoch: 5/20...  Training Step: 893...  Training loss: 2.0374...  2.1066 sec/batch\n",
      "Epoch: 5/20...  Training Step: 894...  Training loss: 2.0246...  2.1241 sec/batch\n",
      "Epoch: 5/20...  Training Step: 895...  Training loss: 2.0031...  2.1106 sec/batch\n",
      "Epoch: 5/20...  Training Step: 896...  Training loss: 2.0010...  2.0515 sec/batch\n",
      "Epoch: 5/20...  Training Step: 897...  Training loss: 2.0030...  2.0725 sec/batch\n",
      "Epoch: 5/20...  Training Step: 898...  Training loss: 2.0188...  1.9652 sec/batch\n",
      "Epoch: 5/20...  Training Step: 899...  Training loss: 2.0049...  2.0284 sec/batch\n",
      "Epoch: 5/20...  Training Step: 900...  Training loss: 2.0370...  2.1302 sec/batch\n",
      "Epoch: 5/20...  Training Step: 901...  Training loss: 2.0274...  2.0299 sec/batch\n",
      "Epoch: 5/20...  Training Step: 902...  Training loss: 2.0117...  2.0665 sec/batch\n",
      "Epoch: 5/20...  Training Step: 903...  Training loss: 2.0102...  2.0078 sec/batch\n",
      "Epoch: 5/20...  Training Step: 904...  Training loss: 2.0062...  1.9803 sec/batch\n",
      "Epoch: 5/20...  Training Step: 905...  Training loss: 2.0091...  2.1257 sec/batch\n",
      "Epoch: 5/20...  Training Step: 906...  Training loss: 1.9960...  2.0415 sec/batch\n",
      "Epoch: 5/20...  Training Step: 907...  Training loss: 2.0039...  2.1577 sec/batch\n",
      "Epoch: 5/20...  Training Step: 908...  Training loss: 1.9680...  2.1237 sec/batch\n",
      "Epoch: 5/20...  Training Step: 909...  Training loss: 2.0211...  2.0961 sec/batch\n",
      "Epoch: 5/20...  Training Step: 910...  Training loss: 2.0081...  2.0008 sec/batch\n",
      "Epoch: 5/20...  Training Step: 911...  Training loss: 2.0179...  2.0284 sec/batch\n",
      "Epoch: 5/20...  Training Step: 912...  Training loss: 2.0093...  2.0941 sec/batch\n",
      "Epoch: 5/20...  Training Step: 913...  Training loss: 2.0307...  2.0515 sec/batch\n",
      "Epoch: 5/20...  Training Step: 914...  Training loss: 1.9923...  2.0911 sec/batch\n",
      "Epoch: 5/20...  Training Step: 915...  Training loss: 1.9941...  1.9933 sec/batch\n",
      "Epoch: 5/20...  Training Step: 916...  Training loss: 2.0318...  2.0650 sec/batch\n",
      "Epoch: 5/20...  Training Step: 917...  Training loss: 2.0033...  2.0425 sec/batch\n",
      "Epoch: 5/20...  Training Step: 918...  Training loss: 1.9723...  2.0505 sec/batch\n",
      "Epoch: 5/20...  Training Step: 919...  Training loss: 2.0250...  2.0880 sec/batch\n",
      "Epoch: 5/20...  Training Step: 920...  Training loss: 2.0224...  1.9748 sec/batch\n",
      "Epoch: 5/20...  Training Step: 921...  Training loss: 2.0073...  2.0474 sec/batch\n",
      "Epoch: 5/20...  Training Step: 922...  Training loss: 2.0112...  1.9346 sec/batch\n",
      "Epoch: 5/20...  Training Step: 923...  Training loss: 1.9925...  2.0249 sec/batch\n",
      "Epoch: 5/20...  Training Step: 924...  Training loss: 1.9799...  2.1898 sec/batch\n",
      "Epoch: 5/20...  Training Step: 925...  Training loss: 2.0193...  2.5132 sec/batch\n",
      "Epoch: 5/20...  Training Step: 926...  Training loss: 2.0098...  1.9932 sec/batch\n",
      "Epoch: 5/20...  Training Step: 927...  Training loss: 2.0138...  1.9778 sec/batch\n",
      "Epoch: 5/20...  Training Step: 928...  Training loss: 2.0065...  1.9893 sec/batch\n",
      "Epoch: 5/20...  Training Step: 929...  Training loss: 2.0097...  1.8411 sec/batch\n",
      "Epoch: 5/20...  Training Step: 930...  Training loss: 2.0088...  1.8986 sec/batch\n",
      "Epoch: 5/20...  Training Step: 931...  Training loss: 2.0302...  2.3222 sec/batch\n",
      "Epoch: 5/20...  Training Step: 932...  Training loss: 2.0017...  2.1207 sec/batch\n",
      "Epoch: 5/20...  Training Step: 933...  Training loss: 2.0359...  2.2450 sec/batch\n",
      "Epoch: 5/20...  Training Step: 934...  Training loss: 1.9975...  1.8554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 935...  Training loss: 2.0089...  2.2524 sec/batch\n",
      "Epoch: 5/20...  Training Step: 936...  Training loss: 1.9948...  2.0710 sec/batch\n",
      "Epoch: 5/20...  Training Step: 937...  Training loss: 1.9901...  1.7266 sec/batch\n",
      "Epoch: 5/20...  Training Step: 938...  Training loss: 2.0149...  1.8055 sec/batch\n",
      "Epoch: 5/20...  Training Step: 939...  Training loss: 2.0109...  1.8427 sec/batch\n",
      "Epoch: 5/20...  Training Step: 940...  Training loss: 2.0196...  1.8700 sec/batch\n",
      "Epoch: 5/20...  Training Step: 941...  Training loss: 2.0012...  2.0063 sec/batch\n",
      "Epoch: 5/20...  Training Step: 942...  Training loss: 1.9972...  2.0996 sec/batch\n",
      "Epoch: 5/20...  Training Step: 943...  Training loss: 1.9996...  2.0896 sec/batch\n",
      "Epoch: 5/20...  Training Step: 944...  Training loss: 2.0305...  1.9091 sec/batch\n",
      "Epoch: 5/20...  Training Step: 945...  Training loss: 2.0112...  1.7697 sec/batch\n",
      "Epoch: 5/20...  Training Step: 946...  Training loss: 2.0113...  1.7286 sec/batch\n",
      "Epoch: 5/20...  Training Step: 947...  Training loss: 2.0028...  1.7642 sec/batch\n",
      "Epoch: 5/20...  Training Step: 948...  Training loss: 1.9906...  1.7827 sec/batch\n",
      "Epoch: 5/20...  Training Step: 949...  Training loss: 1.9984...  1.7141 sec/batch\n",
      "Epoch: 5/20...  Training Step: 950...  Training loss: 1.9988...  1.7116 sec/batch\n",
      "Epoch: 5/20...  Training Step: 951...  Training loss: 1.9798...  1.8013 sec/batch\n",
      "Epoch: 5/20...  Training Step: 952...  Training loss: 2.0485...  1.7557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 953...  Training loss: 2.0207...  1.8048 sec/batch\n",
      "Epoch: 5/20...  Training Step: 954...  Training loss: 1.9843...  1.7737 sec/batch\n",
      "Epoch: 5/20...  Training Step: 955...  Training loss: 2.0081...  1.8013 sec/batch\n",
      "Epoch: 5/20...  Training Step: 956...  Training loss: 2.0049...  1.6965 sec/batch\n",
      "Epoch: 5/20...  Training Step: 957...  Training loss: 1.9884...  1.7532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 958...  Training loss: 1.9861...  1.7888 sec/batch\n",
      "Epoch: 5/20...  Training Step: 959...  Training loss: 1.9995...  1.7762 sec/batch\n",
      "Epoch: 5/20...  Training Step: 960...  Training loss: 2.0335...  1.7963 sec/batch\n",
      "Epoch: 5/20...  Training Step: 961...  Training loss: 1.9833...  1.7191 sec/batch\n",
      "Epoch: 5/20...  Training Step: 962...  Training loss: 1.9861...  1.7386 sec/batch\n",
      "Epoch: 5/20...  Training Step: 963...  Training loss: 1.9802...  1.7923 sec/batch\n",
      "Epoch: 5/20...  Training Step: 964...  Training loss: 1.9830...  1.7557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 965...  Training loss: 2.0225...  1.7862 sec/batch\n",
      "Epoch: 5/20...  Training Step: 966...  Training loss: 2.0105...  1.6990 sec/batch\n",
      "Epoch: 5/20...  Training Step: 967...  Training loss: 2.0098...  1.7782 sec/batch\n",
      "Epoch: 5/20...  Training Step: 968...  Training loss: 1.9956...  1.6955 sec/batch\n",
      "Epoch: 5/20...  Training Step: 969...  Training loss: 1.9845...  1.7597 sec/batch\n",
      "Epoch: 5/20...  Training Step: 970...  Training loss: 2.0008...  1.8218 sec/batch\n",
      "Epoch: 5/20...  Training Step: 971...  Training loss: 1.9691...  1.7692 sec/batch\n",
      "Epoch: 5/20...  Training Step: 972...  Training loss: 1.9498...  1.8213 sec/batch\n",
      "Epoch: 5/20...  Training Step: 973...  Training loss: 1.9691...  1.7246 sec/batch\n",
      "Epoch: 5/20...  Training Step: 974...  Training loss: 1.9833...  1.7441 sec/batch\n",
      "Epoch: 5/20...  Training Step: 975...  Training loss: 1.9885...  1.7908 sec/batch\n",
      "Epoch: 5/20...  Training Step: 976...  Training loss: 2.0183...  1.7451 sec/batch\n",
      "Epoch: 5/20...  Training Step: 977...  Training loss: 1.9845...  1.7898 sec/batch\n",
      "Epoch: 5/20...  Training Step: 978...  Training loss: 1.9748...  1.6840 sec/batch\n",
      "Epoch: 5/20...  Training Step: 979...  Training loss: 1.9886...  1.7602 sec/batch\n",
      "Epoch: 5/20...  Training Step: 980...  Training loss: 1.9695...  1.7898 sec/batch\n",
      "Epoch: 5/20...  Training Step: 981...  Training loss: 1.9721...  1.7637 sec/batch\n",
      "Epoch: 5/20...  Training Step: 982...  Training loss: 1.9803...  1.8003 sec/batch\n",
      "Epoch: 5/20...  Training Step: 983...  Training loss: 1.9909...  1.7567 sec/batch\n",
      "Epoch: 5/20...  Training Step: 984...  Training loss: 1.9682...  1.8013 sec/batch\n",
      "Epoch: 5/20...  Training Step: 985...  Training loss: 1.9837...  1.6970 sec/batch\n",
      "Epoch: 5/20...  Training Step: 986...  Training loss: 1.9608...  1.7622 sec/batch\n",
      "Epoch: 5/20...  Training Step: 987...  Training loss: 1.9512...  1.7722 sec/batch\n",
      "Epoch: 5/20...  Training Step: 988...  Training loss: 1.9751...  1.7434 sec/batch\n",
      "Epoch: 5/20...  Training Step: 989...  Training loss: 1.9755...  1.7587 sec/batch\n",
      "Epoch: 5/20...  Training Step: 990...  Training loss: 1.9654...  1.7476 sec/batch\n",
      "Epoch: 6/20...  Training Step: 991...  Training loss: 2.0875...  1.8635 sec/batch\n",
      "Epoch: 6/20...  Training Step: 992...  Training loss: 1.9572...  1.7251 sec/batch\n",
      "Epoch: 6/20...  Training Step: 993...  Training loss: 1.9589...  1.7406 sec/batch\n",
      "Epoch: 6/20...  Training Step: 994...  Training loss: 1.9447...  1.8269 sec/batch\n",
      "Epoch: 6/20...  Training Step: 995...  Training loss: 1.9481...  1.6960 sec/batch\n",
      "Epoch: 6/20...  Training Step: 996...  Training loss: 1.9445...  1.7331 sec/batch\n",
      "Epoch: 6/20...  Training Step: 997...  Training loss: 1.9715...  1.7913 sec/batch\n",
      "Epoch: 6/20...  Training Step: 998...  Training loss: 1.9617...  1.7456 sec/batch\n",
      "Epoch: 6/20...  Training Step: 999...  Training loss: 2.0133...  1.7903 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1000...  Training loss: 1.9598...  1.7080 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1001...  Training loss: 1.9516...  1.6233 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1002...  Training loss: 1.9564...  1.6053 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1003...  Training loss: 1.9724...  1.7161 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1004...  Training loss: 2.0059...  1.8018 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1005...  Training loss: 1.9619...  1.7456 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1006...  Training loss: 1.9598...  1.7762 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1007...  Training loss: 1.9732...  1.7256 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1008...  Training loss: 2.0044...  1.7381 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1009...  Training loss: 1.9747...  1.7802 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1010...  Training loss: 1.9688...  1.7427 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1011...  Training loss: 1.9469...  1.7928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1012...  Training loss: 2.0058...  1.7391 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1013...  Training loss: 1.9562...  1.7146 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1014...  Training loss: 1.9627...  1.7025 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1015...  Training loss: 1.9537...  1.7512 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1016...  Training loss: 1.9378...  1.7627 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1017...  Training loss: 1.9479...  1.7256 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1018...  Training loss: 1.9758...  1.7842 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1019...  Training loss: 1.9911...  1.7181 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1020...  Training loss: 1.9713...  1.7376 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1021...  Training loss: 1.9730...  1.7688 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1022...  Training loss: 1.9487...  1.7136 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1023...  Training loss: 1.9697...  1.7963 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1024...  Training loss: 1.9858...  1.7321 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1025...  Training loss: 1.9412...  1.7507 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1026...  Training loss: 1.9551...  1.7697 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1027...  Training loss: 1.9510...  1.7361 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1028...  Training loss: 1.9352...  1.7948 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1029...  Training loss: 1.9243...  1.7281 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1030...  Training loss: 1.9277...  1.7587 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1031...  Training loss: 1.9368...  1.6895 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1032...  Training loss: 1.9643...  1.7351 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1033...  Training loss: 1.9348...  1.8148 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1034...  Training loss: 1.9241...  1.7697 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1035...  Training loss: 1.9521...  1.7557 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1036...  Training loss: 1.8927...  1.6755 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1037...  Training loss: 1.9578...  1.7291 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1038...  Training loss: 1.9318...  1.7888 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1039...  Training loss: 1.9306...  1.7517 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1040...  Training loss: 1.9863...  1.7762 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1041...  Training loss: 1.9191...  1.6684 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1042...  Training loss: 1.9924...  1.7121 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1043...  Training loss: 1.9421...  1.7492 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1044...  Training loss: 1.9381...  1.7391 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1045...  Training loss: 1.9314...  1.7552 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1046...  Training loss: 1.9538...  1.7281 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1047...  Training loss: 1.9498...  1.7356 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1048...  Training loss: 1.9326...  1.6760 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1049...  Training loss: 1.9257...  1.7276 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1050...  Training loss: 1.9748...  1.8093 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1051...  Training loss: 1.9565...  1.7296 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1052...  Training loss: 1.9761...  1.7888 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1053...  Training loss: 1.9605...  1.6895 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1054...  Training loss: 1.9521...  1.7256 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1055...  Training loss: 1.9401...  1.8158 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1056...  Training loss: 1.9725...  1.7346 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1057...  Training loss: 1.9578...  1.7903 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1058...  Training loss: 1.9242...  1.7161 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1059...  Training loss: 1.9218...  1.7090 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1060...  Training loss: 1.9434...  1.7010 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1061...  Training loss: 1.9705...  1.7446 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1062...  Training loss: 1.9587...  1.8028 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1063...  Training loss: 1.9606...  1.7647 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1064...  Training loss: 1.9274...  1.8083 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1065...  Training loss: 1.9281...  1.7381 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1066...  Training loss: 1.9718...  1.7462 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1067...  Training loss: 1.9440...  1.7953 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1068...  Training loss: 1.9507...  1.7166 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1069...  Training loss: 1.9126...  1.7391 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1070...  Training loss: 1.9384...  1.6915 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1071...  Training loss: 1.9020...  1.7126 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1072...  Training loss: 1.9558...  1.7331 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1073...  Training loss: 1.9086...  1.7191 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1074...  Training loss: 1.9241...  1.7497 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1075...  Training loss: 1.8958...  1.7055 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1076...  Training loss: 1.9193...  1.7632 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1077...  Training loss: 1.9252...  1.6795 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1078...  Training loss: 1.9139...  1.7361 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1079...  Training loss: 1.8970...  1.8033 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1080...  Training loss: 1.9420...  1.8068 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1081...  Training loss: 1.9142...  1.7862 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1082...  Training loss: 1.9188...  1.7111 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1083...  Training loss: 1.9017...  1.7331 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1084...  Training loss: 1.9176...  1.7918 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1085...  Training loss: 1.9117...  1.7466 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1086...  Training loss: 1.9286...  1.7772 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1087...  Training loss: 1.9180...  1.7040 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1088...  Training loss: 1.8941...  1.7487 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1089...  Training loss: 1.9056...  1.7923 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1090...  Training loss: 1.8851...  1.7416 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1091...  Training loss: 1.9223...  1.8113 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1092...  Training loss: 1.9252...  1.7371 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1093...  Training loss: 1.9128...  1.8093 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1094...  Training loss: 1.9086...  1.7146 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1095...  Training loss: 1.9185...  1.7547 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1096...  Training loss: 1.9254...  1.8043 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1097...  Training loss: 1.9018...  1.7537 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1098...  Training loss: 1.9309...  1.7687 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1099...  Training loss: 1.9410...  1.7010 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1100...  Training loss: 1.9262...  1.7436 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1101...  Training loss: 1.9112...  1.8609 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1102...  Training loss: 1.9051...  1.8424 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1103...  Training loss: 1.9201...  1.8314 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1104...  Training loss: 1.9040...  1.7181 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1105...  Training loss: 1.8956...  1.6810 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1106...  Training loss: 1.8664...  1.6860 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1107...  Training loss: 1.9169...  1.7306 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1108...  Training loss: 1.9159...  1.7762 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1109...  Training loss: 1.9247...  1.7246 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1110...  Training loss: 1.9243...  1.7938 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1111...  Training loss: 1.9234...  1.7146 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1112...  Training loss: 1.8886...  1.7361 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1113...  Training loss: 1.8926...  1.8033 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1114...  Training loss: 1.9443...  1.7587 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1115...  Training loss: 1.9096...  1.8419 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1116...  Training loss: 1.8750...  1.7331 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1117...  Training loss: 1.9350...  1.7597 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1118...  Training loss: 1.9318...  1.7762 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1119...  Training loss: 1.9174...  1.7577 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1120...  Training loss: 1.9203...  1.8183 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1121...  Training loss: 1.8958...  1.7226 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1122...  Training loss: 1.8828...  1.7893 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1123...  Training loss: 1.9390...  1.7221 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1124...  Training loss: 1.9137...  1.7386 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1125...  Training loss: 1.9152...  1.8334 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1126...  Training loss: 1.9132...  1.7441 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1127...  Training loss: 1.9387...  1.8083 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1128...  Training loss: 1.9069...  1.7341 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1129...  Training loss: 1.9376...  1.7617 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1130...  Training loss: 1.9127...  1.8113 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1131...  Training loss: 1.9422...  1.7346 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1132...  Training loss: 1.9061...  1.7988 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1133...  Training loss: 1.9163...  1.7131 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1134...  Training loss: 1.9111...  1.7341 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1135...  Training loss: 1.8892...  1.8168 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1136...  Training loss: 1.9302...  1.7537 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1137...  Training loss: 1.9199...  1.8053 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1138...  Training loss: 1.9356...  1.7281 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1139...  Training loss: 1.9151...  1.7873 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1140...  Training loss: 1.9006...  1.6860 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1141...  Training loss: 1.8968...  1.7306 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1142...  Training loss: 1.9438...  1.7923 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1143...  Training loss: 1.9270...  1.7461 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1144...  Training loss: 1.9216...  1.8299 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1145...  Training loss: 1.9041...  1.6995 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1146...  Training loss: 1.9049...  1.7231 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1147...  Training loss: 1.9155...  1.7832 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1148...  Training loss: 1.9065...  1.7557 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1149...  Training loss: 1.8806...  1.8148 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1150...  Training loss: 1.9438...  1.7085 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1151...  Training loss: 1.9221...  1.7246 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1152...  Training loss: 1.9023...  1.8123 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1153...  Training loss: 1.9182...  1.7376 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1154...  Training loss: 1.9122...  1.8123 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1155...  Training loss: 1.9109...  1.7652 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1156...  Training loss: 1.9058...  1.7642 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1157...  Training loss: 1.9161...  1.7095 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1158...  Training loss: 1.9654...  1.7672 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1159...  Training loss: 1.8979...  1.7928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1160...  Training loss: 1.8986...  1.7612 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1161...  Training loss: 1.8893...  1.8228 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1162...  Training loss: 1.8932...  1.7251 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1163...  Training loss: 1.9287...  1.7727 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1164...  Training loss: 1.9221...  1.8123 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1165...  Training loss: 1.9108...  1.7336 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1166...  Training loss: 1.9079...  1.7993 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1167...  Training loss: 1.8867...  1.7336 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1168...  Training loss: 1.9054...  1.7873 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1169...  Training loss: 1.8784...  1.7131 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1170...  Training loss: 1.8630...  1.7311 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1171...  Training loss: 1.8789...  1.8048 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1172...  Training loss: 1.8943...  1.7376 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1173...  Training loss: 1.9038...  1.7908 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1174...  Training loss: 1.9369...  1.7126 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1175...  Training loss: 1.8940...  1.7456 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1176...  Training loss: 1.8778...  1.7862 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1177...  Training loss: 1.9060...  1.7427 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1178...  Training loss: 1.8773...  1.8078 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1179...  Training loss: 1.8862...  1.7236 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1180...  Training loss: 1.8994...  1.7476 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1181...  Training loss: 1.9080...  1.8284 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1182...  Training loss: 1.8701...  1.7537 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1183...  Training loss: 1.8950...  1.8048 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1184...  Training loss: 1.8804...  1.7497 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1185...  Training loss: 1.8666...  1.7973 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1186...  Training loss: 1.9015...  1.7296 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1187...  Training loss: 1.8862...  1.7366 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1188...  Training loss: 1.8651...  1.8218 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1189...  Training loss: 1.9975...  1.7241 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1190...  Training loss: 1.8748...  1.7928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1191...  Training loss: 1.8750...  1.7181 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1192...  Training loss: 1.8724...  1.7567 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1193...  Training loss: 1.8817...  1.7933 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1194...  Training loss: 1.8517...  1.7326 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1195...  Training loss: 1.8977...  1.8018 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1196...  Training loss: 1.8831...  1.7000 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1197...  Training loss: 1.9235...  1.7396 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1198...  Training loss: 1.8896...  1.7697 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1199...  Training loss: 1.8748...  1.7331 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1200...  Training loss: 1.8762...  1.8068 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1201...  Training loss: 1.8904...  1.6879 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1202...  Training loss: 1.9362...  1.6609 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1203...  Training loss: 1.8814...  1.6504 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1204...  Training loss: 1.8753...  1.6970 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1205...  Training loss: 1.8958...  1.7878 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1206...  Training loss: 1.9199...  1.7527 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1207...  Training loss: 1.8793...  1.8043 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1208...  Training loss: 1.8954...  1.6970 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1209...  Training loss: 1.8642...  1.7612 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1210...  Training loss: 1.9284...  1.8063 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1211...  Training loss: 1.8719...  1.7456 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1212...  Training loss: 1.8738...  1.7928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1213...  Training loss: 1.8716...  1.7652 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1214...  Training loss: 1.8587...  1.7983 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1215...  Training loss: 1.8648...  1.7090 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1216...  Training loss: 1.8929...  1.7747 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1217...  Training loss: 1.9057...  1.8188 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1218...  Training loss: 1.9017...  1.7336 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1219...  Training loss: 1.8881...  1.8108 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1220...  Training loss: 1.8571...  1.7131 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1221...  Training loss: 1.8860...  1.7351 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1222...  Training loss: 1.9096...  1.7873 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1223...  Training loss: 1.8522...  1.7512 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1224...  Training loss: 1.8673...  1.8008 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1225...  Training loss: 1.8781...  1.7100 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1226...  Training loss: 1.8584...  1.7311 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1227...  Training loss: 1.8373...  1.7893 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1228...  Training loss: 1.8603...  1.7497 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1229...  Training loss: 1.8517...  1.8038 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1230...  Training loss: 1.8877...  1.7627 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1231...  Training loss: 1.8522...  1.7827 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1232...  Training loss: 1.8450...  1.7166 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1233...  Training loss: 1.8742...  1.7537 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1234...  Training loss: 1.8159...  1.7176 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1235...  Training loss: 1.8778...  1.7206 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1236...  Training loss: 1.8582...  1.7326 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1237...  Training loss: 1.8583...  1.6624 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1238...  Training loss: 1.9086...  1.7331 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1239...  Training loss: 1.8481...  1.7276 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1240...  Training loss: 1.9252...  1.6995 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1241...  Training loss: 1.8688...  1.7662 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1242...  Training loss: 1.8560...  1.6704 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1243...  Training loss: 1.8634...  1.7095 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1244...  Training loss: 1.8824...  1.7637 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1245...  Training loss: 1.8778...  1.7276 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1246...  Training loss: 1.8509...  1.7532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1247...  Training loss: 1.8507...  1.7076 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1248...  Training loss: 1.8958...  1.7612 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1249...  Training loss: 1.8790...  1.7075 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1250...  Training loss: 1.9135...  1.7171 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1251...  Training loss: 1.8984...  1.7697 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1252...  Training loss: 1.8696...  1.7126 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1253...  Training loss: 1.8536...  1.7391 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1254...  Training loss: 1.8923...  1.6689 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1255...  Training loss: 1.8845...  1.7060 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1256...  Training loss: 1.8436...  1.7842 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1257...  Training loss: 1.8491...  1.7431 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1258...  Training loss: 1.8705...  1.7572 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1259...  Training loss: 1.8997...  1.7222 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1260...  Training loss: 1.8793...  1.7296 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1261...  Training loss: 1.8879...  1.6719 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1262...  Training loss: 1.8584...  1.7141 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1263...  Training loss: 1.8637...  1.7181 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1264...  Training loss: 1.8929...  1.7166 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1265...  Training loss: 1.8619...  1.7582 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1266...  Training loss: 1.8711...  1.6860 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1267...  Training loss: 1.8330...  1.7201 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1268...  Training loss: 1.8537...  1.7436 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1269...  Training loss: 1.8247...  1.7186 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1270...  Training loss: 1.8790...  1.7487 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1271...  Training loss: 1.8363...  1.6629 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1272...  Training loss: 1.8608...  1.7116 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1273...  Training loss: 1.8280...  1.7456 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1274...  Training loss: 1.8456...  1.7366 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1275...  Training loss: 1.8521...  1.7722 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1276...  Training loss: 1.8457...  1.7050 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1277...  Training loss: 1.8304...  1.7837 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1278...  Training loss: 1.8651...  1.6815 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1279...  Training loss: 1.8347...  1.7226 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1280...  Training loss: 1.8459...  1.7692 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1281...  Training loss: 1.8391...  1.7216 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1282...  Training loss: 1.8456...  1.7873 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1283...  Training loss: 1.8425...  1.6684 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1284...  Training loss: 1.8532...  1.6960 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1285...  Training loss: 1.8536...  1.7777 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1286...  Training loss: 1.8307...  1.7431 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1287...  Training loss: 1.8404...  1.7612 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1288...  Training loss: 1.8278...  1.6715 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1289...  Training loss: 1.8577...  1.7216 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1290...  Training loss: 1.8620...  1.7492 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1291...  Training loss: 1.8433...  1.7391 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1292...  Training loss: 1.8408...  1.7612 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1293...  Training loss: 1.8470...  1.7321 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1294...  Training loss: 1.8675...  1.7687 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1295...  Training loss: 1.8436...  1.7025 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1296...  Training loss: 1.8656...  1.7311 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1297...  Training loss: 1.8651...  1.8053 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1298...  Training loss: 1.8619...  1.7100 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1299...  Training loss: 1.8480...  1.7732 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1300...  Training loss: 1.8437...  1.6850 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1301...  Training loss: 1.8482...  1.7121 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1302...  Training loss: 1.8320...  1.7406 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1303...  Training loss: 1.8401...  1.7256 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1304...  Training loss: 1.8192...  1.7998 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1305...  Training loss: 1.8627...  1.6920 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1306...  Training loss: 1.8515...  1.7702 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1307...  Training loss: 1.8615...  1.7080 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1308...  Training loss: 1.8503...  1.7492 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1309...  Training loss: 1.8662...  1.7617 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1310...  Training loss: 1.8165...  1.7201 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1311...  Training loss: 1.8210...  1.7306 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1312...  Training loss: 1.8677...  1.6509 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1313...  Training loss: 1.8469...  1.7131 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1314...  Training loss: 1.8044...  1.7757 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1315...  Training loss: 1.8636...  1.7181 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1316...  Training loss: 1.8691...  1.7853 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1317...  Training loss: 1.8404...  1.6724 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1318...  Training loss: 1.8465...  1.7111 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1319...  Training loss: 1.8214...  1.7672 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1320...  Training loss: 1.8095...  1.7171 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1321...  Training loss: 1.8679...  1.7752 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1322...  Training loss: 1.8467...  1.7336 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1323...  Training loss: 1.8564...  1.7858 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1324...  Training loss: 1.8449...  1.7080 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1325...  Training loss: 1.8629...  1.7336 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1326...  Training loss: 1.8543...  1.7878 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1327...  Training loss: 1.8804...  1.7517 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1328...  Training loss: 1.8444...  1.7883 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1329...  Training loss: 1.8736...  1.7186 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1330...  Training loss: 1.8284...  1.7366 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1331...  Training loss: 1.8360...  1.7632 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1332...  Training loss: 1.8435...  1.7316 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1333...  Training loss: 1.8273...  1.7612 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1334...  Training loss: 1.8572...  1.6905 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1335...  Training loss: 1.8565...  1.7176 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1336...  Training loss: 1.8769...  1.7697 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1337...  Training loss: 1.8502...  1.7371 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1338...  Training loss: 1.8428...  1.7742 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1339...  Training loss: 1.8197...  1.7035 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1340...  Training loss: 1.8710...  1.7662 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1341...  Training loss: 1.8539...  1.6815 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1342...  Training loss: 1.8501...  1.7216 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1343...  Training loss: 1.8387...  1.7577 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1344...  Training loss: 1.8441...  1.7371 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1345...  Training loss: 1.8582...  1.7512 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1346...  Training loss: 1.8367...  1.6800 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1347...  Training loss: 1.8190...  1.7241 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1348...  Training loss: 1.8686...  1.7797 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1349...  Training loss: 1.8788...  1.7236 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1350...  Training loss: 1.8391...  1.7396 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1351...  Training loss: 1.8580...  1.7076 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1352...  Training loss: 1.8414...  1.7692 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1353...  Training loss: 1.8490...  1.6684 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1354...  Training loss: 1.8495...  1.7336 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1355...  Training loss: 1.8535...  1.8118 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1356...  Training loss: 1.8981...  1.7196 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1357...  Training loss: 1.8324...  1.7487 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1358...  Training loss: 1.8406...  1.6604 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1359...  Training loss: 1.8153...  1.7111 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1360...  Training loss: 1.8139...  1.7873 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1361...  Training loss: 1.8633...  1.7181 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1362...  Training loss: 1.8419...  1.7787 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1363...  Training loss: 1.8520...  1.6619 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1364...  Training loss: 1.8302...  1.7046 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1365...  Training loss: 1.8265...  1.7812 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1366...  Training loss: 1.8512...  1.7316 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1367...  Training loss: 1.8042...  1.7406 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1368...  Training loss: 1.7942...  1.7196 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1369...  Training loss: 1.7985...  1.7577 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1370...  Training loss: 1.8332...  1.6890 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1371...  Training loss: 1.8329...  1.7497 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1372...  Training loss: 1.8686...  1.7657 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1373...  Training loss: 1.8348...  1.7411 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1374...  Training loss: 1.8185...  1.7602 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1375...  Training loss: 1.8470...  1.6770 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1376...  Training loss: 1.8192...  1.7131 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1377...  Training loss: 1.8315...  1.7807 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1378...  Training loss: 1.8348...  1.7447 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1379...  Training loss: 1.8427...  1.7637 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1380...  Training loss: 1.8114...  1.6750 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1381...  Training loss: 1.8320...  1.7356 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1382...  Training loss: 1.8050...  1.7552 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1383...  Training loss: 1.8031...  1.7547 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1384...  Training loss: 1.8339...  1.7702 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1385...  Training loss: 1.8261...  1.7191 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1386...  Training loss: 1.8096...  1.7607 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1387...  Training loss: 1.9338...  1.6820 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1388...  Training loss: 1.8177...  1.7036 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1389...  Training loss: 1.8193...  1.7607 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1390...  Training loss: 1.8316...  1.7336 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1391...  Training loss: 1.8234...  1.7567 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1392...  Training loss: 1.7880...  1.6805 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1393...  Training loss: 1.8373...  1.7090 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1394...  Training loss: 1.8120...  1.7462 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1395...  Training loss: 1.8499...  1.7321 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1396...  Training loss: 1.8289...  1.7687 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1397...  Training loss: 1.8072...  1.6745 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1398...  Training loss: 1.8100...  1.6850 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1399...  Training loss: 1.8339...  1.6629 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1400...  Training loss: 1.8638...  1.7271 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1401...  Training loss: 1.8225...  1.6419 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1402...  Training loss: 1.8093...  1.6609 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1403...  Training loss: 1.8330...  1.7632 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1404...  Training loss: 1.8655...  1.6644 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1405...  Training loss: 1.8105...  1.7026 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1406...  Training loss: 1.8349...  1.7527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1407...  Training loss: 1.8170...  1.7321 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1408...  Training loss: 1.8584...  1.7692 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1409...  Training loss: 1.8113...  1.6995 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1410...  Training loss: 1.8260...  1.7251 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1411...  Training loss: 1.8138...  1.7787 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1412...  Training loss: 1.7979...  1.7527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1413...  Training loss: 1.7962...  1.7812 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1414...  Training loss: 1.8375...  1.7171 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1415...  Training loss: 1.8603...  1.8133 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1416...  Training loss: 1.8368...  1.6975 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1417...  Training loss: 1.8194...  1.7266 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1418...  Training loss: 1.7986...  1.7441 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1419...  Training loss: 1.8353...  1.7191 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1420...  Training loss: 1.8386...  1.7893 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1421...  Training loss: 1.8084...  1.6760 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1422...  Training loss: 1.8173...  1.7502 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1423...  Training loss: 1.8024...  1.7983 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1424...  Training loss: 1.7898...  1.7191 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1425...  Training loss: 1.7817...  1.7863 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1426...  Training loss: 1.7888...  1.6780 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1427...  Training loss: 1.7916...  1.7186 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1428...  Training loss: 1.8266...  1.7682 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1429...  Training loss: 1.7729...  1.7402 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1430...  Training loss: 1.7819...  1.8053 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1431...  Training loss: 1.8295...  1.7241 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1432...  Training loss: 1.7541...  1.7316 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1433...  Training loss: 1.8164...  1.6729 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1434...  Training loss: 1.7998...  1.7146 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1435...  Training loss: 1.7936...  1.7702 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1436...  Training loss: 1.8455...  1.7256 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1437...  Training loss: 1.7966...  1.7592 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1438...  Training loss: 1.8642...  1.6845 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1439...  Training loss: 1.8084...  1.7116 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1440...  Training loss: 1.8136...  1.7476 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1441...  Training loss: 1.8008...  1.7206 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1442...  Training loss: 1.8180...  1.7557 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1443...  Training loss: 1.8182...  1.6760 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1444...  Training loss: 1.7839...  1.6820 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1445...  Training loss: 1.7948...  1.6895 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1446...  Training loss: 1.8471...  1.7286 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1447...  Training loss: 1.8130...  1.7567 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1448...  Training loss: 1.8658...  1.7136 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1449...  Training loss: 1.8332...  1.7487 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1450...  Training loss: 1.8152...  1.6740 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1451...  Training loss: 1.7994...  1.7261 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1452...  Training loss: 1.8322...  1.7827 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1453...  Training loss: 1.8366...  1.7166 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1454...  Training loss: 1.7847...  1.7527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1455...  Training loss: 1.7972...  1.6920 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1456...  Training loss: 1.8093...  1.7191 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1457...  Training loss: 1.8417...  1.7817 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1458...  Training loss: 1.8293...  1.7346 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1459...  Training loss: 1.8301...  1.7707 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1460...  Training loss: 1.7951...  1.7166 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1461...  Training loss: 1.8055...  1.7687 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1462...  Training loss: 1.8371...  1.6915 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1463...  Training loss: 1.8136...  1.7141 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1464...  Training loss: 1.8172...  1.7657 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1465...  Training loss: 1.7895...  1.7121 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1466...  Training loss: 1.8055...  1.7712 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1467...  Training loss: 1.7746...  1.7000 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1468...  Training loss: 1.8295...  1.8349 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1469...  Training loss: 1.7641...  1.8078 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1470...  Training loss: 1.7951...  1.7537 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1471...  Training loss: 1.7813...  1.8118 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1472...  Training loss: 1.7823...  1.6920 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1473...  Training loss: 1.7880...  1.7291 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1474...  Training loss: 1.7816...  1.7777 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1475...  Training loss: 1.7766...  1.7201 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1476...  Training loss: 1.8220...  1.7466 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1477...  Training loss: 1.7648...  1.7081 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1478...  Training loss: 1.7949...  1.7341 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1479...  Training loss: 1.7839...  1.6795 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1480...  Training loss: 1.7857...  1.7256 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1481...  Training loss: 1.7845...  1.7637 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1482...  Training loss: 1.8071...  1.7131 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1483...  Training loss: 1.7996...  1.7707 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1484...  Training loss: 1.7695...  1.6795 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1485...  Training loss: 1.7961...  1.7386 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1486...  Training loss: 1.7615...  1.7772 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1487...  Training loss: 1.8023...  1.7166 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1488...  Training loss: 1.7952...  1.7943 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1489...  Training loss: 1.7923...  1.6699 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1490...  Training loss: 1.7763...  1.6795 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1491...  Training loss: 1.7767...  1.6775 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1492...  Training loss: 1.8063...  1.7346 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1493...  Training loss: 1.7842...  1.7908 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1494...  Training loss: 1.8042...  1.7281 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1495...  Training loss: 1.8063...  1.7832 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1496...  Training loss: 1.8158...  1.6985 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1497...  Training loss: 1.8039...  1.7121 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1498...  Training loss: 1.7849...  1.7582 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1499...  Training loss: 1.7898...  1.7211 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1500...  Training loss: 1.7737...  1.7602 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1501...  Training loss: 1.7806...  1.6654 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1502...  Training loss: 1.7523...  1.6920 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1503...  Training loss: 1.8082...  1.7522 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1504...  Training loss: 1.7866...  1.7356 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1505...  Training loss: 1.8008...  1.7431 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1506...  Training loss: 1.7915...  1.7137 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1507...  Training loss: 1.8062...  1.7497 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1508...  Training loss: 1.7630...  1.6775 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1509...  Training loss: 1.7581...  1.7487 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1510...  Training loss: 1.8179...  1.7542 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1511...  Training loss: 1.7949...  1.7237 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1512...  Training loss: 1.7519...  1.7587 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1513...  Training loss: 1.8122...  1.6694 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1514...  Training loss: 1.8112...  1.7326 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1515...  Training loss: 1.7916...  1.7662 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1516...  Training loss: 1.7857...  1.7868 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1517...  Training loss: 1.7602...  1.7667 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1518...  Training loss: 1.7519...  1.6950 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1519...  Training loss: 1.8147...  1.7466 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1520...  Training loss: 1.8006...  1.7657 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1521...  Training loss: 1.7949...  1.7416 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1522...  Training loss: 1.7926...  1.7411 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1523...  Training loss: 1.8108...  1.7176 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1524...  Training loss: 1.8037...  1.7517 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1525...  Training loss: 1.8150...  1.6975 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1526...  Training loss: 1.7868...  1.7467 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1527...  Training loss: 1.8399...  1.7582 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1528...  Training loss: 1.7931...  1.7271 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1529...  Training loss: 1.7907...  1.7767 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1530...  Training loss: 1.7928...  1.6845 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1531...  Training loss: 1.7688...  1.7732 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1532...  Training loss: 1.8038...  1.7436 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1533...  Training loss: 1.8073...  1.7557 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1534...  Training loss: 1.8253...  1.7617 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1535...  Training loss: 1.8052...  1.6724 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1536...  Training loss: 1.7895...  1.6604 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1537...  Training loss: 1.7697...  1.6750 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1538...  Training loss: 1.8142...  1.7441 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1539...  Training loss: 1.7986...  1.7712 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1540...  Training loss: 1.7927...  1.7371 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1541...  Training loss: 1.7876...  1.7587 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1542...  Training loss: 1.7824...  1.6755 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1543...  Training loss: 1.8036...  1.7171 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1544...  Training loss: 1.7855...  1.7552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1545...  Training loss: 1.7576...  1.7397 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1546...  Training loss: 1.8217...  1.7552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1547...  Training loss: 1.8144...  1.6699 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1548...  Training loss: 1.7839...  1.7321 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1549...  Training loss: 1.8017...  1.7772 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1550...  Training loss: 1.7897...  1.7296 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1551...  Training loss: 1.7899...  1.7752 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1552...  Training loss: 1.7877...  1.7281 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1553...  Training loss: 1.8086...  1.7832 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1554...  Training loss: 1.8554...  1.6985 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1555...  Training loss: 1.7805...  1.7276 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1556...  Training loss: 1.7888...  1.7712 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1557...  Training loss: 1.7682...  1.7181 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1558...  Training loss: 1.7705...  1.7441 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1559...  Training loss: 1.8190...  1.6745 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1560...  Training loss: 1.7933...  1.7171 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1561...  Training loss: 1.8055...  1.7637 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1562...  Training loss: 1.7925...  1.7617 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1563...  Training loss: 1.7789...  1.8068 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1564...  Training loss: 1.7997...  1.6945 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1565...  Training loss: 1.7659...  1.7286 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1566...  Training loss: 1.7568...  1.7597 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1567...  Training loss: 1.7584...  1.7361 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1568...  Training loss: 1.7761...  1.7657 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1569...  Training loss: 1.7876...  1.7221 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1570...  Training loss: 1.8086...  1.7742 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1571...  Training loss: 1.7834...  1.6930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1572...  Training loss: 1.7684...  1.7442 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1573...  Training loss: 1.7916...  1.7943 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1574...  Training loss: 1.7698...  1.7461 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1575...  Training loss: 1.7735...  1.8058 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1576...  Training loss: 1.7798...  1.7030 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1577...  Training loss: 1.7908...  1.7136 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1578...  Training loss: 1.7635...  1.7471 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1579...  Training loss: 1.7770...  1.7286 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1580...  Training loss: 1.7659...  1.7948 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1581...  Training loss: 1.7550...  1.6945 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1582...  Training loss: 1.7925...  1.6865 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1583...  Training loss: 1.7760...  1.6980 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1584...  Training loss: 1.7630...  1.7231 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1585...  Training loss: 1.8839...  1.7582 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1586...  Training loss: 1.7718...  1.6990 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1587...  Training loss: 1.7729...  1.7637 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1588...  Training loss: 1.7742...  1.6980 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1589...  Training loss: 1.7741...  1.7401 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1590...  Training loss: 1.7449...  1.8008 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1591...  Training loss: 1.7869...  1.7341 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1592...  Training loss: 1.7608...  1.7797 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1593...  Training loss: 1.8151...  1.6885 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1594...  Training loss: 1.7746...  1.7035 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1595...  Training loss: 1.7407...  1.7672 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1596...  Training loss: 1.7591...  1.7376 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1597...  Training loss: 1.7718...  1.7837 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1598...  Training loss: 1.8020...  1.7376 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1599...  Training loss: 1.7777...  1.7712 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1600...  Training loss: 1.7541...  1.6800 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1601...  Training loss: 1.7731...  1.6689 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1602...  Training loss: 1.8040...  1.7542 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1603...  Training loss: 1.7746...  1.7075 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1604...  Training loss: 1.7884...  1.7451 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1605...  Training loss: 1.7583...  1.6624 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1606...  Training loss: 1.7969...  1.7085 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1607...  Training loss: 1.7618...  1.7406 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1608...  Training loss: 1.7784...  1.7291 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1609...  Training loss: 1.7666...  1.7767 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1610...  Training loss: 1.7380...  1.6714 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1611...  Training loss: 1.7558...  1.7090 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1612...  Training loss: 1.7883...  1.7647 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1613...  Training loss: 1.8047...  1.7176 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1614...  Training loss: 1.7993...  1.7767 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1615...  Training loss: 1.7729...  1.7271 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1616...  Training loss: 1.7518...  1.7602 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1617...  Training loss: 1.7924...  1.6855 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1618...  Training loss: 1.7866...  1.7316 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1619...  Training loss: 1.7589...  1.7727 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1620...  Training loss: 1.7630...  1.7110 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1621...  Training loss: 1.7567...  1.7587 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1622...  Training loss: 1.7408...  1.6714 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1623...  Training loss: 1.7254...  1.7141 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1624...  Training loss: 1.7483...  1.7547 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1625...  Training loss: 1.7406...  1.7321 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1626...  Training loss: 1.7924...  1.7517 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1627...  Training loss: 1.7471...  1.6950 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1628...  Training loss: 1.7362...  1.6855 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1629...  Training loss: 1.7685...  1.6494 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1630...  Training loss: 1.7154...  1.7311 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1631...  Training loss: 1.7683...  1.7682 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1632...  Training loss: 1.7572...  1.7461 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1633...  Training loss: 1.7412...  1.7807 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1634...  Training loss: 1.7966...  1.6810 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1635...  Training loss: 1.7577...  1.7176 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1636...  Training loss: 1.8206...  1.7807 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1637...  Training loss: 1.7654...  1.7572 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1638...  Training loss: 1.7518...  1.7692 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1639...  Training loss: 1.7485...  1.6957 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1640...  Training loss: 1.7671...  1.7101 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1641...  Training loss: 1.7806...  1.7782 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1642...  Training loss: 1.7509...  1.7281 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1643...  Training loss: 1.7547...  1.7978 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1644...  Training loss: 1.8043...  1.7271 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1645...  Training loss: 1.7667...  1.7236 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1646...  Training loss: 1.8150...  1.6800 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1647...  Training loss: 1.7932...  1.7416 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1648...  Training loss: 1.7739...  1.7416 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1649...  Training loss: 1.7546...  1.7146 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1650...  Training loss: 1.7874...  1.7757 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1651...  Training loss: 1.7904...  1.6975 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1652...  Training loss: 1.7450...  1.7331 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1653...  Training loss: 1.7554...  1.7635 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1654...  Training loss: 1.7631...  1.7126 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1655...  Training loss: 1.7873...  1.7978 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1656...  Training loss: 1.7903...  1.7060 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1657...  Training loss: 1.7966...  1.7407 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1658...  Training loss: 1.7526...  1.7822 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1659...  Training loss: 1.7587...  1.7181 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1660...  Training loss: 1.7880...  1.7677 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1661...  Training loss: 1.7502...  1.7080 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1662...  Training loss: 1.7705...  1.7537 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1663...  Training loss: 1.7329...  1.6830 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1664...  Training loss: 1.7541...  1.7296 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1665...  Training loss: 1.7210...  1.7602 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1666...  Training loss: 1.7790...  1.7231 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1667...  Training loss: 1.7324...  1.7577 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1668...  Training loss: 1.7564...  1.7000 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1669...  Training loss: 1.7323...  1.7311 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1670...  Training loss: 1.7479...  1.7687 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1671...  Training loss: 1.7595...  1.7286 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1672...  Training loss: 1.7362...  1.7642 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1673...  Training loss: 1.7256...  1.6920 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1674...  Training loss: 1.7699...  1.6900 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1675...  Training loss: 1.7365...  1.7732 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1676...  Training loss: 1.7483...  1.7416 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1677...  Training loss: 1.7414...  1.7682 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1678...  Training loss: 1.7453...  1.7211 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1679...  Training loss: 1.7323...  1.7852 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1680...  Training loss: 1.7776...  1.6795 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1681...  Training loss: 1.7535...  1.7321 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1682...  Training loss: 1.7275...  1.7441 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1683...  Training loss: 1.7382...  1.7251 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1684...  Training loss: 1.7240...  1.7817 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1685...  Training loss: 1.7626...  1.6805 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1686...  Training loss: 1.7521...  1.7296 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1687...  Training loss: 1.7396...  1.8138 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1688...  Training loss: 1.7377...  1.7406 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1689...  Training loss: 1.7434...  1.7842 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1690...  Training loss: 1.7644...  1.7101 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1691...  Training loss: 1.7490...  1.7837 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1692...  Training loss: 1.7542...  1.6669 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1693...  Training loss: 1.7697...  1.7261 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1694...  Training loss: 1.7692...  1.7717 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1695...  Training loss: 1.7478...  1.7251 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1696...  Training loss: 1.7327...  1.7667 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1697...  Training loss: 1.7329...  1.7070 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1698...  Training loss: 1.7337...  1.7251 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1699...  Training loss: 1.7418...  1.7672 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1700...  Training loss: 1.7175...  1.7507 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1701...  Training loss: 1.7600...  1.7818 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1702...  Training loss: 1.7435...  1.6845 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1703...  Training loss: 1.7547...  1.7161 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1704...  Training loss: 1.7350...  1.7702 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1705...  Training loss: 1.7559...  1.7421 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1706...  Training loss: 1.7243...  1.8153 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1707...  Training loss: 1.7127...  1.7161 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1708...  Training loss: 1.7791...  1.7873 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1709...  Training loss: 1.7408...  1.7080 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1710...  Training loss: 1.7083...  1.7191 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1711...  Training loss: 1.7697...  1.7657 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1712...  Training loss: 1.7685...  1.7236 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1713...  Training loss: 1.7458...  1.7446 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1714...  Training loss: 1.7411...  1.6830 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1715...  Training loss: 1.7073...  1.7366 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1716...  Training loss: 1.7113...  1.7657 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1717...  Training loss: 1.7634...  1.7231 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1718...  Training loss: 1.7524...  1.7737 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1719...  Training loss: 1.7581...  1.6805 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1720...  Training loss: 1.7481...  1.6945 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1721...  Training loss: 1.7738...  1.7502 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1722...  Training loss: 1.7604...  1.7065 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1723...  Training loss: 1.7702...  1.7752 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1724...  Training loss: 1.7436...  1.7081 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1725...  Training loss: 1.7928...  1.7457 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1726...  Training loss: 1.7472...  1.6935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1727...  Training loss: 1.7512...  1.7316 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1728...  Training loss: 1.7508...  1.7813 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1729...  Training loss: 1.7354...  1.7492 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1730...  Training loss: 1.7718...  1.7697 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1731...  Training loss: 1.7567...  1.6905 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1732...  Training loss: 1.7789...  1.7241 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1733...  Training loss: 1.7617...  1.7832 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1734...  Training loss: 1.7455...  1.7161 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1735...  Training loss: 1.7156...  1.7692 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1736...  Training loss: 1.7597...  1.7080 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1737...  Training loss: 1.7629...  1.7451 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1738...  Training loss: 1.7518...  1.6714 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1739...  Training loss: 1.7408...  1.7316 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1740...  Training loss: 1.7431...  1.7812 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1741...  Training loss: 1.7507...  1.7331 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1742...  Training loss: 1.7455...  1.7396 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1743...  Training loss: 1.7187...  1.6805 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1744...  Training loss: 1.7831...  1.7316 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1745...  Training loss: 1.7804...  1.7882 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1746...  Training loss: 1.7512...  1.7316 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1747...  Training loss: 1.7711...  1.7627 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1748...  Training loss: 1.7453...  1.6955 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1749...  Training loss: 1.7462...  1.7497 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1750...  Training loss: 1.7473...  1.7797 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1751...  Training loss: 1.7619...  1.7507 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1752...  Training loss: 1.8133...  1.7857 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1753...  Training loss: 1.7493...  1.7055 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1754...  Training loss: 1.7407...  1.7687 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1755...  Training loss: 1.7299...  1.6654 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1756...  Training loss: 1.7287...  1.7311 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1757...  Training loss: 1.7742...  1.7577 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1758...  Training loss: 1.7575...  1.7181 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1759...  Training loss: 1.7723...  1.7401 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1760...  Training loss: 1.7327...  1.6669 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1761...  Training loss: 1.7370...  1.7342 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1762...  Training loss: 1.7654...  1.7762 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1763...  Training loss: 1.7174...  1.7246 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1764...  Training loss: 1.7051...  1.7707 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1765...  Training loss: 1.7158...  1.6750 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1766...  Training loss: 1.7344...  1.7171 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1767...  Training loss: 1.7332...  1.7552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1768...  Training loss: 1.7672...  1.7411 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1769...  Training loss: 1.7435...  1.7532 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1770...  Training loss: 1.7254...  1.7271 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1771...  Training loss: 1.7554...  1.7832 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1772...  Training loss: 1.7287...  1.6880 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1773...  Training loss: 1.7309...  1.7161 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1774...  Training loss: 1.7387...  1.7837 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1775...  Training loss: 1.7471...  1.7076 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1776...  Training loss: 1.7250...  1.8103 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1777...  Training loss: 1.7473...  1.7040 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1778...  Training loss: 1.7234...  1.7171 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1779...  Training loss: 1.7144...  1.7657 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1780...  Training loss: 1.7431...  1.7276 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1781...  Training loss: 1.7312...  1.8033 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1782...  Training loss: 1.7241...  1.7657 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1783...  Training loss: 1.8447...  1.7527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1784...  Training loss: 1.7460...  1.6885 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1785...  Training loss: 1.7376...  1.7356 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1786...  Training loss: 1.7329...  1.7547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1787...  Training loss: 1.7308...  1.7050 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1788...  Training loss: 1.7082...  1.7276 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1789...  Training loss: 1.7522...  1.6574 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1790...  Training loss: 1.7310...  1.7326 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1791...  Training loss: 1.7608...  1.7682 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1792...  Training loss: 1.7337...  1.7211 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1793...  Training loss: 1.7252...  1.7411 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1794...  Training loss: 1.7370...  1.6745 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1795...  Training loss: 1.7293...  1.7131 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1796...  Training loss: 1.7701...  1.7627 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1797...  Training loss: 1.7363...  1.7532 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1798...  Training loss: 1.7098...  1.7702 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1799...  Training loss: 1.7316...  1.7391 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1800...  Training loss: 1.7693...  1.7802 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1801...  Training loss: 1.7279...  1.6138 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1802...  Training loss: 1.7463...  1.7376 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1803...  Training loss: 1.7199...  1.7557 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1804...  Training loss: 1.7637...  1.7105 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1805...  Training loss: 1.7258...  1.7286 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1806...  Training loss: 1.7363...  1.6759 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1807...  Training loss: 1.7362...  1.7091 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1808...  Training loss: 1.7158...  1.7697 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1809...  Training loss: 1.7034...  1.7296 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1810...  Training loss: 1.7523...  1.7722 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1811...  Training loss: 1.7643...  1.6870 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1812...  Training loss: 1.7566...  1.7256 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1813...  Training loss: 1.7413...  1.7787 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1814...  Training loss: 1.7096...  1.7497 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1815...  Training loss: 1.7529...  1.7707 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1816...  Training loss: 1.7606...  1.7251 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1817...  Training loss: 1.7198...  1.7787 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1818...  Training loss: 1.7280...  1.6790 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1819...  Training loss: 1.7161...  1.7437 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1820...  Training loss: 1.7028...  1.7717 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1821...  Training loss: 1.6937...  1.7361 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1822...  Training loss: 1.7160...  1.7822 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1823...  Training loss: 1.7123...  1.6905 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1824...  Training loss: 1.7546...  1.7216 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1825...  Training loss: 1.7203...  1.7567 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1826...  Training loss: 1.6942...  1.7356 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1827...  Training loss: 1.7368...  1.7607 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1828...  Training loss: 1.6836...  1.7391 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1829...  Training loss: 1.7288...  1.7612 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1830...  Training loss: 1.7079...  1.7000 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1831...  Training loss: 1.7150...  1.7391 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1832...  Training loss: 1.7614...  1.7812 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1833...  Training loss: 1.7166...  1.7522 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1834...  Training loss: 1.7744...  1.7817 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1835...  Training loss: 1.7247...  1.6920 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1836...  Training loss: 1.7212...  1.7040 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1837...  Training loss: 1.7135...  1.8068 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1838...  Training loss: 1.7290...  1.7261 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1839...  Training loss: 1.7498...  1.7822 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1840...  Training loss: 1.7080...  1.6865 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1841...  Training loss: 1.7059...  1.6915 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1842...  Training loss: 1.7631...  1.7697 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1843...  Training loss: 1.7306...  1.7326 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1844...  Training loss: 1.7721...  1.7827 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1845...  Training loss: 1.7514...  1.7276 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1846...  Training loss: 1.7369...  1.7471 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1847...  Training loss: 1.7287...  1.6775 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1848...  Training loss: 1.7380...  1.7321 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1849...  Training loss: 1.7423...  1.7677 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1850...  Training loss: 1.7006...  1.7191 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1851...  Training loss: 1.7248...  1.7953 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1852...  Training loss: 1.7253...  1.6734 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1853...  Training loss: 1.7606...  1.7341 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1854...  Training loss: 1.7505...  1.7812 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1855...  Training loss: 1.7604...  1.7261 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1856...  Training loss: 1.7127...  1.7822 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1857...  Training loss: 1.7239...  1.6950 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1858...  Training loss: 1.7378...  1.7306 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1859...  Training loss: 1.7323...  1.7492 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1860...  Training loss: 1.7312...  1.7552 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1861...  Training loss: 1.6991...  1.7466 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1862...  Training loss: 1.7142...  1.7221 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1863...  Training loss: 1.6843...  1.7797 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1864...  Training loss: 1.7416...  1.6900 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1865...  Training loss: 1.6848...  1.7156 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1866...  Training loss: 1.7256...  1.7722 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1867...  Training loss: 1.6979...  1.7146 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1868...  Training loss: 1.7127...  1.7832 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1869...  Training loss: 1.7065...  1.6995 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1870...  Training loss: 1.7059...  1.6985 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1871...  Training loss: 1.6873...  1.7612 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1872...  Training loss: 1.7376...  1.7416 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1873...  Training loss: 1.7014...  1.7777 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1874...  Training loss: 1.7091...  1.7451 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1875...  Training loss: 1.6961...  1.7672 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1876...  Training loss: 1.6857...  1.6830 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1877...  Training loss: 1.7009...  1.7306 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1878...  Training loss: 1.7236...  1.7602 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1879...  Training loss: 1.7159...  1.7176 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1880...  Training loss: 1.6856...  1.7873 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1881...  Training loss: 1.7031...  1.7090 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1882...  Training loss: 1.6830...  1.7246 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1883...  Training loss: 1.7242...  1.7697 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1884...  Training loss: 1.7155...  1.7191 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1885...  Training loss: 1.7078...  1.8008 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1886...  Training loss: 1.7051...  1.7005 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1887...  Training loss: 1.7109...  1.7196 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1888...  Training loss: 1.7259...  1.7762 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1889...  Training loss: 1.7186...  1.7321 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1890...  Training loss: 1.7215...  1.7712 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1891...  Training loss: 1.7289...  1.7201 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1892...  Training loss: 1.7336...  1.7587 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1893...  Training loss: 1.7025...  1.7010 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1894...  Training loss: 1.7030...  1.7371 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1895...  Training loss: 1.7089...  1.7562 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1896...  Training loss: 1.6999...  1.7231 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1897...  Training loss: 1.7064...  1.7657 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1898...  Training loss: 1.6777...  1.6860 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1899...  Training loss: 1.7211...  1.7276 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1900...  Training loss: 1.7163...  1.7727 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1901...  Training loss: 1.7222...  1.7371 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1902...  Training loss: 1.7054...  1.7732 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1903...  Training loss: 1.7198...  1.7211 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1904...  Training loss: 1.6839...  1.7236 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1905...  Training loss: 1.6856...  1.7582 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1906...  Training loss: 1.7297...  1.7692 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1907...  Training loss: 1.7026...  1.7827 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1908...  Training loss: 1.6669...  1.7507 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1909...  Training loss: 1.7462...  1.7918 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1910...  Training loss: 1.7257...  1.6820 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1911...  Training loss: 1.7031...  1.7231 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1912...  Training loss: 1.7088...  1.7652 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1913...  Training loss: 1.6756...  1.7442 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1914...  Training loss: 1.6945...  1.7978 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1915...  Training loss: 1.7235...  1.6780 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1916...  Training loss: 1.7278...  1.7176 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1917...  Training loss: 1.7288...  1.7898 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1918...  Training loss: 1.7188...  1.7346 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1919...  Training loss: 1.7452...  1.7642 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1920...  Training loss: 1.7139...  1.6980 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1921...  Training loss: 1.7387...  1.6795 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1922...  Training loss: 1.7138...  1.6905 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1923...  Training loss: 1.7626...  1.7371 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1924...  Training loss: 1.7116...  1.7532 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1925...  Training loss: 1.7263...  1.7141 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1926...  Training loss: 1.7281...  1.7787 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1927...  Training loss: 1.6996...  1.6830 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1928...  Training loss: 1.7340...  1.7502 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1929...  Training loss: 1.7362...  1.7687 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1930...  Training loss: 1.7514...  1.7221 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1931...  Training loss: 1.7240...  1.7637 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1932...  Training loss: 1.7049...  1.7035 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1933...  Training loss: 1.6887...  1.7241 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1934...  Training loss: 1.7315...  1.7592 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1935...  Training loss: 1.7258...  1.7592 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1936...  Training loss: 1.7249...  1.7582 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1937...  Training loss: 1.7221...  1.7326 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1938...  Training loss: 1.7039...  1.7673 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1939...  Training loss: 1.7338...  1.6684 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1940...  Training loss: 1.7199...  1.7326 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1941...  Training loss: 1.6855...  1.7692 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1942...  Training loss: 1.7430...  1.7131 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1943...  Training loss: 1.7432...  1.7968 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1944...  Training loss: 1.7053...  1.6885 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1945...  Training loss: 1.7284...  1.7256 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1946...  Training loss: 1.7286...  1.7782 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1947...  Training loss: 1.7130...  1.7186 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1948...  Training loss: 1.7202...  1.7527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1949...  Training loss: 1.7246...  1.6800 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1950...  Training loss: 1.7780...  1.7231 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1951...  Training loss: 1.7062...  1.7802 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1952...  Training loss: 1.7066...  1.7386 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1953...  Training loss: 1.6878...  1.7627 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1954...  Training loss: 1.6955...  1.7191 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1955...  Training loss: 1.7315...  1.7707 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1956...  Training loss: 1.7265...  1.7019 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1957...  Training loss: 1.7293...  1.7221 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1958...  Training loss: 1.7047...  1.7898 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1959...  Training loss: 1.6985...  1.7306 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1960...  Training loss: 1.7187...  1.8003 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1961...  Training loss: 1.6850...  1.6900 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1962...  Training loss: 1.6773...  1.7356 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1963...  Training loss: 1.6806...  1.7627 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1964...  Training loss: 1.7069...  1.7552 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1965...  Training loss: 1.7179...  1.7852 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1966...  Training loss: 1.7373...  1.6815 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1967...  Training loss: 1.7086...  1.7005 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1968...  Training loss: 1.6898...  1.6780 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1969...  Training loss: 1.7268...  1.7261 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1970...  Training loss: 1.6962...  1.7381 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1971...  Training loss: 1.7011...  1.7161 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1972...  Training loss: 1.6977...  1.7627 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1973...  Training loss: 1.7219...  1.6825 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1974...  Training loss: 1.6821...  1.7136 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1975...  Training loss: 1.7214...  1.7888 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1976...  Training loss: 1.6871...  1.7356 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1977...  Training loss: 1.6846...  1.7868 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1978...  Training loss: 1.7181...  1.6750 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1979...  Training loss: 1.7056...  1.7271 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1980...  Training loss: 1.6878...  1.7878 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1981...  Training loss: 1.8058...  1.7266 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1982...  Training loss: 1.7001...  1.7532 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1983...  Training loss: 1.6994...  1.7181 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1984...  Training loss: 1.7047...  1.7757 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1985...  Training loss: 1.6923...  1.6840 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1986...  Training loss: 1.6738...  1.7331 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1987...  Training loss: 1.7204...  1.7762 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1988...  Training loss: 1.6866...  1.7507 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1989...  Training loss: 1.7303...  1.7582 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1990...  Training loss: 1.6896...  1.6805 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1991...  Training loss: 1.6889...  1.7346 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1992...  Training loss: 1.6907...  1.7857 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1993...  Training loss: 1.6938...  1.7346 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1994...  Training loss: 1.7351...  1.7602 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1995...  Training loss: 1.6975...  1.6745 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1996...  Training loss: 1.6852...  1.7181 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1997...  Training loss: 1.7179...  1.7782 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1998...  Training loss: 1.7360...  1.7286 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1999...  Training loss: 1.6990...  1.7647 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2000...  Training loss: 1.7153...  1.7166 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2001...  Training loss: 1.6988...  1.6368 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2002...  Training loss: 1.7340...  1.6228 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2003...  Training loss: 1.7000...  1.6780 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2004...  Training loss: 1.6943...  1.7427 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2005...  Training loss: 1.7025...  1.7065 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2006...  Training loss: 1.6654...  1.7647 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2007...  Training loss: 1.6726...  1.6990 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2008...  Training loss: 1.7168...  1.7271 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2009...  Training loss: 1.7370...  1.7652 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2010...  Training loss: 1.7142...  1.7432 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2011...  Training loss: 1.7012...  1.7832 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2012...  Training loss: 1.6813...  1.6930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2013...  Training loss: 1.7124...  1.6765 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2014...  Training loss: 1.7173...  1.6740 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2015...  Training loss: 1.6890...  1.7056 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2016...  Training loss: 1.6954...  1.7682 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2017...  Training loss: 1.6917...  1.7251 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2018...  Training loss: 1.6665...  1.7572 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2019...  Training loss: 1.6554...  1.6840 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2020...  Training loss: 1.6791...  1.7261 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2021...  Training loss: 1.6758...  1.7662 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2022...  Training loss: 1.7125...  1.7281 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2023...  Training loss: 1.6809...  1.7923 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2024...  Training loss: 1.6647...  1.7050 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2025...  Training loss: 1.7087...  1.7065 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2026...  Training loss: 1.6457...  1.7707 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2027...  Training loss: 1.6873...  1.7166 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2028...  Training loss: 1.6856...  1.7471 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2029...  Training loss: 1.6816...  1.7050 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2030...  Training loss: 1.7388...  1.7637 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2031...  Training loss: 1.6713...  1.6985 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2032...  Training loss: 1.7381...  1.7191 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2033...  Training loss: 1.7107...  1.7647 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2034...  Training loss: 1.7001...  1.7306 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2035...  Training loss: 1.6813...  1.7672 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2036...  Training loss: 1.7002...  1.7055 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2037...  Training loss: 1.7158...  1.7296 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2038...  Training loss: 1.6849...  1.7787 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2039...  Training loss: 1.6716...  1.7481 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2040...  Training loss: 1.7294...  1.7893 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2041...  Training loss: 1.6858...  1.7111 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2042...  Training loss: 1.7448...  1.7286 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2043...  Training loss: 1.7224...  1.7737 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2044...  Training loss: 1.7107...  1.7727 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2045...  Training loss: 1.6846...  1.8149 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2046...  Training loss: 1.7151...  1.7306 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2047...  Training loss: 1.7128...  1.7747 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2048...  Training loss: 1.6761...  1.6800 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2049...  Training loss: 1.7024...  1.7446 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2050...  Training loss: 1.7004...  1.7847 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2051...  Training loss: 1.7409...  1.7582 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2052...  Training loss: 1.7132...  1.8058 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2053...  Training loss: 1.7260...  1.7035 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2054...  Training loss: 1.6788...  1.7266 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2055...  Training loss: 1.6967...  1.7341 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2056...  Training loss: 1.7183...  1.7371 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2057...  Training loss: 1.7008...  1.7426 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2058...  Training loss: 1.6965...  1.6815 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2059...  Training loss: 1.6601...  1.6930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2060...  Training loss: 1.6818...  1.6649 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2061...  Training loss: 1.6519...  1.7466 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2062...  Training loss: 1.7166...  1.7647 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2063...  Training loss: 1.6560...  1.7517 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2064...  Training loss: 1.6847...  1.7793 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2065...  Training loss: 1.6625...  1.7015 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2066...  Training loss: 1.6789...  1.7411 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2067...  Training loss: 1.6716...  1.7752 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2068...  Training loss: 1.6709...  1.7126 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2069...  Training loss: 1.6572...  1.7737 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2070...  Training loss: 1.7076...  1.6830 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2071...  Training loss: 1.6626...  1.7126 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2072...  Training loss: 1.6756...  1.7943 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2073...  Training loss: 1.6737...  1.7441 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2074...  Training loss: 1.6761...  1.7854 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2075...  Training loss: 1.6685...  1.7261 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2076...  Training loss: 1.6887...  1.7712 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2077...  Training loss: 1.6846...  1.6765 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2078...  Training loss: 1.6538...  1.7226 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2079...  Training loss: 1.6791...  1.7522 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2080...  Training loss: 1.6453...  1.7030 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2081...  Training loss: 1.6907...  1.7923 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2082...  Training loss: 1.6843...  1.6930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2083...  Training loss: 1.6671...  1.7326 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2084...  Training loss: 1.6724...  1.7722 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2085...  Training loss: 1.6750...  1.7221 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2086...  Training loss: 1.6875...  1.7782 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2087...  Training loss: 1.6846...  1.6932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2088...  Training loss: 1.6837...  1.7336 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2089...  Training loss: 1.7039...  1.7376 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2090...  Training loss: 1.7064...  1.7476 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2091...  Training loss: 1.6831...  1.7652 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2092...  Training loss: 1.6760...  1.7211 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2093...  Training loss: 1.6741...  1.7717 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2094...  Training loss: 1.6602...  1.6920 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2095...  Training loss: 1.6751...  1.7161 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2096...  Training loss: 1.6438...  1.7905 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2097...  Training loss: 1.6928...  1.7035 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2098...  Training loss: 1.6813...  1.7642 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2099...  Training loss: 1.6874...  1.6830 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2100...  Training loss: 1.6721...  1.7216 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2101...  Training loss: 1.6906...  1.7757 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2102...  Training loss: 1.6469...  1.7431 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2103...  Training loss: 1.6370...  1.7863 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2104...  Training loss: 1.7090...  1.6915 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2105...  Training loss: 1.6773...  1.7336 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2106...  Training loss: 1.6469...  1.7547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2107...  Training loss: 1.7088...  1.7266 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2108...  Training loss: 1.6987...  1.7822 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2109...  Training loss: 1.6726...  1.7346 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2110...  Training loss: 1.6635...  1.7682 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2111...  Training loss: 1.6448...  1.7030 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2112...  Training loss: 1.6456...  1.7261 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2113...  Training loss: 1.6975...  1.8088 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2114...  Training loss: 1.6991...  1.7471 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2115...  Training loss: 1.6936...  1.7797 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2116...  Training loss: 1.6958...  1.6890 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2117...  Training loss: 1.7107...  1.7005 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2118...  Training loss: 1.6932...  1.7682 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2119...  Training loss: 1.7097...  1.7376 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2120...  Training loss: 1.6787...  1.7928 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2121...  Training loss: 1.7312...  1.7296 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2122...  Training loss: 1.6895...  1.7852 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2123...  Training loss: 1.6835...  1.6740 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2124...  Training loss: 1.6920...  1.7336 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2125...  Training loss: 1.6633...  1.7742 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2126...  Training loss: 1.6999...  1.7216 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2127...  Training loss: 1.6977...  1.7852 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2128...  Training loss: 1.7200...  1.6845 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2129...  Training loss: 1.7012...  1.7251 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2130...  Training loss: 1.6719...  1.7712 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2131...  Training loss: 1.6544...  1.7401 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2132...  Training loss: 1.6979...  1.7973 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2133...  Training loss: 1.7057...  1.6915 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2134...  Training loss: 1.6863...  1.7411 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2135...  Training loss: 1.6817...  1.7933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2136...  Training loss: 1.6811...  1.7241 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2137...  Training loss: 1.6994...  1.7998 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2138...  Training loss: 1.6830...  1.7366 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2139...  Training loss: 1.6393...  1.8033 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2140...  Training loss: 1.7070...  1.7025 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2141...  Training loss: 1.7131...  1.7271 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2142...  Training loss: 1.6819...  1.7732 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2143...  Training loss: 1.7046...  1.7055 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2144...  Training loss: 1.6778...  1.7717 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2145...  Training loss: 1.6823...  1.6955 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2146...  Training loss: 1.6845...  1.7271 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2147...  Training loss: 1.6940...  1.7998 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2148...  Training loss: 1.7510...  1.7276 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2149...  Training loss: 1.6789...  1.7687 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2150...  Training loss: 1.6838...  1.6970 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2151...  Training loss: 1.6664...  1.7146 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2152...  Training loss: 1.6572...  1.7396 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2153...  Training loss: 1.7038...  1.7502 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2154...  Training loss: 1.6926...  1.7883 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2155...  Training loss: 1.7033...  1.7045 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2156...  Training loss: 1.6708...  1.7652 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2157...  Training loss: 1.6721...  1.6694 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2158...  Training loss: 1.7086...  1.7256 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2159...  Training loss: 1.6594...  1.7723 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2160...  Training loss: 1.6493...  1.7351 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2161...  Training loss: 1.6463...  1.7782 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2162...  Training loss: 1.6757...  1.7060 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2163...  Training loss: 1.6913...  1.7266 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2164...  Training loss: 1.7001...  1.7657 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2165...  Training loss: 1.6837...  1.7276 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2166...  Training loss: 1.6526...  1.7637 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2167...  Training loss: 1.6908...  1.7095 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2168...  Training loss: 1.6715...  1.7627 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2169...  Training loss: 1.6718...  1.7100 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2170...  Training loss: 1.6771...  1.7446 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2171...  Training loss: 1.6811...  1.7832 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2172...  Training loss: 1.6569...  1.7286 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2173...  Training loss: 1.6848...  1.7873 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2174...  Training loss: 1.6674...  1.7020 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2175...  Training loss: 1.6596...  1.7411 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2176...  Training loss: 1.6804...  1.7963 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2177...  Training loss: 1.6738...  1.7527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2178...  Training loss: 1.6650...  1.7923 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2179...  Training loss: 1.7801...  1.6745 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2180...  Training loss: 1.6724...  1.7106 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2181...  Training loss: 1.6761...  1.7672 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2182...  Training loss: 1.6817...  1.7637 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2183...  Training loss: 1.6678...  1.7953 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2184...  Training loss: 1.6355...  1.7131 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2185...  Training loss: 1.6868...  1.7582 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2186...  Training loss: 1.6704...  1.6785 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2187...  Training loss: 1.7048...  1.7542 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2188...  Training loss: 1.6674...  1.7582 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2189...  Training loss: 1.6493...  1.7171 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2190...  Training loss: 1.6544...  1.7747 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2191...  Training loss: 1.6635...  1.6765 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2192...  Training loss: 1.7060...  1.7231 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2193...  Training loss: 1.6730...  1.7873 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2194...  Training loss: 1.6540...  1.7436 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2195...  Training loss: 1.6786...  1.7933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2196...  Training loss: 1.7062...  1.7136 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2197...  Training loss: 1.6871...  1.7126 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2198...  Training loss: 1.6972...  1.7527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2199...  Training loss: 1.6634...  1.7456 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2200...  Training loss: 1.6980...  1.7697 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2201...  Training loss: 1.6616...  1.6569 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2202...  Training loss: 1.6826...  1.7677 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2203...  Training loss: 1.6753...  1.7105 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2204...  Training loss: 1.6443...  1.7166 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2205...  Training loss: 1.6605...  1.7692 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2206...  Training loss: 1.6911...  1.7572 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2207...  Training loss: 1.7095...  1.7802 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2208...  Training loss: 1.7038...  1.6960 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2209...  Training loss: 1.6799...  1.7146 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2210...  Training loss: 1.6487...  1.7441 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2211...  Training loss: 1.6984...  1.7386 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2212...  Training loss: 1.6926...  1.7522 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2213...  Training loss: 1.6709...  1.7050 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2214...  Training loss: 1.6741...  1.7597 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2215...  Training loss: 1.6580...  1.6800 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2216...  Training loss: 1.6385...  1.7336 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2217...  Training loss: 1.6286...  1.7777 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2218...  Training loss: 1.6382...  1.7131 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2219...  Training loss: 1.6525...  1.7562 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2220...  Training loss: 1.6859...  1.6810 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2221...  Training loss: 1.6501...  1.7146 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2222...  Training loss: 1.6383...  1.7602 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2223...  Training loss: 1.6845...  1.7637 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2224...  Training loss: 1.6183...  1.7787 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2225...  Training loss: 1.6634...  1.6885 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2226...  Training loss: 1.6475...  1.7241 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2227...  Training loss: 1.6547...  1.7717 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2228...  Training loss: 1.7069...  1.7562 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2229...  Training loss: 1.6497...  1.8038 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2230...  Training loss: 1.7162...  1.7271 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2231...  Training loss: 1.6725...  1.8063 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2232...  Training loss: 1.6756...  1.6905 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2233...  Training loss: 1.6519...  1.7366 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2234...  Training loss: 1.6710...  1.7677 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2235...  Training loss: 1.6902...  1.7427 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2236...  Training loss: 1.6526...  1.7476 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2237...  Training loss: 1.6513...  1.6900 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2238...  Training loss: 1.7110...  1.7106 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2239...  Training loss: 1.6675...  1.7928 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2240...  Training loss: 1.7215...  1.7461 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2241...  Training loss: 1.6964...  1.7852 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2242...  Training loss: 1.6767...  1.7236 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2243...  Training loss: 1.6667...  1.7261 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2244...  Training loss: 1.6862...  1.7677 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2245...  Training loss: 1.6979...  1.7426 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2246...  Training loss: 1.6446...  1.7822 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2247...  Training loss: 1.6598...  1.7306 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2248...  Training loss: 1.6524...  1.7587 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2249...  Training loss: 1.7195...  1.6795 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2250...  Training loss: 1.6937...  1.7517 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2251...  Training loss: 1.7006...  1.7747 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2252...  Training loss: 1.6589...  1.7356 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2253...  Training loss: 1.6681...  1.7552 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2254...  Training loss: 1.6746...  1.6810 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2255...  Training loss: 1.6739...  1.7171 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2256...  Training loss: 1.6597...  1.7848 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2257...  Training loss: 1.6442...  1.7271 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2258...  Training loss: 1.6656...  1.7702 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2259...  Training loss: 1.6343...  1.7311 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2260...  Training loss: 1.6737...  1.7647 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2261...  Training loss: 1.6288...  1.6930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2262...  Training loss: 1.6703...  1.7351 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2263...  Training loss: 1.6463...  1.7792 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2264...  Training loss: 1.6561...  1.7291 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2265...  Training loss: 1.6495...  1.7732 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2266...  Training loss: 1.6490...  1.7116 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2267...  Training loss: 1.6191...  1.7216 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2268...  Training loss: 1.6805...  1.7863 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2269...  Training loss: 1.6291...  1.7246 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2270...  Training loss: 1.6445...  1.7492 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2271...  Training loss: 1.6464...  1.6925 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2272...  Training loss: 1.6427...  1.7266 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2273...  Training loss: 1.6313...  1.7773 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2274...  Training loss: 1.6757...  1.7617 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2275...  Training loss: 1.6538...  1.7697 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2276...  Training loss: 1.6268...  1.7231 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2277...  Training loss: 1.6418...  1.7512 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2278...  Training loss: 1.6306...  1.6750 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2279...  Training loss: 1.6696...  1.7371 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2280...  Training loss: 1.6500...  1.7847 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2281...  Training loss: 1.6563...  1.7482 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2282...  Training loss: 1.6475...  1.7948 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2283...  Training loss: 1.6444...  1.7035 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2284...  Training loss: 1.6619...  1.7146 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2285...  Training loss: 1.6597...  1.7657 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2286...  Training loss: 1.6673...  1.7256 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2287...  Training loss: 1.6646...  1.7677 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2288...  Training loss: 1.6782...  1.6950 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2289...  Training loss: 1.6448...  1.7241 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2290...  Training loss: 1.6483...  1.7642 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2291...  Training loss: 1.6562...  1.7406 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2292...  Training loss: 1.6331...  1.7682 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2293...  Training loss: 1.6474...  1.7321 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2294...  Training loss: 1.6176...  1.7411 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2295...  Training loss: 1.6676...  1.6885 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2296...  Training loss: 1.6670...  1.7316 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2297...  Training loss: 1.6618...  1.7612 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2298...  Training loss: 1.6500...  1.7331 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2299...  Training loss: 1.6626...  1.7702 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2300...  Training loss: 1.6192...  1.6895 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2301...  Training loss: 1.6260...  1.7241 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2302...  Training loss: 1.6747...  1.7757 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2303...  Training loss: 1.6408...  1.7416 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2304...  Training loss: 1.6057...  1.7782 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2305...  Training loss: 1.6851...  1.7316 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2306...  Training loss: 1.6631...  1.7512 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2307...  Training loss: 1.6579...  1.6855 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2308...  Training loss: 1.6385...  1.7371 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2309...  Training loss: 1.6224...  1.7777 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2310...  Training loss: 1.6244...  1.7206 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2311...  Training loss: 1.6681...  1.7893 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2312...  Training loss: 1.6619...  1.6845 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2313...  Training loss: 1.6624...  1.7211 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2314...  Training loss: 1.6578...  1.7562 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2315...  Training loss: 1.6824...  1.7301 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2316...  Training loss: 1.6666...  1.7592 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2317...  Training loss: 1.6825...  1.7090 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2318...  Training loss: 1.6558...  1.7246 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2319...  Training loss: 1.7216...  1.7968 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2320...  Training loss: 1.6628...  1.7592 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2321...  Training loss: 1.6563...  1.7777 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2322...  Training loss: 1.6776...  1.7060 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2323...  Training loss: 1.6429...  1.7702 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2324...  Training loss: 1.6923...  1.6880 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2325...  Training loss: 1.6627...  1.7562 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2326...  Training loss: 1.6957...  1.8123 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2327...  Training loss: 1.6740...  1.7547 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2328...  Training loss: 1.6496...  1.7411 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2329...  Training loss: 1.6311...  1.6679 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2330...  Training loss: 1.6646...  1.7121 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2331...  Training loss: 1.6765...  1.7767 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2332...  Training loss: 1.6666...  1.7396 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2333...  Training loss: 1.6627...  1.8128 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2334...  Training loss: 1.6616...  1.6855 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2335...  Training loss: 1.6741...  1.7271 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2336...  Training loss: 1.6617...  1.8048 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2337...  Training loss: 1.6226...  1.7306 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2338...  Training loss: 1.6867...  1.7782 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2339...  Training loss: 1.6814...  1.7166 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2340...  Training loss: 1.6476...  1.7456 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2341...  Training loss: 1.6698...  1.7040 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2342...  Training loss: 1.6646...  1.7211 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2343...  Training loss: 1.6561...  1.7767 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2344...  Training loss: 1.6708...  1.7446 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2345...  Training loss: 1.6746...  1.8058 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2346...  Training loss: 1.7182...  1.7100 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2347...  Training loss: 1.6530...  1.7201 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2348...  Training loss: 1.6532...  1.7507 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2349...  Training loss: 1.6449...  1.7261 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2350...  Training loss: 1.6388...  1.7682 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2351...  Training loss: 1.6864...  1.7306 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2352...  Training loss: 1.6620...  1.7692 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2353...  Training loss: 1.6812...  1.6805 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2354...  Training loss: 1.6442...  1.7426 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2355...  Training loss: 1.6473...  1.7903 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2356...  Training loss: 1.6738...  1.7410 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2357...  Training loss: 1.6421...  1.7802 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2358...  Training loss: 1.6226...  1.6885 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2359...  Training loss: 1.6198...  1.7426 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2360...  Training loss: 1.6557...  1.7933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2361...  Training loss: 1.6607...  1.7376 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2362...  Training loss: 1.6783...  1.7772 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2363...  Training loss: 1.6579...  1.6980 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2364...  Training loss: 1.6354...  1.7166 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2365...  Training loss: 1.6710...  1.7647 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2366...  Training loss: 1.6425...  1.7346 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2367...  Training loss: 1.6457...  1.7657 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2368...  Training loss: 1.6445...  1.7296 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2369...  Training loss: 1.6577...  1.7807 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2370...  Training loss: 1.6352...  1.6795 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2371...  Training loss: 1.6500...  1.7471 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2372...  Training loss: 1.6371...  1.7812 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2373...  Training loss: 1.6362...  1.7296 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2374...  Training loss: 1.6701...  1.7878 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2375...  Training loss: 1.6526...  1.7085 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2376...  Training loss: 1.6461...  1.7105 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2377...  Training loss: 1.7474...  1.7707 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2378...  Training loss: 1.6544...  1.7381 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2379...  Training loss: 1.6435...  1.7837 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2380...  Training loss: 1.6561...  1.7060 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2381...  Training loss: 1.6379...  1.7221 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2382...  Training loss: 1.6129...  1.7807 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2383...  Training loss: 1.6454...  1.7642 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2384...  Training loss: 1.6422...  1.7963 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2385...  Training loss: 1.6795...  1.7306 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2386...  Training loss: 1.6403...  1.7672 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2387...  Training loss: 1.6239...  1.6639 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2388...  Training loss: 1.6327...  1.7451 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2389...  Training loss: 1.6440...  1.7682 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2390...  Training loss: 1.6960...  1.7266 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2391...  Training loss: 1.6470...  1.7697 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2392...  Training loss: 1.6240...  1.6950 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2393...  Training loss: 1.6562...  1.7251 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2394...  Training loss: 1.6814...  1.7557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2395...  Training loss: 1.6572...  1.7336 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2396...  Training loss: 1.6782...  1.7777 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2397...  Training loss: 1.6315...  1.6815 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2398...  Training loss: 1.6622...  1.7737 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2399...  Training loss: 1.6408...  1.6755 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2400...  Training loss: 1.6584...  1.7316 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2401...  Training loss: 1.6456...  1.6293 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2402...  Training loss: 1.6138...  1.6534 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2403...  Training loss: 1.6324...  1.7612 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2404...  Training loss: 1.6660...  1.6639 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2405...  Training loss: 1.6762...  1.7266 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2406...  Training loss: 1.6610...  1.7913 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2407...  Training loss: 1.6475...  1.7396 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2408...  Training loss: 1.6307...  1.7702 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2409...  Training loss: 1.6695...  1.6830 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2410...  Training loss: 1.6643...  1.7101 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2411...  Training loss: 1.6327...  1.7913 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2412...  Training loss: 1.6516...  1.7406 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2413...  Training loss: 1.6269...  1.7522 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2414...  Training loss: 1.6229...  1.7241 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2415...  Training loss: 1.5957...  1.7472 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2416...  Training loss: 1.6247...  1.6900 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2417...  Training loss: 1.6265...  1.7171 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2418...  Training loss: 1.6624...  1.7928 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2419...  Training loss: 1.6220...  1.7316 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2420...  Training loss: 1.6177...  1.7582 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2421...  Training loss: 1.6627...  1.6845 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2422...  Training loss: 1.6063...  1.7241 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2423...  Training loss: 1.6473...  1.7873 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2424...  Training loss: 1.6254...  1.7437 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2425...  Training loss: 1.6357...  1.7998 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2426...  Training loss: 1.6801...  1.6725 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2427...  Training loss: 1.6251...  1.7080 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2428...  Training loss: 1.6935...  1.7461 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2429...  Training loss: 1.6558...  1.7582 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2430...  Training loss: 1.6520...  1.7923 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2431...  Training loss: 1.6276...  1.7186 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2432...  Training loss: 1.6542...  1.7512 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2433...  Training loss: 1.6668...  1.6770 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2434...  Training loss: 1.6351...  1.7487 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2435...  Training loss: 1.6165...  1.7853 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2436...  Training loss: 1.6858...  1.7371 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2437...  Training loss: 1.6541...  1.7727 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2438...  Training loss: 1.6959...  1.6940 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2439...  Training loss: 1.6774...  1.7156 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2440...  Training loss: 1.6579...  1.7792 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2441...  Training loss: 1.6423...  1.7351 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2442...  Training loss: 1.6654...  1.7632 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2443...  Training loss: 1.6659...  1.6915 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2444...  Training loss: 1.6261...  1.6815 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2445...  Training loss: 1.6410...  1.6785 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2446...  Training loss: 1.6411...  1.7381 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2447...  Training loss: 1.6824...  1.7958 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2448...  Training loss: 1.6773...  1.7261 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2449...  Training loss: 1.6722...  1.7762 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2450...  Training loss: 1.6369...  1.6965 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2451...  Training loss: 1.6312...  1.7226 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2452...  Training loss: 1.6595...  1.7416 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2453...  Training loss: 1.6443...  1.7276 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2454...  Training loss: 1.6441...  1.7828 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2455...  Training loss: 1.6161...  1.6855 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2456...  Training loss: 1.6324...  1.7090 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2457...  Training loss: 1.6048...  1.7797 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2458...  Training loss: 1.6566...  1.7462 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2459...  Training loss: 1.5983...  1.7857 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2460...  Training loss: 1.6428...  1.7361 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2461...  Training loss: 1.6170...  1.7471 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2462...  Training loss: 1.6332...  1.6609 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2463...  Training loss: 1.6139...  1.7256 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2464...  Training loss: 1.6269...  1.7737 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2465...  Training loss: 1.6069...  1.7226 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2466...  Training loss: 1.6575...  1.7607 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2467...  Training loss: 1.6150...  1.6840 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2468...  Training loss: 1.6295...  1.7236 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2469...  Training loss: 1.6251...  1.7657 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2470...  Training loss: 1.6266...  1.7436 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2471...  Training loss: 1.6276...  1.7953 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2472...  Training loss: 1.6485...  1.6985 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2473...  Training loss: 1.6315...  1.7176 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2474...  Training loss: 1.6018...  1.8033 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2475...  Training loss: 1.6134...  1.7527 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2476...  Training loss: 1.6035...  1.7757 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2477...  Training loss: 1.6448...  1.7211 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2478...  Training loss: 1.6407...  1.7466 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2479...  Training loss: 1.6276...  1.6835 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2480...  Training loss: 1.6187...  1.7336 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2481...  Training loss: 1.6189...  1.7903 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2482...  Training loss: 1.6368...  1.7381 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2483...  Training loss: 1.6263...  1.7717 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2484...  Training loss: 1.6438...  1.7055 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2485...  Training loss: 1.6445...  1.7091 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2486...  Training loss: 1.6520...  1.7802 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2487...  Training loss: 1.6293...  1.7381 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2488...  Training loss: 1.6229...  1.7723 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2489...  Training loss: 1.6256...  1.6900 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2490...  Training loss: 1.6190...  1.6634 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2491...  Training loss: 1.6282...  1.6855 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2492...  Training loss: 1.6037...  1.7311 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2493...  Training loss: 1.6513...  1.7717 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2494...  Training loss: 1.6380...  1.7226 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2495...  Training loss: 1.6466...  1.7888 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2496...  Training loss: 1.6299...  1.6950 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2497...  Training loss: 1.6448...  1.7241 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2498...  Training loss: 1.6024...  1.7903 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2499...  Training loss: 1.5918...  1.7211 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2500...  Training loss: 1.6563...  1.8289 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2501...  Training loss: 1.6318...  1.6965 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2502...  Training loss: 1.5963...  1.7176 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2503...  Training loss: 1.6556...  1.7792 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2504...  Training loss: 1.6502...  1.7231 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2505...  Training loss: 1.6346...  1.7762 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2506...  Training loss: 1.6193...  1.7441 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2507...  Training loss: 1.5976...  1.7747 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2508...  Training loss: 1.6110...  1.6875 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2509...  Training loss: 1.6518...  1.7417 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2510...  Training loss: 1.6448...  1.7507 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2511...  Training loss: 1.6448...  1.7221 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2512...  Training loss: 1.6420...  1.7752 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2513...  Training loss: 1.6627...  1.6925 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2514...  Training loss: 1.6526...  1.7216 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2515...  Training loss: 1.6482...  1.7677 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2516...  Training loss: 1.6347...  1.7281 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2517...  Training loss: 1.6901...  1.7802 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2518...  Training loss: 1.6327...  1.6945 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2519...  Training loss: 1.6348...  1.7151 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2520...  Training loss: 1.6489...  1.7757 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2521...  Training loss: 1.6051...  1.7251 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2522...  Training loss: 1.6684...  1.7677 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2523...  Training loss: 1.6478...  1.7351 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2524...  Training loss: 1.6793...  1.7873 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2525...  Training loss: 1.6471...  1.6950 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2526...  Training loss: 1.6280...  1.7341 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2527...  Training loss: 1.5974...  1.7631 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2528...  Training loss: 1.6426...  1.7191 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2529...  Training loss: 1.6556...  1.7512 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2530...  Training loss: 1.6509...  1.6885 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2531...  Training loss: 1.6287...  1.7026 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2532...  Training loss: 1.6285...  1.7451 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2533...  Training loss: 1.6408...  1.7171 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2534...  Training loss: 1.6365...  1.7687 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2535...  Training loss: 1.6030...  1.6755 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2536...  Training loss: 1.6672...  1.6810 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2537...  Training loss: 1.6604...  1.6599 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2538...  Training loss: 1.6319...  1.7251 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2539...  Training loss: 1.6500...  1.7627 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2540...  Training loss: 1.6365...  1.7313 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2541...  Training loss: 1.6359...  1.7807 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2542...  Training loss: 1.6296...  1.6970 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2543...  Training loss: 1.6525...  1.7346 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2544...  Training loss: 1.7044...  1.7717 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2545...  Training loss: 1.6400...  1.7411 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2546...  Training loss: 1.6260...  1.7742 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2547...  Training loss: 1.6159...  1.6970 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2548...  Training loss: 1.6104...  1.7241 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2549...  Training loss: 1.6628...  1.7747 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2550...  Training loss: 1.6365...  1.7316 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2551...  Training loss: 1.6605...  1.7953 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2552...  Training loss: 1.6143...  1.7376 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2553...  Training loss: 1.6198...  1.7812 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2554...  Training loss: 1.6587...  1.6795 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2555...  Training loss: 1.6169...  1.7336 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2556...  Training loss: 1.5979...  1.7732 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2557...  Training loss: 1.5938...  1.7366 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2558...  Training loss: 1.6342...  1.7802 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2559...  Training loss: 1.6311...  1.6995 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2560...  Training loss: 1.6537...  1.7507 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2561...  Training loss: 1.6280...  1.7341 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2562...  Training loss: 1.6042...  1.7692 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2563...  Training loss: 1.6454...  1.7416 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2564...  Training loss: 1.6153...  1.6684 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2565...  Training loss: 1.6286...  1.7076 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2566...  Training loss: 1.6326...  1.7903 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2567...  Training loss: 1.6416...  1.7276 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2568...  Training loss: 1.6197...  1.7617 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2569...  Training loss: 1.6295...  1.7627 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2570...  Training loss: 1.6126...  1.7898 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2571...  Training loss: 1.6102...  1.6850 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2572...  Training loss: 1.6447...  1.7411 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2573...  Training loss: 1.6165...  1.7888 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2574...  Training loss: 1.6134...  1.7271 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2575...  Training loss: 1.7329...  1.7446 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2576...  Training loss: 1.6373...  1.6659 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2577...  Training loss: 1.6241...  1.7201 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2578...  Training loss: 1.6313...  1.7597 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2579...  Training loss: 1.6229...  1.7196 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2580...  Training loss: 1.5938...  1.7792 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2581...  Training loss: 1.6380...  1.6805 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2582...  Training loss: 1.6162...  1.6890 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2583...  Training loss: 1.6538...  1.6674 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2584...  Training loss: 1.6140...  1.7296 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2585...  Training loss: 1.6071...  1.7532 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2586...  Training loss: 1.6271...  1.7291 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2587...  Training loss: 1.6370...  1.7847 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2588...  Training loss: 1.6686...  1.6699 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2589...  Training loss: 1.6230...  1.7502 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2590...  Training loss: 1.5969...  1.7983 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2591...  Training loss: 1.6392...  1.7482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2592...  Training loss: 1.6595...  1.7852 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2593...  Training loss: 1.6321...  1.6855 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2594...  Training loss: 1.6575...  1.7231 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2595...  Training loss: 1.6280...  1.7978 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2596...  Training loss: 1.6480...  1.7446 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2597...  Training loss: 1.6156...  1.7792 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2598...  Training loss: 1.6331...  1.7005 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2599...  Training loss: 1.6342...  1.7612 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2600...  Training loss: 1.5949...  1.6729 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2601...  Training loss: 1.5973...  1.6824 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2602...  Training loss: 1.6506...  1.7306 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2603...  Training loss: 1.6517...  1.7056 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2604...  Training loss: 1.6595...  1.7813 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2605...  Training loss: 1.6219...  1.7075 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2606...  Training loss: 1.6029...  1.7411 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2607...  Training loss: 1.6390...  1.7672 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2608...  Training loss: 1.6542...  1.7467 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2609...  Training loss: 1.6199...  1.7682 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2610...  Training loss: 1.6290...  1.6765 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2611...  Training loss: 1.6041...  1.7221 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2612...  Training loss: 1.5932...  1.7381 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2613...  Training loss: 1.5918...  1.7441 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2614...  Training loss: 1.6079...  1.7787 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2615...  Training loss: 1.6036...  1.7256 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2616...  Training loss: 1.6493...  1.7767 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2617...  Training loss: 1.6030...  1.7396 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2618...  Training loss: 1.5915...  1.7637 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2619...  Training loss: 1.6355...  1.8013 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2620...  Training loss: 1.5823...  1.7431 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2621...  Training loss: 1.6242...  1.7652 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2622...  Training loss: 1.6071...  1.6820 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2623...  Training loss: 1.6095...  1.7301 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2624...  Training loss: 1.6579...  1.7933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2625...  Training loss: 1.6063...  1.7416 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2626...  Training loss: 1.6695...  1.7918 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2627...  Training loss: 1.6271...  1.6740 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2628...  Training loss: 1.6224...  1.6674 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2629...  Training loss: 1.6060...  1.7858 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2630...  Training loss: 1.6201...  1.7356 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2631...  Training loss: 1.6386...  1.7767 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2632...  Training loss: 1.6085...  1.7417 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2633...  Training loss: 1.6036...  1.7793 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2634...  Training loss: 1.6565...  1.6815 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2635...  Training loss: 1.6291...  1.7045 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2636...  Training loss: 1.6854...  1.8173 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2637...  Training loss: 1.6563...  1.7341 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2638...  Training loss: 1.6296...  1.7722 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2639...  Training loss: 1.6222...  1.6850 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2640...  Training loss: 1.6438...  1.7156 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2641...  Training loss: 1.6498...  1.7908 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2642...  Training loss: 1.6046...  1.7381 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2643...  Training loss: 1.6134...  1.7847 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2644...  Training loss: 1.6341...  1.7326 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2645...  Training loss: 1.6711...  1.7777 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2646...  Training loss: 1.6400...  1.6825 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2647...  Training loss: 1.6521...  1.7404 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2648...  Training loss: 1.6186...  1.7632 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2649...  Training loss: 1.6182...  1.7336 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2650...  Training loss: 1.6387...  1.7963 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2651...  Training loss: 1.6228...  1.6905 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2652...  Training loss: 1.6258...  1.7236 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2653...  Training loss: 1.5939...  1.7777 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2654...  Training loss: 1.6150...  1.7366 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2655...  Training loss: 1.5817...  1.7632 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2656...  Training loss: 1.6380...  1.7116 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2657...  Training loss: 1.5918...  1.7221 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2658...  Training loss: 1.6214...  1.7401 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2659...  Training loss: 1.5995...  1.7421 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2660...  Training loss: 1.6068...  1.7842 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2661...  Training loss: 1.6006...  1.7421 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2662...  Training loss: 1.6296...  1.7822 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2663...  Training loss: 1.5868...  1.7291 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2664...  Training loss: 1.6273...  1.7346 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2665...  Training loss: 1.5950...  1.7787 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2666...  Training loss: 1.6109...  1.7111 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2667...  Training loss: 1.6005...  1.7988 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2668...  Training loss: 1.6035...  1.7030 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2669...  Training loss: 1.6009...  1.7296 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2670...  Training loss: 1.6346...  1.8128 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2671...  Training loss: 1.6071...  1.7572 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2672...  Training loss: 1.5750...  1.8239 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2673...  Training loss: 1.6019...  1.7111 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2674...  Training loss: 1.5778...  1.6945 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2675...  Training loss: 1.6218...  1.7863 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2676...  Training loss: 1.6190...  1.7231 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2677...  Training loss: 1.6032...  1.7762 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2678...  Training loss: 1.6029...  1.7166 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2679...  Training loss: 1.6059...  1.7752 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2680...  Training loss: 1.6214...  1.6699 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2681...  Training loss: 1.6132...  1.7466 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2682...  Training loss: 1.6235...  1.7627 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2683...  Training loss: 1.6169...  1.7341 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2684...  Training loss: 1.6304...  1.7662 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2685...  Training loss: 1.6086...  1.6960 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2686...  Training loss: 1.6051...  1.7206 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2687...  Training loss: 1.6179...  1.7737 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2688...  Training loss: 1.5934...  1.7522 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2689...  Training loss: 1.6009...  1.7797 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2690...  Training loss: 1.5778...  1.7236 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2691...  Training loss: 1.6212...  1.7772 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2692...  Training loss: 1.6129...  1.6865 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2693...  Training loss: 1.6084...  1.7466 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2694...  Training loss: 1.6018...  1.7938 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2695...  Training loss: 1.6190...  1.7572 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2696...  Training loss: 1.5807...  1.7837 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2697...  Training loss: 1.5684...  1.6985 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2698...  Training loss: 1.6349...  1.7256 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2699...  Training loss: 1.6069...  1.7497 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2700...  Training loss: 1.5725...  1.7166 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2701...  Training loss: 1.6372...  1.7787 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2702...  Training loss: 1.6309...  1.6845 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2703...  Training loss: 1.6138...  1.7090 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2704...  Training loss: 1.6051...  1.7562 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2705...  Training loss: 1.5740...  1.7326 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2706...  Training loss: 1.5806...  1.7637 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2707...  Training loss: 1.6398...  1.7431 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2708...  Training loss: 1.6308...  1.7818 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2709...  Training loss: 1.6257...  1.6925 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2710...  Training loss: 1.6145...  1.7296 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2711...  Training loss: 1.6402...  1.7657 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2712...  Training loss: 1.6183...  1.7176 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2713...  Training loss: 1.6252...  1.7908 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2714...  Training loss: 1.6139...  1.6920 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2715...  Training loss: 1.6600...  1.7035 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2716...  Training loss: 1.6165...  1.7792 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2717...  Training loss: 1.6184...  1.7261 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2718...  Training loss: 1.6268...  1.7702 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2719...  Training loss: 1.5939...  1.7045 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2720...  Training loss: 1.6440...  1.7301 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2721...  Training loss: 1.6302...  1.7842 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2722...  Training loss: 1.6613...  1.7271 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2723...  Training loss: 1.6218...  1.7988 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2724...  Training loss: 1.6059...  1.7542 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2725...  Training loss: 1.5809...  1.7642 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2726...  Training loss: 1.6211...  1.6830 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2727...  Training loss: 1.6267...  1.7401 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2728...  Training loss: 1.6195...  1.7958 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2729...  Training loss: 1.6203...  1.7306 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2730...  Training loss: 1.6159...  1.7767 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2731...  Training loss: 1.6268...  1.6775 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2732...  Training loss: 1.6209...  1.7426 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2733...  Training loss: 1.5732...  1.7782 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2734...  Training loss: 1.6448...  1.7577 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2735...  Training loss: 1.6488...  1.7632 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2736...  Training loss: 1.6184...  1.7236 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2737...  Training loss: 1.6230...  1.7537 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2738...  Training loss: 1.6214...  1.6915 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2739...  Training loss: 1.6167...  1.9572 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2740...  Training loss: 1.6191...  1.8614 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2741...  Training loss: 1.6315...  1.9196 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2742...  Training loss: 1.6792...  1.9351 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2743...  Training loss: 1.6074...  1.7873 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2744...  Training loss: 1.6094...  2.0068 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2745...  Training loss: 1.6088...  1.8198 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2746...  Training loss: 1.5959...  1.7637 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2747...  Training loss: 1.6473...  1.8259 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2748...  Training loss: 1.6293...  1.7241 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2749...  Training loss: 1.6356...  1.7421 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2750...  Training loss: 1.5965...  1.7847 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2751...  Training loss: 1.5995...  1.7637 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2752...  Training loss: 1.6358...  1.8163 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2753...  Training loss: 1.5877...  1.7557 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2754...  Training loss: 1.5805...  1.8158 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2755...  Training loss: 1.5862...  1.7191 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2756...  Training loss: 1.6149...  1.7446 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2757...  Training loss: 1.6167...  1.7943 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2758...  Training loss: 1.6241...  1.7662 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2759...  Training loss: 1.6038...  1.8279 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2760...  Training loss: 1.6036...  1.7316 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2761...  Training loss: 1.6375...  1.7401 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2762...  Training loss: 1.6039...  1.7998 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2763...  Training loss: 1.6075...  1.7712 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2764...  Training loss: 1.6120...  1.8093 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2765...  Training loss: 1.6231...  1.7121 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2766...  Training loss: 1.5946...  1.7381 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2767...  Training loss: 1.6103...  1.8068 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2768...  Training loss: 1.5963...  1.7757 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2769...  Training loss: 1.5822...  1.8203 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2770...  Training loss: 1.6379...  1.7717 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2771...  Training loss: 1.6026...  1.8183 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2772...  Training loss: 1.5999...  1.7261 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2773...  Training loss: 1.7077...  1.7532 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2774...  Training loss: 1.6074...  1.8194 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2775...  Training loss: 1.6112...  1.7772 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2776...  Training loss: 1.6192...  1.8289 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2777...  Training loss: 1.6127...  1.7171 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2778...  Training loss: 1.5760...  1.7617 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2779...  Training loss: 1.6239...  1.8098 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2780...  Training loss: 1.6007...  1.7903 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2781...  Training loss: 1.6332...  1.8113 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2782...  Training loss: 1.6015...  1.7642 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2783...  Training loss: 1.5936...  1.8128 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2784...  Training loss: 1.5959...  1.7223 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2785...  Training loss: 1.6127...  1.7868 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2786...  Training loss: 1.6462...  1.8088 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2787...  Training loss: 1.6026...  1.7512 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2788...  Training loss: 1.5861...  1.8018 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2789...  Training loss: 1.6196...  1.7045 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2790...  Training loss: 1.6472...  1.7708 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2791...  Training loss: 1.6122...  1.8259 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2792...  Training loss: 1.6292...  1.7792 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2793...  Training loss: 1.6090...  1.8153 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2794...  Training loss: 1.6279...  1.7291 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2795...  Training loss: 1.5942...  1.7627 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2796...  Training loss: 1.6069...  1.8083 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2797...  Training loss: 1.6078...  1.7737 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2798...  Training loss: 1.5760...  1.8409 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2799...  Training loss: 1.5840...  1.8199 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2800...  Training loss: 1.6256...  1.8419 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2801...  Training loss: 1.6304...  1.6413 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2802...  Training loss: 1.6278...  1.7512 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2803...  Training loss: 1.6151...  1.8163 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2804...  Training loss: 1.5891...  1.7567 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2805...  Training loss: 1.6208...  1.7837 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2806...  Training loss: 1.6245...  1.7376 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2807...  Training loss: 1.5959...  1.9006 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2808...  Training loss: 1.6092...  1.8299 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2809...  Training loss: 1.5871...  1.7737 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2810...  Training loss: 1.5766...  1.8389 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2811...  Training loss: 1.5659...  1.7000 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2812...  Training loss: 1.5811...  1.7557 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2813...  Training loss: 1.5846...  1.7898 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2814...  Training loss: 1.6289...  1.7812 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2815...  Training loss: 1.5944...  1.8028 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2816...  Training loss: 1.5712...  1.7652 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2817...  Training loss: 1.6194...  1.8334 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2818...  Training loss: 1.5626...  1.7221 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2819...  Training loss: 1.6055...  1.7537 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2820...  Training loss: 1.5990...  1.8208 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2821...  Training loss: 1.5815...  1.7642 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2822...  Training loss: 1.6354...  1.8033 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2823...  Training loss: 1.5866...  1.7161 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2824...  Training loss: 1.6479...  1.7802 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2825...  Training loss: 1.6106...  1.8183 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2826...  Training loss: 1.6124...  1.7903 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2827...  Training loss: 1.5805...  1.7908 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2828...  Training loss: 1.6099...  1.7837 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2829...  Training loss: 1.6364...  1.7845 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2830...  Training loss: 1.5923...  1.7381 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2831...  Training loss: 1.5904...  1.7747 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2832...  Training loss: 1.6405...  1.7953 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2833...  Training loss: 1.6102...  1.7396 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2834...  Training loss: 1.6600...  1.8098 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2835...  Training loss: 1.6295...  1.7236 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2836...  Training loss: 1.6226...  1.7777 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2837...  Training loss: 1.5892...  1.7983 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2838...  Training loss: 1.6292...  1.7702 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2839...  Training loss: 1.6216...  1.8133 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2840...  Training loss: 1.5833...  1.7231 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2841...  Training loss: 1.6065...  1.7587 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2842...  Training loss: 1.6006...  1.7968 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2843...  Training loss: 1.6566...  1.7802 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2844...  Training loss: 1.6308...  1.8093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2845...  Training loss: 1.6308...  1.7702 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2846...  Training loss: 1.5983...  1.8088 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2847...  Training loss: 1.5957...  1.7196 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2848...  Training loss: 1.6198...  1.7692 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2849...  Training loss: 1.5967...  1.8249 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2850...  Training loss: 1.6133...  1.7577 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2851...  Training loss: 1.5778...  1.8053 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2852...  Training loss: 1.5964...  1.7216 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2853...  Training loss: 1.5677...  1.7416 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2854...  Training loss: 1.6200...  1.8279 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2855...  Training loss: 1.5649...  1.7712 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2856...  Training loss: 1.5953...  1.8118 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2857...  Training loss: 1.5788...  1.7095 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2858...  Training loss: 1.5887...  1.7587 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2859...  Training loss: 1.5716...  1.8133 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2860...  Training loss: 1.5840...  1.7707 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2861...  Training loss: 1.5671...  1.7913 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2862...  Training loss: 1.6142...  1.7592 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2863...  Training loss: 1.5802...  1.8108 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2864...  Training loss: 1.5859...  1.7291 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2865...  Training loss: 1.5913...  1.7928 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2866...  Training loss: 1.5855...  1.8589 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2867...  Training loss: 1.5844...  1.7888 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2868...  Training loss: 1.6139...  1.8173 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2869...  Training loss: 1.5913...  1.7286 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2870...  Training loss: 1.5601...  1.7517 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2871...  Training loss: 1.5885...  1.8218 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2872...  Training loss: 1.5590...  1.7652 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2873...  Training loss: 1.5997...  1.8088 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2874...  Training loss: 1.5940...  1.7537 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2875...  Training loss: 1.5979...  1.7993 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2876...  Training loss: 1.5900...  1.7226 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2877...  Training loss: 1.5894...  1.7477 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2878...  Training loss: 1.6042...  1.8003 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2879...  Training loss: 1.5909...  1.7848 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2880...  Training loss: 1.6075...  1.8078 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2881...  Training loss: 1.6007...  1.7351 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2882...  Training loss: 1.6158...  1.7773 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2883...  Training loss: 1.5822...  1.8259 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2884...  Training loss: 1.5854...  1.7632 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2885...  Training loss: 1.5948...  1.8168 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2886...  Training loss: 1.5784...  1.7296 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2887...  Training loss: 1.5754...  1.8615 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2888...  Training loss: 1.5627...  1.9321 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2889...  Training loss: 1.6094...  1.9156 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2890...  Training loss: 1.6026...  1.9276 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2891...  Training loss: 1.5909...  1.7437 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2892...  Training loss: 1.5843...  1.7883 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2893...  Training loss: 1.6031...  1.7136 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2894...  Training loss: 1.5547...  1.7857 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2895...  Training loss: 1.5638...  1.7898 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2896...  Training loss: 1.6182...  1.7486 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2897...  Training loss: 1.5859...  1.7672 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2898...  Training loss: 1.5510...  1.7276 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2899...  Training loss: 1.6206...  1.7482 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2900...  Training loss: 1.6073...  1.8198 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2901...  Training loss: 1.5886...  1.7818 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2902...  Training loss: 1.5743...  1.8188 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2903...  Training loss: 1.5520...  1.7105 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2904...  Training loss: 1.5585...  1.7757 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2905...  Training loss: 1.6143...  1.7622 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2906...  Training loss: 1.6169...  1.7948 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2907...  Training loss: 1.5972...  1.8213 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2908...  Training loss: 1.6039...  1.7632 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2909...  Training loss: 1.6209...  1.8028 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2910...  Training loss: 1.6159...  1.7276 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2911...  Training loss: 1.6181...  1.7908 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2912...  Training loss: 1.5992...  1.8449 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2913...  Training loss: 1.6492...  1.7652 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2914...  Training loss: 1.5999...  1.8314 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2915...  Training loss: 1.6013...  1.7497 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2916...  Training loss: 1.6173...  1.7712 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2917...  Training loss: 1.5751...  1.8449 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2918...  Training loss: 1.6266...  1.7772 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2919...  Training loss: 1.5987...  1.8183 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2920...  Training loss: 1.6461...  1.7537 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2921...  Training loss: 1.6216...  1.8083 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2922...  Training loss: 1.5828...  1.7326 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2923...  Training loss: 1.5646...  1.7732 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2924...  Training loss: 1.6088...  1.8334 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2925...  Training loss: 1.6159...  1.7707 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2926...  Training loss: 1.6078...  1.8254 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2927...  Training loss: 1.5960...  1.7261 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2928...  Training loss: 1.5921...  1.7532 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2929...  Training loss: 1.6084...  1.8229 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2930...  Training loss: 1.5892...  1.7602 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2931...  Training loss: 1.5668...  1.8038 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2932...  Training loss: 1.6173...  1.7416 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2933...  Training loss: 1.6352...  1.8199 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2934...  Training loss: 1.6086...  1.8374 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2935...  Training loss: 1.6099...  1.7642 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2936...  Training loss: 1.6004...  1.8404 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2937...  Training loss: 1.5958...  1.7749 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2938...  Training loss: 1.6051...  1.7998 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2939...  Training loss: 1.6169...  1.7281 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2940...  Training loss: 1.6752...  1.7642 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2941...  Training loss: 1.5883...  1.8123 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2942...  Training loss: 1.5946...  1.7812 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2943...  Training loss: 1.5720...  1.8299 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2944...  Training loss: 1.5801...  1.7166 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2945...  Training loss: 1.6192...  1.7652 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2946...  Training loss: 1.6150...  1.8008 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2947...  Training loss: 1.6184...  1.7802 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2948...  Training loss: 1.5904...  1.8375 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2949...  Training loss: 1.5900...  1.7642 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2950...  Training loss: 1.6113...  1.7672 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2951...  Training loss: 1.5873...  1.8379 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2952...  Training loss: 1.5636...  1.7903 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2953...  Training loss: 1.5720...  1.8359 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2954...  Training loss: 1.5873...  1.7782 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2955...  Training loss: 1.5998...  1.8339 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2956...  Training loss: 1.6083...  1.7351 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2957...  Training loss: 1.5914...  1.7602 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2958...  Training loss: 1.5712...  1.8344 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2959...  Training loss: 1.6124...  1.7943 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2960...  Training loss: 1.5837...  1.8449 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2961...  Training loss: 1.5935...  1.7246 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2962...  Training loss: 1.5965...  1.7582 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2963...  Training loss: 1.5964...  1.8193 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2964...  Training loss: 1.5721...  1.7778 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2965...  Training loss: 1.5929...  1.8519 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2966...  Training loss: 1.5811...  1.7572 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2967...  Training loss: 1.5677...  1.8058 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2968...  Training loss: 1.6012...  1.7366 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2969...  Training loss: 1.5915...  1.7637 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2970...  Training loss: 1.5756...  1.8294 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2971...  Training loss: 1.6944...  1.7687 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2972...  Training loss: 1.5970...  1.8264 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2973...  Training loss: 1.5885...  1.7456 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2974...  Training loss: 1.6022...  1.7612 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2975...  Training loss: 1.5796...  1.8394 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2976...  Training loss: 1.5667...  1.7647 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2977...  Training loss: 1.6045...  1.8589 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2978...  Training loss: 1.5801...  1.7271 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2979...  Training loss: 1.6178...  1.7657 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2980...  Training loss: 1.5859...  1.8264 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2981...  Training loss: 1.5668...  1.7692 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2982...  Training loss: 1.5843...  1.8329 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2983...  Training loss: 1.5904...  1.7782 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2984...  Training loss: 1.6396...  1.8188 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2985...  Training loss: 1.5854...  1.7466 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2986...  Training loss: 1.5714...  1.7807 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2987...  Training loss: 1.6026...  1.8244 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2988...  Training loss: 1.6275...  1.7893 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2989...  Training loss: 1.6060...  1.8259 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2990...  Training loss: 1.6154...  1.7281 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2991...  Training loss: 1.5899...  1.7762 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2992...  Training loss: 1.6202...  1.8324 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2993...  Training loss: 1.5844...  1.7873 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2994...  Training loss: 1.6070...  1.8088 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2995...  Training loss: 1.5847...  1.7416 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2996...  Training loss: 1.5626...  1.7622 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2997...  Training loss: 1.5740...  1.8168 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2998...  Training loss: 1.6071...  1.7717 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2999...  Training loss: 1.6177...  1.8178 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3000...  Training loss: 1.6070...  1.8148 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3001...  Training loss: 1.6020...  1.6899 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3002...  Training loss: 1.5739...  1.6454 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3003...  Training loss: 1.6066...  1.7216 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3004...  Training loss: 1.6083...  1.7817 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3005...  Training loss: 1.5808...  1.7552 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3006...  Training loss: 1.5970...  1.8379 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3007...  Training loss: 1.5749...  1.7452 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3008...  Training loss: 1.5555...  1.7737 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3009...  Training loss: 1.5520...  1.8409 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3010...  Training loss: 1.5614...  1.7672 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3011...  Training loss: 1.5676...  1.8194 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3012...  Training loss: 1.6292...  1.7868 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3013...  Training loss: 1.5710...  1.8489 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3014...  Training loss: 1.5602...  1.7321 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3015...  Training loss: 1.5972...  1.7933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3016...  Training loss: 1.5468...  1.8394 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3017...  Training loss: 1.5808...  1.7562 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3018...  Training loss: 1.5838...  1.8043 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3019...  Training loss: 1.5694...  1.7226 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3020...  Training loss: 1.6299...  1.7667 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3021...  Training loss: 1.5717...  1.8118 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3022...  Training loss: 1.6347...  1.7948 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3023...  Training loss: 1.6070...  1.8314 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3024...  Training loss: 1.5963...  1.7492 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3025...  Training loss: 1.5834...  1.7747 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3026...  Training loss: 1.5881...  1.8043 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3027...  Training loss: 1.6089...  1.7712 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3028...  Training loss: 1.5751...  1.8118 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3029...  Training loss: 1.5685...  1.7652 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3030...  Training loss: 1.6281...  1.8223 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3031...  Training loss: 1.5963...  1.7271 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3032...  Training loss: 1.6444...  1.7682 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3033...  Training loss: 1.6167...  1.8193 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3034...  Training loss: 1.5928...  1.7477 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3035...  Training loss: 1.5795...  1.8208 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3036...  Training loss: 1.6024...  1.7366 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3037...  Training loss: 1.5990...  1.7517 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3038...  Training loss: 1.5766...  1.8284 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3039...  Training loss: 1.5826...  1.7657 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3040...  Training loss: 1.5843...  1.8314 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3041...  Training loss: 1.6237...  1.7522 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3042...  Training loss: 1.6222...  1.7607 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3043...  Training loss: 1.6233...  1.8319 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3044...  Training loss: 1.5873...  1.7773 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3045...  Training loss: 1.5811...  1.8289 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3046...  Training loss: 1.6103...  1.7697 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3047...  Training loss: 1.5885...  1.8359 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3048...  Training loss: 1.5793...  1.7476 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3049...  Training loss: 1.5572...  1.7762 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3050...  Training loss: 1.5882...  1.8068 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3051...  Training loss: 1.5536...  1.7707 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3052...  Training loss: 1.5910...  1.8584 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3053...  Training loss: 1.5525...  1.7286 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3054...  Training loss: 1.5812...  1.7672 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3055...  Training loss: 1.5624...  1.8178 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3056...  Training loss: 1.5676...  1.7692 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3057...  Training loss: 1.5651...  1.8304 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3058...  Training loss: 1.5738...  1.7396 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3059...  Training loss: 1.5468...  1.7361 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3060...  Training loss: 1.6081...  1.7492 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3061...  Training loss: 1.5708...  1.7762 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3062...  Training loss: 1.5805...  1.8294 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3063...  Training loss: 1.5703...  1.7722 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3064...  Training loss: 1.5634...  1.8028 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3065...  Training loss: 1.5589...  1.7121 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3066...  Training loss: 1.5983...  1.7707 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3067...  Training loss: 1.5818...  1.8991 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3068...  Training loss: 1.5461...  1.8354 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3069...  Training loss: 1.5709...  1.8379 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3070...  Training loss: 1.5552...  1.7401 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3071...  Training loss: 1.5852...  1.7582 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3072...  Training loss: 1.5792...  1.8164 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3073...  Training loss: 1.5700...  1.7667 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3074...  Training loss: 1.5732...  1.7953 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3075...  Training loss: 1.5719...  1.7612 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3076...  Training loss: 1.5846...  1.8118 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3077...  Training loss: 1.5796...  1.7401 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3078...  Training loss: 1.5810...  1.7807 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3079...  Training loss: 1.5849...  1.7938 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3080...  Training loss: 1.5931...  1.7647 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3081...  Training loss: 1.5682...  1.8078 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3082...  Training loss: 1.5844...  1.7070 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3083...  Training loss: 1.5763...  1.7296 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3084...  Training loss: 1.5722...  1.7427 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3085...  Training loss: 1.5600...  1.7397 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3086...  Training loss: 1.5362...  1.7597 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3087...  Training loss: 1.6004...  1.6740 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3088...  Training loss: 1.5832...  1.7306 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3089...  Training loss: 1.5803...  1.7577 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3090...  Training loss: 1.5753...  1.7507 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3091...  Training loss: 1.5937...  1.7632 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3092...  Training loss: 1.5515...  1.7436 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3093...  Training loss: 1.5485...  1.7742 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3094...  Training loss: 1.6078...  1.7075 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3095...  Training loss: 1.5766...  1.7421 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3096...  Training loss: 1.5391...  1.7567 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3097...  Training loss: 1.5943...  1.7291 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3098...  Training loss: 1.5915...  1.7842 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3099...  Training loss: 1.5598...  1.6915 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3100...  Training loss: 1.5650...  1.7602 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3101...  Training loss: 1.5379...  1.7988 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3102...  Training loss: 1.5486...  1.7341 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3103...  Training loss: 1.6127...  1.7702 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3104...  Training loss: 1.6047...  1.7141 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3105...  Training loss: 1.6040...  1.6980 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3106...  Training loss: 1.5951...  1.6885 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3107...  Training loss: 1.6086...  1.7527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3108...  Training loss: 1.5902...  1.7963 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3109...  Training loss: 1.5988...  1.7431 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3110...  Training loss: 1.5793...  1.8043 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3111...  Training loss: 1.6343...  1.7080 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3112...  Training loss: 1.5887...  1.7582 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3113...  Training loss: 1.5894...  1.7537 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3114...  Training loss: 1.5993...  1.7346 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3115...  Training loss: 1.5629...  1.7888 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3116...  Training loss: 1.6141...  1.7065 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3117...  Training loss: 1.5942...  1.7211 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3118...  Training loss: 1.6253...  1.7837 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3119...  Training loss: 1.6018...  1.7371 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3120...  Training loss: 1.5813...  1.7918 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3121...  Training loss: 1.5401...  1.7261 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3122...  Training loss: 1.5765...  1.8138 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3123...  Training loss: 1.6097...  1.7085 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3124...  Training loss: 1.5912...  1.7456 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3125...  Training loss: 1.5869...  1.7782 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3126...  Training loss: 1.5837...  1.7612 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3127...  Training loss: 1.5994...  1.7797 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3128...  Training loss: 1.5793...  1.7126 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3129...  Training loss: 1.5453...  1.7637 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3130...  Training loss: 1.6187...  1.7366 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3131...  Training loss: 1.6240...  1.7447 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3132...  Training loss: 1.5867...  1.7196 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3133...  Training loss: 1.5942...  1.6704 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3134...  Training loss: 1.5918...  1.7090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3135...  Training loss: 1.5870...  1.7597 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3136...  Training loss: 1.5898...  1.7537 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3137...  Training loss: 1.6007...  1.7672 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3138...  Training loss: 1.6606...  1.7241 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3139...  Training loss: 1.5896...  1.7537 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3140...  Training loss: 1.5735...  1.6729 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3141...  Training loss: 1.5700...  1.7081 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3142...  Training loss: 1.5660...  1.7527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3143...  Training loss: 1.6081...  1.7366 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3144...  Training loss: 1.5854...  1.7346 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3145...  Training loss: 1.6118...  1.6740 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3146...  Training loss: 1.5589...  1.7020 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3147...  Training loss: 1.5741...  1.7647 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3148...  Training loss: 1.6022...  1.7396 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3149...  Training loss: 1.5525...  1.7722 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3150...  Training loss: 1.5478...  1.6890 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3151...  Training loss: 1.5551...  1.7065 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3152...  Training loss: 1.5840...  1.6790 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3153...  Training loss: 1.5854...  1.7441 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3154...  Training loss: 1.6031...  1.7572 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3155...  Training loss: 1.5761...  1.7306 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3156...  Training loss: 1.5616...  1.7747 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3157...  Training loss: 1.6012...  1.7045 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3158...  Training loss: 1.5642...  1.7381 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3159...  Training loss: 1.5766...  1.7502 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3160...  Training loss: 1.5794...  1.7146 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3161...  Training loss: 1.5790...  1.7562 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3162...  Training loss: 1.5582...  1.6855 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3163...  Training loss: 1.5833...  1.7111 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3164...  Training loss: 1.5608...  1.7527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3165...  Training loss: 1.5544...  1.7030 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3166...  Training loss: 1.5884...  1.7908 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3167...  Training loss: 1.5640...  1.7131 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3168...  Training loss: 1.5729...  1.7512 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3169...  Training loss: 1.6777...  1.6714 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3170...  Training loss: 1.5866...  1.7166 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3171...  Training loss: 1.5728...  1.7577 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3172...  Training loss: 1.5826...  1.7482 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3173...  Training loss: 1.5670...  1.7557 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3174...  Training loss: 1.5413...  1.6835 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3175...  Training loss: 1.5832...  1.7171 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3176...  Training loss: 1.5680...  1.7617 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3177...  Training loss: 1.5990...  1.7141 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3178...  Training loss: 1.5699...  1.7612 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3179...  Training loss: 1.5582...  1.7236 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3180...  Training loss: 1.5712...  1.7356 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3181...  Training loss: 1.5720...  1.7817 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3182...  Training loss: 1.6214...  1.7416 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3183...  Training loss: 1.5778...  1.7657 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3184...  Training loss: 1.5490...  1.7206 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3185...  Training loss: 1.5851...  1.7176 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3186...  Training loss: 1.6153...  1.6805 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3187...  Training loss: 1.5828...  1.7015 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3188...  Training loss: 1.6001...  1.7517 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3189...  Training loss: 1.5764...  1.7116 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3190...  Training loss: 1.6134...  1.7421 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3191...  Training loss: 1.5712...  1.6815 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3192...  Training loss: 1.5964...  1.7296 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3193...  Training loss: 1.5705...  1.7677 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3194...  Training loss: 1.5431...  1.7311 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3195...  Training loss: 1.5497...  1.7817 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3196...  Training loss: 1.6044...  1.6734 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3197...  Training loss: 1.5994...  1.6970 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3198...  Training loss: 1.5878...  1.7356 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3199...  Training loss: 1.5772...  1.7201 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3200...  Training loss: 1.5604...  1.7472 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3201...  Training loss: 1.5947...  1.6484 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3202...  Training loss: 1.5990...  1.7311 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3203...  Training loss: 1.5655...  1.6584 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3204...  Training loss: 1.5846...  1.7126 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3205...  Training loss: 1.5483...  1.7692 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3206...  Training loss: 1.5416...  1.7346 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3207...  Training loss: 1.5340...  1.7577 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3208...  Training loss: 1.5592...  1.7166 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3209...  Training loss: 1.5529...  1.7020 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3210...  Training loss: 1.5955...  1.7669 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3211...  Training loss: 1.5574...  1.7246 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3212...  Training loss: 1.5394...  1.7632 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3213...  Training loss: 1.5972...  1.7050 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3214...  Training loss: 1.5352...  1.7812 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3215...  Training loss: 1.5675...  1.6900 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3216...  Training loss: 1.5614...  1.7577 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3217...  Training loss: 1.5553...  1.7913 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3218...  Training loss: 1.5997...  1.7527 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3219...  Training loss: 1.5566...  1.7632 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3220...  Training loss: 1.6241...  1.6965 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3221...  Training loss: 1.5901...  1.7246 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3222...  Training loss: 1.5852...  1.7757 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3223...  Training loss: 1.5621...  1.7075 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3224...  Training loss: 1.5824...  1.7712 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3225...  Training loss: 1.5994...  1.6740 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3226...  Training loss: 1.5584...  1.7241 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3227...  Training loss: 1.5488...  1.7622 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3228...  Training loss: 1.6081...  1.7436 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3229...  Training loss: 1.5814...  1.7547 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3230...  Training loss: 1.6355...  1.7151 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3231...  Training loss: 1.6153...  1.7827 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3232...  Training loss: 1.5876...  1.7066 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3233...  Training loss: 1.5691...  1.7306 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3234...  Training loss: 1.5860...  1.7507 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3235...  Training loss: 1.6010...  1.7231 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3236...  Training loss: 1.5564...  1.7642 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3237...  Training loss: 1.5679...  1.6704 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3238...  Training loss: 1.5758...  1.7236 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3239...  Training loss: 1.6164...  1.7707 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3240...  Training loss: 1.5996...  1.7336 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3241...  Training loss: 1.6038...  1.7492 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3242...  Training loss: 1.5659...  1.6659 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3243...  Training loss: 1.5689...  1.6905 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3244...  Training loss: 1.5983...  1.7532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3245...  Training loss: 1.5872...  1.7411 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3246...  Training loss: 1.5642...  1.7702 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3247...  Training loss: 1.5423...  1.7331 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3248...  Training loss: 1.5677...  1.7973 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3249...  Training loss: 1.5319...  1.6835 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3250...  Training loss: 1.5766...  1.7497 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3251...  Training loss: 1.5489...  1.7522 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3252...  Training loss: 1.5753...  1.7426 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3253...  Training loss: 1.5367...  1.7451 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3254...  Training loss: 1.5606...  1.6674 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3255...  Training loss: 1.5611...  1.7396 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3256...  Training loss: 1.5578...  1.7657 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3257...  Training loss: 1.5394...  1.7316 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3258...  Training loss: 1.5917...  1.7762 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3259...  Training loss: 1.5417...  1.7076 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3260...  Training loss: 1.5690...  1.7632 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3261...  Training loss: 1.5557...  1.6935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3262...  Training loss: 1.5504...  1.7391 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3263...  Training loss: 1.5456...  1.7537 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3264...  Training loss: 1.5836...  1.7196 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3265...  Training loss: 1.5661...  1.7572 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3266...  Training loss: 1.5320...  1.6825 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3267...  Training loss: 1.5565...  1.7221 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3268...  Training loss: 1.5463...  1.7557 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3269...  Training loss: 1.5823...  1.7276 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3270...  Training loss: 1.5652...  1.7447 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3271...  Training loss: 1.5631...  1.6910 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3272...  Training loss: 1.5501...  1.7296 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3273...  Training loss: 1.5735...  1.7381 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3274...  Training loss: 1.5695...  1.7948 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3275...  Training loss: 1.5548...  1.7868 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3276...  Training loss: 1.5765...  1.7371 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3277...  Training loss: 1.5730...  1.7502 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3278...  Training loss: 1.5900...  1.6790 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3279...  Training loss: 1.5472...  1.7376 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3280...  Training loss: 1.5657...  1.7973 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3281...  Training loss: 1.5683...  1.7025 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3282...  Training loss: 1.5500...  1.7592 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3283...  Training loss: 1.5336...  1.6699 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3284...  Training loss: 1.5275...  1.7191 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3285...  Training loss: 1.5716...  1.7712 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3286...  Training loss: 1.5838...  1.7396 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3287...  Training loss: 1.5733...  1.7321 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3288...  Training loss: 1.5629...  1.6975 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3289...  Training loss: 1.5676...  1.6895 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3290...  Training loss: 1.5239...  1.7582 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3291...  Training loss: 1.5195...  1.7396 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3292...  Training loss: 1.5834...  1.7597 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3293...  Training loss: 1.5706...  1.7086 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3294...  Training loss: 1.5259...  1.7276 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3295...  Training loss: 1.5805...  1.7016 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3296...  Training loss: 1.5817...  1.7306 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3297...  Training loss: 1.5609...  1.7582 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3298...  Training loss: 1.5471...  1.7366 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3299...  Training loss: 1.5163...  1.7517 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3300...  Training loss: 1.5392...  1.6765 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3301...  Training loss: 1.5831...  1.7136 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3302...  Training loss: 1.5779...  1.7426 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3303...  Training loss: 1.5710...  1.7286 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3304...  Training loss: 1.5716...  1.7732 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3305...  Training loss: 1.5901...  1.7221 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3306...  Training loss: 1.5805...  1.7522 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3307...  Training loss: 1.5903...  1.6850 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3308...  Training loss: 1.5718...  1.7241 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3309...  Training loss: 1.6209...  1.7532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3310...  Training loss: 1.5786...  1.7216 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3311...  Training loss: 1.5673...  1.7542 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3312...  Training loss: 1.5867...  1.6800 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3313...  Training loss: 1.5574...  1.7206 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3314...  Training loss: 1.5925...  1.7642 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3315...  Training loss: 1.5745...  1.7431 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3316...  Training loss: 1.6226...  1.7852 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3317...  Training loss: 1.5924...  1.6980 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3318...  Training loss: 1.5717...  1.7326 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3319...  Training loss: 1.5204...  1.7622 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3320...  Training loss: 1.5752...  1.7412 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3321...  Training loss: 1.5790...  1.7592 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3322...  Training loss: 1.5726...  1.7196 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3323...  Training loss: 1.5673...  1.7542 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3324...  Training loss: 1.5741...  1.6765 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3325...  Training loss: 1.5843...  1.7356 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3326...  Training loss: 1.5636...  1.7231 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3327...  Training loss: 1.5271...  1.7261 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3328...  Training loss: 1.5914...  1.7452 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3329...  Training loss: 1.6011...  1.6980 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3330...  Training loss: 1.5748...  1.7186 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3331...  Training loss: 1.5742...  1.7652 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3332...  Training loss: 1.5654...  1.7296 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3333...  Training loss: 1.5696...  1.7537 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3334...  Training loss: 1.5683...  1.6990 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3335...  Training loss: 1.5848...  1.7497 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3336...  Training loss: 1.6494...  1.7617 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3337...  Training loss: 1.5695...  1.7517 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3338...  Training loss: 1.5593...  1.7677 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3339...  Training loss: 1.5611...  1.7351 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3340...  Training loss: 1.5555...  1.7572 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3341...  Training loss: 1.5938...  1.6765 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3342...  Training loss: 1.5648...  1.7291 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3343...  Training loss: 1.6013...  1.8023 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3344...  Training loss: 1.5506...  1.7572 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3345...  Training loss: 1.5643...  1.7677 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3346...  Training loss: 1.5843...  1.6985 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3347...  Training loss: 1.5470...  1.7095 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3348...  Training loss: 1.5236...  1.7672 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3349...  Training loss: 1.5428...  1.7351 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3350...  Training loss: 1.5604...  1.7567 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3351...  Training loss: 1.5701...  1.7166 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3352...  Training loss: 1.5833...  1.7682 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3353...  Training loss: 1.5591...  1.6494 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3354...  Training loss: 1.5476...  1.7617 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3355...  Training loss: 1.5813...  1.7176 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3356...  Training loss: 1.5492...  1.7271 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3357...  Training loss: 1.5561...  1.7291 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3358...  Training loss: 1.5645...  1.6584 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3359...  Training loss: 1.5706...  1.7146 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3360...  Training loss: 1.5505...  1.7712 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3361...  Training loss: 1.5577...  1.7421 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3362...  Training loss: 1.5351...  1.7677 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3363...  Training loss: 1.5325...  1.6915 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3364...  Training loss: 1.5796...  1.7236 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3365...  Training loss: 1.5597...  1.7692 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3366...  Training loss: 1.5485...  1.7502 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3367...  Training loss: 1.6611...  1.8048 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3368...  Training loss: 1.5810...  1.7481 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3369...  Training loss: 1.5546...  1.7998 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3370...  Training loss: 1.5728...  1.6915 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3371...  Training loss: 1.5532...  1.7371 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3372...  Training loss: 1.5337...  1.7772 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3373...  Training loss: 1.5662...  1.7492 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3374...  Training loss: 1.5509...  1.7657 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3375...  Training loss: 1.5863...  1.6845 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3376...  Training loss: 1.5627...  1.7166 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3377...  Training loss: 1.5497...  1.7512 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3378...  Training loss: 1.5535...  1.7377 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3379...  Training loss: 1.5533...  1.7266 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3380...  Training loss: 1.5986...  1.6634 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3381...  Training loss: 1.5608...  1.6970 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3382...  Training loss: 1.5487...  1.7311 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3383...  Training loss: 1.5758...  1.7542 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3384...  Training loss: 1.6004...  1.7547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3385...  Training loss: 1.5663...  1.7151 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3386...  Training loss: 1.5904...  1.7366 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3387...  Training loss: 1.5487...  1.6494 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3388...  Training loss: 1.5883...  1.7346 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3389...  Training loss: 1.5553...  1.7863 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3390...  Training loss: 1.5768...  1.7436 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3391...  Training loss: 1.5677...  1.7772 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3392...  Training loss: 1.5268...  1.7025 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3393...  Training loss: 1.5337...  1.7216 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3394...  Training loss: 1.5818...  1.7542 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3395...  Training loss: 1.5902...  1.7296 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3396...  Training loss: 1.5854...  1.7712 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3397...  Training loss: 1.5642...  1.7121 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3398...  Training loss: 1.5420...  1.7542 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3399...  Training loss: 1.5781...  1.6845 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3400...  Training loss: 1.5786...  1.7451 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3401...  Training loss: 1.5549...  1.6288 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3402...  Training loss: 1.5678...  1.6594 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3403...  Training loss: 1.5417...  1.7181 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3404...  Training loss: 1.5372...  1.6550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3405...  Training loss: 1.5219...  1.7125 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3406...  Training loss: 1.5545...  1.7802 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3407...  Training loss: 1.5357...  1.7527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3408...  Training loss: 1.5884...  1.7865 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3409...  Training loss: 1.5496...  1.6915 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3410...  Training loss: 1.5408...  1.7271 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3411...  Training loss: 1.5737...  1.7481 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3412...  Training loss: 1.5310...  1.7682 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3413...  Training loss: 1.5509...  1.7963 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3414...  Training loss: 1.5481...  1.7361 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3415...  Training loss: 1.5502...  1.7772 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3416...  Training loss: 1.5880...  1.6935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3417...  Training loss: 1.5489...  1.7286 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3418...  Training loss: 1.6109...  1.7562 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3419...  Training loss: 1.5714...  1.7507 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3420...  Training loss: 1.5665...  1.7677 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3421...  Training loss: 1.5452...  1.6920 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3422...  Training loss: 1.5620...  1.7442 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3423...  Training loss: 1.5830...  1.7762 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3424...  Training loss: 1.5452...  1.7331 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3425...  Training loss: 1.5392...  1.7527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3426...  Training loss: 1.6000...  1.7040 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3427...  Training loss: 1.5670...  1.7131 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3428...  Training loss: 1.6143...  1.7446 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3429...  Training loss: 1.5939...  1.7426 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3430...  Training loss: 1.5675...  1.7842 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3431...  Training loss: 1.5566...  1.7116 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3432...  Training loss: 1.5651...  1.7732 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3433...  Training loss: 1.5929...  1.7211 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3434...  Training loss: 1.5401...  1.7426 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3435...  Training loss: 1.5581...  1.7827 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3436...  Training loss: 1.5670...  1.7271 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3437...  Training loss: 1.6092...  1.7677 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3438...  Training loss: 1.5901...  1.7060 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3439...  Training loss: 1.6025...  1.7286 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3440...  Training loss: 1.5632...  1.7632 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3441...  Training loss: 1.5596...  1.7301 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3442...  Training loss: 1.5840...  1.7567 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3443...  Training loss: 1.5691...  1.6975 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3444...  Training loss: 1.5593...  1.6775 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3445...  Training loss: 1.5273...  1.6935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3446...  Training loss: 1.5514...  1.7256 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3447...  Training loss: 1.5181...  1.7717 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3448...  Training loss: 1.5636...  1.7186 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3449...  Training loss: 1.5246...  1.7657 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3450...  Training loss: 1.5599...  1.6835 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3451...  Training loss: 1.5379...  1.7246 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3452...  Training loss: 1.5462...  1.7812 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3453...  Training loss: 1.5360...  1.7376 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3454...  Training loss: 1.5513...  1.7792 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3455...  Training loss: 1.5196...  1.7070 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3456...  Training loss: 1.5814...  1.7391 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3457...  Training loss: 1.5345...  1.7572 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3458...  Training loss: 1.5456...  1.7602 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3459...  Training loss: 1.5440...  1.7842 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3460...  Training loss: 1.5419...  1.7341 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3461...  Training loss: 1.5362...  1.7727 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3462...  Training loss: 1.5700...  1.6880 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3463...  Training loss: 1.5606...  1.7466 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3464...  Training loss: 1.5283...  1.7767 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3465...  Training loss: 1.5374...  1.7481 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3466...  Training loss: 1.5228...  1.7582 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3467...  Training loss: 1.5610...  1.6955 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3468...  Training loss: 1.5478...  1.7441 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3469...  Training loss: 1.5550...  1.7968 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3470...  Training loss: 1.5428...  1.7351 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3471...  Training loss: 1.5493...  1.7622 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3472...  Training loss: 1.5505...  1.6745 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3473...  Training loss: 1.5591...  1.7251 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3474...  Training loss: 1.5620...  1.7702 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3475...  Training loss: 1.5542...  1.7487 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3476...  Training loss: 1.5776...  1.7532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3477...  Training loss: 1.5360...  1.7080 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3478...  Training loss: 1.5481...  1.7873 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3479...  Training loss: 1.5521...  1.6985 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3480...  Training loss: 1.5348...  1.7386 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3481...  Training loss: 1.5393...  1.7827 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3482...  Training loss: 1.5192...  1.7401 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3483...  Training loss: 1.5629...  1.7847 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3484...  Training loss: 1.5652...  1.7010 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3485...  Training loss: 1.5481...  1.7401 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3486...  Training loss: 1.5484...  1.7833 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3487...  Training loss: 1.5638...  1.7727 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3488...  Training loss: 1.5217...  1.7832 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3489...  Training loss: 1.5055...  1.7131 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3490...  Training loss: 1.5737...  1.6704 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3491...  Training loss: 1.5450...  1.6835 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3492...  Training loss: 1.5094...  1.7321 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3493...  Training loss: 1.5664...  1.7672 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3494...  Training loss: 1.5722...  1.7181 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3495...  Training loss: 1.5376...  1.7807 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3496...  Training loss: 1.5458...  1.7085 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3497...  Training loss: 1.5102...  1.7361 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3498...  Training loss: 1.5229...  1.7627 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3499...  Training loss: 1.5639...  1.7231 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3500...  Training loss: 1.5613...  1.7808 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3501...  Training loss: 1.5708...  1.6750 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3502...  Training loss: 1.5585...  1.7381 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3503...  Training loss: 1.5860...  1.7426 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3504...  Training loss: 1.5690...  1.7657 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3505...  Training loss: 1.5807...  1.7807 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3506...  Training loss: 1.5671...  1.7176 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3507...  Training loss: 1.6155...  1.7687 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3508...  Training loss: 1.5609...  1.6900 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3509...  Training loss: 1.5576...  1.7306 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3510...  Training loss: 1.5793...  1.7722 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3511...  Training loss: 1.5439...  1.7336 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3512...  Training loss: 1.5917...  1.7727 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3513...  Training loss: 1.5710...  1.6985 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3514...  Training loss: 1.5959...  1.7206 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3515...  Training loss: 1.5691...  1.7577 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3516...  Training loss: 1.5544...  1.7426 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3517...  Training loss: 1.5127...  1.7717 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3518...  Training loss: 1.5554...  1.6880 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3519...  Training loss: 1.5804...  1.7105 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3520...  Training loss: 1.5603...  1.7472 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3521...  Training loss: 1.5593...  1.7156 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3522...  Training loss: 1.5552...  1.7737 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3523...  Training loss: 1.5722...  1.7286 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3524...  Training loss: 1.5544...  1.7572 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3525...  Training loss: 1.5249...  1.6895 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3526...  Training loss: 1.5839...  1.7426 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3527...  Training loss: 1.5894...  1.7727 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3528...  Training loss: 1.5627...  1.7331 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3529...  Training loss: 1.5623...  1.7567 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3530...  Training loss: 1.5541...  1.6960 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3531...  Training loss: 1.5553...  1.7236 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3532...  Training loss: 1.5623...  1.7797 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3533...  Training loss: 1.5778...  1.7547 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3534...  Training loss: 1.6367...  1.7822 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3535...  Training loss: 1.5569...  1.6810 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3536...  Training loss: 1.5498...  1.6985 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3537...  Training loss: 1.5388...  1.6905 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3538...  Training loss: 1.5361...  1.7316 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3539...  Training loss: 1.5846...  1.7602 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3540...  Training loss: 1.5648...  1.7121 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3541...  Training loss: 1.5815...  1.7802 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3542...  Training loss: 1.5347...  1.7258 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3543...  Training loss: 1.5457...  1.7436 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3544...  Training loss: 1.5749...  1.7637 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3545...  Training loss: 1.5408...  1.7236 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3546...  Training loss: 1.5342...  1.7582 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3547...  Training loss: 1.5273...  1.6825 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3548...  Training loss: 1.5513...  1.7281 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3549...  Training loss: 1.5693...  1.7767 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3550...  Training loss: 1.5594...  1.7637 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3551...  Training loss: 1.5424...  1.7842 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3552...  Training loss: 1.5401...  1.7271 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3553...  Training loss: 1.5812...  1.7607 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3554...  Training loss: 1.5461...  1.6840 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3555...  Training loss: 1.5369...  1.7486 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3556...  Training loss: 1.5442...  1.7697 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3557...  Training loss: 1.5349...  1.7301 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3558...  Training loss: 1.5464...  1.7612 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3559...  Training loss: 1.5617...  1.7131 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3560...  Training loss: 1.5419...  1.7221 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3561...  Training loss: 1.5264...  1.7642 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3562...  Training loss: 1.5714...  1.7291 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3563...  Training loss: 1.5550...  1.7562 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3564...  Training loss: 1.5365...  1.6970 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3565...  Training loss: 1.6523...  1.7176 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3566...  Training loss: 1.5642...  1.7361 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3567...  Training loss: 1.5481...  1.7386 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3568...  Training loss: 1.5648...  1.7782 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3569...  Training loss: 1.5458...  1.7321 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3570...  Training loss: 1.5178...  1.7577 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3571...  Training loss: 1.5717...  1.7050 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3572...  Training loss: 1.5405...  1.7201 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3573...  Training loss: 1.5776...  1.7567 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3574...  Training loss: 1.5427...  1.7281 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3575...  Training loss: 1.5375...  1.7497 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3576...  Training loss: 1.5439...  1.6980 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3577...  Training loss: 1.5454...  1.7386 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3578...  Training loss: 1.5930...  1.7717 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3579...  Training loss: 1.5517...  1.7492 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3580...  Training loss: 1.5266...  1.7737 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3581...  Training loss: 1.5634...  1.7161 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3582...  Training loss: 1.5781...  1.7396 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3583...  Training loss: 1.5626...  1.7677 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3584...  Training loss: 1.5786...  1.7316 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3585...  Training loss: 1.5472...  1.7767 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3586...  Training loss: 1.5716...  1.7266 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3587...  Training loss: 1.5464...  1.7602 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3588...  Training loss: 1.5666...  1.6740 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3589...  Training loss: 1.5561...  1.7357 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3590...  Training loss: 1.5122...  1.7717 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3591...  Training loss: 1.5259...  1.7421 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3592...  Training loss: 1.5706...  1.7652 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3593...  Training loss: 1.5821...  1.6925 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3594...  Training loss: 1.5634...  1.7131 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3595...  Training loss: 1.5598...  1.7587 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3596...  Training loss: 1.5335...  1.7126 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3597...  Training loss: 1.5723...  1.7492 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3598...  Training loss: 1.5687...  1.8429 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3599...  Training loss: 1.5433...  1.7371 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3600...  Training loss: 1.5493...  1.6669 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3601...  Training loss: 1.5275...  1.7005 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3602...  Training loss: 1.5210...  1.7321 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3603...  Training loss: 1.5097...  1.7126 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3604...  Training loss: 1.5431...  1.7667 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3605...  Training loss: 1.5259...  1.7431 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3606...  Training loss: 1.5733...  1.7582 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3607...  Training loss: 1.5364...  1.7366 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3608...  Training loss: 1.5183...  1.7582 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3609...  Training loss: 1.5576...  1.8058 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3610...  Training loss: 1.5141...  1.7144 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3611...  Training loss: 1.5385...  1.7326 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3612...  Training loss: 1.5354...  1.7572 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3613...  Training loss: 1.5408...  1.7356 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3614...  Training loss: 1.5906...  1.7622 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3615...  Training loss: 1.5344...  1.7226 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3616...  Training loss: 1.5928...  1.7476 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3617...  Training loss: 1.5674...  1.6745 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3618...  Training loss: 1.5597...  1.7702 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3619...  Training loss: 1.5292...  1.7948 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3620...  Training loss: 1.5417...  1.7502 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3621...  Training loss: 1.5667...  1.7717 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3622...  Training loss: 1.5209...  1.6970 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3623...  Training loss: 1.5183...  1.7321 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3624...  Training loss: 1.5770...  1.7547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3625...  Training loss: 1.5571...  1.7331 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3626...  Training loss: 1.6019...  1.7788 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3627...  Training loss: 1.5871...  1.7030 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3628...  Training loss: 1.5474...  1.7266 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3629...  Training loss: 1.5439...  1.7562 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3630...  Training loss: 1.5574...  1.7582 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3631...  Training loss: 1.5653...  1.8083 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3632...  Training loss: 1.5283...  1.7271 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3633...  Training loss: 1.5486...  1.8013 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3634...  Training loss: 1.5408...  1.6900 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3635...  Training loss: 1.5897...  1.7436 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3636...  Training loss: 1.5661...  1.7572 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3637...  Training loss: 1.5786...  1.7351 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3638...  Training loss: 1.5345...  1.7712 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3639...  Training loss: 1.5402...  1.7070 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3640...  Training loss: 1.5746...  1.7386 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3641...  Training loss: 1.5513...  1.7522 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3642...  Training loss: 1.5431...  1.7306 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3643...  Training loss: 1.5150...  1.7787 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3644...  Training loss: 1.5431...  1.7301 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3645...  Training loss: 1.5102...  1.7722 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3646...  Training loss: 1.5597...  1.6915 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3647...  Training loss: 1.5224...  1.7206 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3648...  Training loss: 1.5349...  1.7943 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3649...  Training loss: 1.5150...  1.7396 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3650...  Training loss: 1.5427...  1.7732 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3651...  Training loss: 1.5262...  1.7116 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3652...  Training loss: 1.5419...  1.7231 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3653...  Training loss: 1.5163...  1.8043 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3654...  Training loss: 1.5609...  1.7476 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3655...  Training loss: 1.5219...  1.7522 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3656...  Training loss: 1.5371...  1.6985 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3657...  Training loss: 1.5231...  1.7241 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3658...  Training loss: 1.5243...  1.7998 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3659...  Training loss: 1.5347...  1.7983 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3660...  Training loss: 1.5609...  1.7672 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3661...  Training loss: 1.5522...  1.7467 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3662...  Training loss: 1.5167...  1.7832 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3663...  Training loss: 1.5388...  1.6945 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3664...  Training loss: 1.5092...  1.7487 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3665...  Training loss: 1.5607...  1.7792 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3666...  Training loss: 1.5441...  1.7512 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3667...  Training loss: 1.5368...  1.7702 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3668...  Training loss: 1.5348...  1.6905 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3669...  Training loss: 1.5385...  1.7542 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3670...  Training loss: 1.5367...  1.7837 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3671...  Training loss: 1.5400...  1.7376 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3672...  Training loss: 1.5456...  1.7461 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3673...  Training loss: 1.5436...  1.7075 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3674...  Training loss: 1.5567...  1.7361 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3675...  Training loss: 1.5248...  1.7452 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3676...  Training loss: 1.5439...  1.7411 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3677...  Training loss: 1.5398...  1.7832 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3678...  Training loss: 1.5222...  1.7351 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3679...  Training loss: 1.5181...  1.7792 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3680...  Training loss: 1.5108...  1.6945 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3681...  Training loss: 1.5561...  1.7451 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3682...  Training loss: 1.5505...  1.7804 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3683...  Training loss: 1.5493...  1.7411 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3684...  Training loss: 1.5393...  1.7627 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3685...  Training loss: 1.5495...  1.6980 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3686...  Training loss: 1.5039...  1.7246 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3687...  Training loss: 1.4977...  1.8023 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3688...  Training loss: 1.5576...  1.7642 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3689...  Training loss: 1.5369...  1.7817 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3690...  Training loss: 1.5019...  1.7632 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3691...  Training loss: 1.5578...  1.7823 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3692...  Training loss: 1.5507...  1.7070 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3693...  Training loss: 1.5335...  1.7572 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3694...  Training loss: 1.5129...  1.7792 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3695...  Training loss: 1.4984...  1.7055 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3696...  Training loss: 1.5143...  1.8048 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3697...  Training loss: 1.5598...  1.7035 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3698...  Training loss: 1.5544...  1.7381 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3699...  Training loss: 1.5542...  1.7637 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3700...  Training loss: 1.5418...  1.7431 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3701...  Training loss: 1.5732...  1.7777 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3702...  Training loss: 1.5474...  1.7000 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3703...  Training loss: 1.5607...  1.7126 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3704...  Training loss: 1.5491...  1.7818 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3705...  Training loss: 1.6004...  1.7466 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3706...  Training loss: 1.5418...  1.7557 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3707...  Training loss: 1.5335...  1.7131 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3708...  Training loss: 1.5651...  1.7672 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3709...  Training loss: 1.5243...  1.6990 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3710...  Training loss: 1.5698...  1.7612 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3711...  Training loss: 1.5569...  1.7792 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3712...  Training loss: 1.5952...  1.7412 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3713...  Training loss: 1.5591...  1.7816 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3714...  Training loss: 1.5407...  1.6960 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3715...  Training loss: 1.5014...  1.7196 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3716...  Training loss: 1.5409...  1.7782 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3717...  Training loss: 1.5620...  1.7391 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3718...  Training loss: 1.5439...  1.7852 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3719...  Training loss: 1.5455...  1.6910 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3720...  Training loss: 1.5527...  1.7126 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3721...  Training loss: 1.5577...  1.7642 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3722...  Training loss: 1.5412...  1.7456 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3723...  Training loss: 1.5161...  1.7707 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3724...  Training loss: 1.5671...  1.7201 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3725...  Training loss: 1.5803...  1.7817 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3726...  Training loss: 1.5563...  1.6990 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3727...  Training loss: 1.5440...  1.7226 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3728...  Training loss: 1.5438...  1.7592 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3729...  Training loss: 1.5405...  1.7346 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3730...  Training loss: 1.5383...  1.7767 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3731...  Training loss: 1.5626...  1.6925 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3732...  Training loss: 1.6121...  1.7256 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3733...  Training loss: 1.5421...  1.8068 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3734...  Training loss: 1.5440...  1.7416 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3735...  Training loss: 1.5247...  1.7557 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3736...  Training loss: 1.5252...  1.7366 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3737...  Training loss: 1.5802...  1.7762 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3738...  Training loss: 1.5539...  1.7070 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3739...  Training loss: 1.5563...  1.7371 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3740...  Training loss: 1.5260...  1.7466 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3741...  Training loss: 1.5420...  1.7231 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3742...  Training loss: 1.5604...  1.7792 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3743...  Training loss: 1.5211...  1.6865 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3744...  Training loss: 1.5210...  1.7306 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3745...  Training loss: 1.5146...  1.7818 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3746...  Training loss: 1.5388...  1.7587 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3747...  Training loss: 1.5424...  1.7933 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3748...  Training loss: 1.5435...  1.7236 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3749...  Training loss: 1.5360...  1.7391 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3750...  Training loss: 1.5239...  1.7727 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3751...  Training loss: 1.5629...  1.7471 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3752...  Training loss: 1.5362...  1.7918 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3753...  Training loss: 1.5407...  1.7446 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3754...  Training loss: 1.5384...  1.7762 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3755...  Training loss: 1.5365...  1.7366 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3756...  Training loss: 1.5199...  1.7938 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3757...  Training loss: 1.5384...  1.7727 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3758...  Training loss: 1.5258...  1.7276 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3759...  Training loss: 1.5082...  1.7697 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3760...  Training loss: 1.5533...  1.6935 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3761...  Training loss: 1.5372...  1.7040 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3762...  Training loss: 1.5308...  1.7707 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3763...  Training loss: 1.6396...  1.7502 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3764...  Training loss: 1.5484...  1.7552 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3765...  Training loss: 1.5398...  1.7191 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3766...  Training loss: 1.5558...  1.7141 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3767...  Training loss: 1.5269...  1.7642 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3768...  Training loss: 1.5015...  1.7471 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3769...  Training loss: 1.5511...  1.7657 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3770...  Training loss: 1.5372...  1.7321 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3771...  Training loss: 1.5659...  1.7797 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3772...  Training loss: 1.5293...  1.7226 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3773...  Training loss: 1.5266...  1.7321 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3774...  Training loss: 1.5235...  1.7682 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3775...  Training loss: 1.5348...  1.7301 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3776...  Training loss: 1.5736...  1.7803 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3777...  Training loss: 1.5325...  1.7136 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3778...  Training loss: 1.5213...  1.7241 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3779...  Training loss: 1.5472...  1.7742 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3780...  Training loss: 1.5733...  1.7256 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3781...  Training loss: 1.5454...  1.7913 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3782...  Training loss: 1.5714...  1.7085 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3783...  Training loss: 1.5318...  1.6860 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3784...  Training loss: 1.5665...  1.7642 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3785...  Training loss: 1.5428...  1.7592 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3786...  Training loss: 1.5481...  1.8078 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3787...  Training loss: 1.5444...  1.7547 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3788...  Training loss: 1.4911...  1.7757 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3789...  Training loss: 1.5106...  1.6955 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3790...  Training loss: 1.5586...  1.7371 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3791...  Training loss: 1.5656...  1.7792 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3792...  Training loss: 1.5598...  1.7567 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3793...  Training loss: 1.5437...  1.7702 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3794...  Training loss: 1.5115...  1.6780 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3795...  Training loss: 1.5645...  1.7151 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3796...  Training loss: 1.5616...  1.8013 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3797...  Training loss: 1.5299...  1.7476 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3798...  Training loss: 1.5420...  1.7657 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3799...  Training loss: 1.5213...  1.7346 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3800...  Training loss: 1.5054...  1.7537 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3801...  Training loss: 1.4861...  1.6233 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3802...  Training loss: 1.5283...  1.7251 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3803...  Training loss: 1.5189...  1.7411 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3804...  Training loss: 1.5600...  1.7226 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3805...  Training loss: 1.5297...  1.7492 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3806...  Training loss: 1.5193...  1.6950 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3807...  Training loss: 1.5451...  1.7446 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3808...  Training loss: 1.4956...  1.7863 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3809...  Training loss: 1.5328...  1.7226 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3810...  Training loss: 1.5301...  1.7637 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3811...  Training loss: 1.5163...  1.6975 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3812...  Training loss: 1.5654...  1.7126 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3813...  Training loss: 1.5094...  1.7832 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3814...  Training loss: 1.5855...  1.7497 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3815...  Training loss: 1.5464...  1.7792 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3816...  Training loss: 1.5474...  1.7386 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3817...  Training loss: 1.5299...  1.7662 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3818...  Training loss: 1.5343...  1.7015 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3819...  Training loss: 1.5542...  1.7291 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3820...  Training loss: 1.5221...  1.7692 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3821...  Training loss: 1.5185...  1.7326 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3822...  Training loss: 1.5719...  1.7862 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3823...  Training loss: 1.5449...  1.7181 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3824...  Training loss: 1.5908...  1.7752 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3825...  Training loss: 1.5678...  1.7657 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3826...  Training loss: 1.5433...  1.7517 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3827...  Training loss: 1.5317...  1.7752 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3828...  Training loss: 1.5490...  1.7284 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3829...  Training loss: 1.5533...  1.6995 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3830...  Training loss: 1.5138...  1.6835 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3831...  Training loss: 1.5332...  1.7376 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3832...  Training loss: 1.5334...  1.7471 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3833...  Training loss: 1.5817...  1.7321 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3834...  Training loss: 1.5641...  1.7767 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3835...  Training loss: 1.5710...  1.6925 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3836...  Training loss: 1.5238...  1.7382 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3837...  Training loss: 1.5349...  1.7697 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3838...  Training loss: 1.5616...  1.7582 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3839...  Training loss: 1.5398...  1.8008 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3840...  Training loss: 1.5312...  1.6975 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3841...  Training loss: 1.5047...  1.7296 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3842...  Training loss: 1.5440...  1.7933 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3843...  Training loss: 1.5001...  1.7567 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3844...  Training loss: 1.5432...  1.7777 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3845...  Training loss: 1.5087...  1.7351 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3846...  Training loss: 1.5292...  1.7697 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3847...  Training loss: 1.5100...  1.6870 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3848...  Training loss: 1.5308...  1.7356 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3849...  Training loss: 1.5148...  1.7832 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3850...  Training loss: 1.5251...  1.7442 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3851...  Training loss: 1.4999...  1.7697 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3852...  Training loss: 1.5509...  1.6900 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3853...  Training loss: 1.5191...  1.7236 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3854...  Training loss: 1.5225...  1.7612 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3855...  Training loss: 1.5209...  1.7542 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3856...  Training loss: 1.5164...  1.7928 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3857...  Training loss: 1.5125...  1.7131 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3858...  Training loss: 1.5524...  1.7286 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3859...  Training loss: 1.5408...  1.7787 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3860...  Training loss: 1.4946...  1.7331 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3861...  Training loss: 1.5158...  1.7607 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3862...  Training loss: 1.5048...  1.7421 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3863...  Training loss: 1.5429...  1.7963 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3864...  Training loss: 1.5328...  1.6940 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3865...  Training loss: 1.5284...  1.7577 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3866...  Training loss: 1.5222...  1.7647 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3867...  Training loss: 1.5322...  1.7528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3868...  Training loss: 1.5425...  1.7807 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3869...  Training loss: 1.5254...  1.7045 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3870...  Training loss: 1.5310...  1.7216 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3871...  Training loss: 1.5317...  1.7868 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3872...  Training loss: 1.5499...  1.7386 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3873...  Training loss: 1.5125...  1.7792 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3874...  Training loss: 1.5194...  1.7030 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3875...  Training loss: 1.5253...  1.7136 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3876...  Training loss: 1.5133...  1.6810 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3877...  Training loss: 1.5177...  1.7436 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3878...  Training loss: 1.4981...  1.7797 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3879...  Training loss: 1.5400...  1.7166 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3880...  Training loss: 1.5413...  1.7617 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3881...  Training loss: 1.5222...  1.6945 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3882...  Training loss: 1.5270...  1.7276 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3883...  Training loss: 1.5323...  1.7807 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3884...  Training loss: 1.4979...  1.7261 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3885...  Training loss: 1.4941...  1.7908 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3886...  Training loss: 1.5515...  1.6970 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3887...  Training loss: 1.5273...  1.7106 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3888...  Training loss: 1.4833...  1.7812 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3889...  Training loss: 1.5632...  1.7447 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3890...  Training loss: 1.5412...  1.7446 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3891...  Training loss: 1.5265...  1.7396 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3892...  Training loss: 1.5162...  1.7847 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3893...  Training loss: 1.4796...  1.7020 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3894...  Training loss: 1.4985...  1.7532 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3895...  Training loss: 1.5443...  1.7532 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3896...  Training loss: 1.5438...  1.7306 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3897...  Training loss: 1.5410...  1.7597 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3898...  Training loss: 1.5269...  1.6890 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3899...  Training loss: 1.5623...  1.7441 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3900...  Training loss: 1.5433...  1.7878 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3901...  Training loss: 1.5413...  1.7742 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3902...  Training loss: 1.5426...  1.7873 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3903...  Training loss: 1.5922...  1.7256 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3904...  Training loss: 1.5398...  1.7436 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3905...  Training loss: 1.5236...  1.7762 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3906...  Training loss: 1.5574...  1.7642 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3907...  Training loss: 1.5232...  1.7857 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3908...  Training loss: 1.5558...  1.7401 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3909...  Training loss: 1.5457...  1.7667 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3910...  Training loss: 1.5748...  1.6880 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3911...  Training loss: 1.5466...  1.7381 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3912...  Training loss: 1.5272...  1.7667 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3913...  Training loss: 1.4910...  1.7486 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3914...  Training loss: 1.5352...  1.7863 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3915...  Training loss: 1.5423...  1.7166 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3916...  Training loss: 1.5304...  1.7161 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3917...  Training loss: 1.5393...  1.7356 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3918...  Training loss: 1.5350...  1.7481 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3919...  Training loss: 1.5518...  1.7923 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3920...  Training loss: 1.5336...  1.6865 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3921...  Training loss: 1.4988...  1.6800 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3922...  Training loss: 1.5700...  1.6915 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3923...  Training loss: 1.5601...  1.7396 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3924...  Training loss: 1.5300...  1.7697 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3925...  Training loss: 1.5257...  1.7497 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3926...  Training loss: 1.5382...  1.7948 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3927...  Training loss: 1.5344...  1.7085 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3928...  Training loss: 1.5394...  1.7366 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3929...  Training loss: 1.5469...  1.7732 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3930...  Training loss: 1.6143...  1.7256 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3931...  Training loss: 1.5306...  1.7732 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3932...  Training loss: 1.5318...  1.6895 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3933...  Training loss: 1.5127...  1.7401 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3934...  Training loss: 1.5082...  1.7943 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3935...  Training loss: 1.5687...  1.7602 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3936...  Training loss: 1.5367...  1.7797 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3937...  Training loss: 1.5547...  1.7271 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3938...  Training loss: 1.5119...  1.7702 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3939...  Training loss: 1.5223...  1.6830 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3940...  Training loss: 1.5550...  1.7456 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3941...  Training loss: 1.5093...  1.7707 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3942...  Training loss: 1.5175...  1.7226 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3943...  Training loss: 1.5036...  1.8083 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3944...  Training loss: 1.5331...  1.6985 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3945...  Training loss: 1.5343...  1.7361 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3946...  Training loss: 1.5375...  1.7236 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3947...  Training loss: 1.5243...  1.7517 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3948...  Training loss: 1.5148...  1.7522 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3949...  Training loss: 1.5452...  1.6955 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3950...  Training loss: 1.5223...  1.7361 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3951...  Training loss: 1.5277...  1.7878 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3952...  Training loss: 1.5311...  1.7622 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3953...  Training loss: 1.5138...  1.7617 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3954...  Training loss: 1.5057...  1.7110 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3955...  Training loss: 1.5369...  1.7622 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3956...  Training loss: 1.5095...  1.6740 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3957...  Training loss: 1.5050...  1.7356 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3958...  Training loss: 1.5405...  1.7637 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3959...  Training loss: 1.5234...  1.7422 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3960...  Training loss: 1.5187...  1.7878 sec/batch\n"
     ]
    }
   ],
   "source": [
    "# Network training:\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps, lstm_size=lstm_size,\n",
    "               num_layers=num_layers, learning_rate=learning_rate)\n",
    "# Saver:\n",
    "saver = tf.train.Saver(max_to_keep = 50)\n",
    "\n",
    "# Tensorflow session:\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    # saver.restore(sess, 'checkpoints/___ckpt')\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # Training:\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        \n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x, model.targets: y, model.keep_prob: keep_prob,\n",
    "                   model.initial_state: new_state}\n",
    "            \n",
    "            batch_loss, new_state, _ = sess.run([model.loss, model.final_state, model.optimizer],\n",
    "                                               feed_dict = feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            \n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints\\\\i3960_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i200_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i400_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i600_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i800_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1200_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1400_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1600_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1800_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2200_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2400_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2600_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2800_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3200_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3400_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3600_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3800_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3960_l256.ckpt\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sampling:\n",
    "\n",
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p = p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints\\\\i3960_l256.ckpt'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andy Miller and the most servenase, and he had not their\n",
      "terabinate of seeil which she had been saight in who asked the struck to\n",
      "the toucher, he saw to him, with his husband and what he could brought to him that the mother said that she saw it was said on his beat and tell, and had to she had been still happened in her heart, the standing towards, and then the came an one and walked him on weak of whom was always thought about to some of a\n",
      "condision of those hand, he carriage of a memal time too with that\n",
      "which he had\n",
      "seen over the stattory and what she had not seemed that they they had\n",
      "no an account and to she was the\n",
      "patituon.\n",
      "\n",
      "\"And you have been see all this\n",
      "mother? I'm not stopped him, to me of saying at the\n",
      "stabes, what there then\n",
      "all he has at once hear the peasant.\n",
      "\n",
      "\"And I'm shuller, and I consent to see a state, because they were so thas how see him.\"\n",
      "\n",
      "\"Yes, I said\n",
      "what was see a lefs this what's so,\" she was sating to the part of whan she had so hand to him. \"If I'll not are that it's\n",
      "not\n",
      "a croad,\"\n",
      "said Vronsky, as talking to him them, and so which said to the position on the position and tearse, had been been the profless\n",
      "to her and her finger and already and so sees to\n",
      "she had nave the simple the saud with the steps had so that that shis heasing his whates to have said and taken about a meant and a cight, he was soll had been hears the party. \"You don't know what was ifterself into\n",
      "this worns, I have not to be into the misting that she did not say the children at it well it in the contricate, I's\n",
      "bast with themselvision, what I said of to his stopir to the present,\" said Shtinith's\n",
      "some shall\n",
      "of whos when to say that he could not though the matter and taker to the same and attanded it.\n",
      "\n",
      "\"I have seen about the country of another. An every tond.\"\n",
      "\n",
      "And all a shall sont of the words on\n",
      "all to him to see that standing. Though he had not had been sort of\n",
      "the stards\n",
      "our what time was the mind on the conderation into the choon was not he sense an expected her and shakory wh\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Andy Miller\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
